{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entrenamiento de la red"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resultados de las redes sin entrenar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En la clase anterior creamos una red neuronal, de dos maneras, desde cero y a través del transfer learning. En el primer caso (desde cero) todos sus parámetros se han inicializado aleatoriamente, mientras que en el segundo (transfer learning) solo los de la última capa son aleatorios. En ambos casos, al inicializarse estos aleatoriamente va a hacer que la red no sea capaz de hacer su cometido.\n",
        "\n",
        "Veamos un ejemplo con las dos redes. Nos vamos a descargar el conjunto de datos ``CIFAR-10`` desde el conjunto de datos de Pytorch, vamos a crear los datasets, vamos a crear los dos tipos de redes, y a las dos les vamos a dar una imagen, a ver si son capaces de predecir bien qué hay en ella"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Descargamos y creamos el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Descargamos y creamos el dataset\n",
        "dataset_train = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "dataset_test = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos un dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataloader = DataLoader(dataset_train, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(dataset_test, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Redes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos las redes neuronales desde cero y desde transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "# Creamos la red neuronal desde cero\n",
        "class NeuralNetworkFromScratch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetworkFromScratch, self).__init__()   # Se inicializa el módulo nn.Module\n",
        "        self.flatten = nn.Flatten()             # Se crea una primera capa que aplana la imagen de entrada\n",
        "        self.linear_relu_stack = nn.Sequential( # Se crea una módulo de arquitectura secuencial:\n",
        "            nn.Linear(3*32*32, 512),                # Se añade una primera capa lineal que está preparada \n",
        "                                                    # para que le entre un vector de 28*28 (784)\n",
        "                                                    # y sacará un vector de 512\n",
        "            nn.ReLU(),                              # Se añade una no linealidad\n",
        "            nn.Linear(512, 512),                    # Se añade una segunda capa lineal que le entran 512 \n",
        "                                                    # datos y saca 512 datos\n",
        "            nn.ReLU(),                              # Se añade una no linealidad\n",
        "            nn.Linear(512, 10)                      # Se añade una tercera capa lineal que le entran 512 \n",
        "                                                    # datos y saca un array de tamaño 10 (el número\n",
        "                                                    # de etiquetas)\n",
        "        )\n",
        "        #self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)                         # Se pasa la imagen por la capa de aplanado\n",
        "        logits = self.linear_relu_stack(x)          # Se pasa el vector resultante por la red\n",
        "        #probs = self.softmax(logits)\n",
        "        return logits\n",
        "\n",
        "model_scratch = NeuralNetworkFromScratch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "\n",
        "# Creamos la red neuronal mediante transfer learning\n",
        "model_tl = models.resnet18(pretrained=True)\n",
        "model_tl.fc = nn.Linear(512, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Muestra del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cogemos una muestra aletoria del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from random import randint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cogemos una muestra al azahar\n",
        "idx = randint(0, len(train_dataloader)-1)\n",
        "sample_dataloader = next(iter(train_dataloader))\n",
        "sample_dataloader_images, sample_dataloader_labels = sample_dataloader\n",
        "idx = randint(0, len(sample_dataloader_images)-1)\n",
        "sample_image = sample_dataloader_images[idx]\n",
        "sample_label = sample_dataloader_labels[idx]\n",
        "\n",
        "labels_map = {\n",
        "    0: \"airplane\",\n",
        "    1: \"automobile\",\n",
        "    2: \"bird\",\n",
        "    3: \"cat\",\n",
        "    4: \"deer\",\n",
        "    5: \"dog\",\n",
        "    6: \"frog\",\n",
        "    7: \"horse\",\n",
        "    8: \"ship\",\n",
        "    9: \"truck\",\n",
        "}\n",
        "\n",
        "figure = plt.figure(figsize=(5, 5))\n",
        "plt.title(labels_map[sample_label.item()])\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.imshow(sample_image.permute(1,2,0).squeeze())  \n",
        "# permute cambia el orden de las dimensiones, ya que las\n",
        "# imágenes tienen tamaño 3x32x32, pero necesitamos que\n",
        "# las dimensiones sean 32x32x3 para que matplotlib las pinte\n",
        "# así que se permutan poniendo primero las dos de 32 y \n",
        "# la última la de 3\n",
        "\n",
        "# squeeze elimina todas las dimensiones 1 de un tensor, \n",
        "# si se le mete un tensor de dimensiones (Ax1xBxCx1xD) \n",
        "# devuelve un tensor de dimensiones (AxBxCxD)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se la pasamos a las redes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La red ha devuelto torch.Size([64, 10]) logits\n"
          ]
        }
      ],
      "source": [
        "# Le pasamos la imagen a la red neuronal creada desde cero\n",
        "logits_scratch = model_scratch(sample_dataloader_images)\n",
        "print(f\"La red ha devuelto {logits_scratch.shape} logits\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La red ha creado 64 logits porque hemos creado un Dataloader con un batch size de 68, es decir hemos hecho que se divida el Dataset que contienen 50.000 imágenes de entrenamiento en batches con 64 imágenes cada uno.\n",
        "\n",
        "Como se le ha pasado a la red ese Dataloader con 64 imágenes devuelve 64 resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La red cree que en la imagen hay horse\n"
          ]
        }
      ],
      "source": [
        "# La red ha devuelto 64 logits, por lo que nos quedamos con el \n",
        "# número idx que es el que se ha representado antes\n",
        "probs_scratch = logits_scratch.softmax(dim=1)\n",
        "label = probs_scratch[idx].argmax().item()\n",
        "print(f\"La red cree que en la imagen hay {labels_map[label]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La red cree que en la imagen hay airplane\n"
          ]
        }
      ],
      "source": [
        "# Le pasamos la imagen a la red neuronal creada desde transfer learning\n",
        "\n",
        "# En este caso la red espera un batch de imágenes, además de 3 canales (color),\n",
        "# mientras que ahora tenemos una sola imagen de un solo canal (blanco y negro), \n",
        "# por lo que expandimos el tensor que se le mete a la entrada para que la red se \n",
        "# piense que tiene un batch de varias imágenes de 3 canales (en color)\n",
        "sample_image_expand = sample_image.expand(5, 3, 32, 32)\n",
        "\n",
        "# Introducimos este tensor a la red, esto nos dará 5 predicciones\n",
        "label_tl = model_tl(sample_image_expand)\n",
        "\n",
        "# Cogemos una de las predicciones\n",
        "label = label_tl[0].argmax().item()\n",
        "print(f\"La red cree que en la imagen hay {labels_map[label]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entrenamiento de la red neuronal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos ver, ninguna de las dos ha acertado, por lo que pasamos a entrenarlas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero creamos unos dataloaders para entrenar por lotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataloader = DataLoader(dataset_train, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(dataset_test, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Función de pérdida"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuación creamos una función de pérdida. Como recordatorio, la función de pérdida es lo que nos permite saber cómo de equivocada está la red, y es la función que queremos minimizar. Para calcular esta función de pérdida comparamos el resultado que nos ha dado la red, con el que sabemos a través de las etiquetas que debería haber \n",
        "\n",
        "Las funciones de pérdida comunes incluyen [``nn.MSELoss``](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) (error cuadrático medio) para tareas de regresión y [``nn.NLLLoss``](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) (probabilidad de registro negativo) para clasificación.\n",
        "\n",
        "En este caso estamos en un problema de clasificación\n",
        "\n",
        "[``nn.CrossEntropyLoss``](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) combina ``nn.LogSoftmax`` y ``nn.NLLLoss``.\n",
        "\n",
        "Si te fijas, en la red creada desde cero hemos comentado la capa de `softmax` para así poder usar la función ``nn.CrossEntropyLoss``. Es mejor no usar la capa de `softmax` y usar la función ``nn.CrossEntropyLoss`` que ya incluye la parte de `softmax`. Esto es debido a motivos computacionales, haciéndolo así es más estable. Ahora no es el momento de entender esto\n",
        "\n",
        "Pasamos los logits de salida de nuestro modelo a ``nn.CrossEntropyLoss``, que normalizarán los logits y calcularán el error de predicción."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El optimizador es el proceso por el que minimizamos la función de pérdida, haciendo que la diferencia entre lo que predice la red y lo que de verdad es sea lo menos posible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La optimización es el proceso de ajustar los parámetros del modelo para reducir el error del modelo en cada paso de entrenamiento. **Los algoritmos de optimización** definen cómo se realiza este proceso (en este ejemplo usamos Descenso de gradiente estocástico). Toda la lógica de optimización está encapsulada en el objeto ``optimizer``. Aquí usamos el optimizador SGD; Además, hay muchos [optimizadores diferentes](https://pytorch.org/docs/stable/optim.html) disponibles en PyTorch, como `ADAM` y `RMSProp`, que funcionan mejor para diferentes tipos de modelos y datos.\n",
        "\n",
        "Inicializamos el optimizador registrando los parámetros del modelo que necesitan ser entrenados y pasando el hiperparámetro de tasa de aprendizaje."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a entrenar solo la red neuronal creada desde cero, ya que es un perceptron multicapa, mientras que la creada mediante transfer learning es una red convolucional que aun no se ha explicado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "LR = 1e-2\n",
        "\n",
        "optimizer = torch.optim.SGD(model_scratch.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ciclo de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dentro del ciclo de entrenamiento, la optimización ocurre en tres pasos:\n",
        " * Llamada a ``optimizer.zero_grad()`` para restablecer los gradientes de los parámetros del modelo. Los degradados se suman por defecto; para evitar el doble conteo, los ponemos a cero explícitamente en cada iteración.\n",
        " * Retropropagación de la pérdida de predicción con una llamada a ``loss.backwards()``. PyTorch deposita los gradientes de pérdida con cada parámetro.\n",
        " * Una vez que tenemos nuestros degradados, llamamos ``optimizer.step()`` a ajustar los parámetros por los degradados recogidos en la backward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos ``train_loop`` que recorre nuestro código de optimización y ``test_loop`` que evalúa el rendimiento del modelo en comparación con nuestros datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # X and y to device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            # X and y to device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            \n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "NeuralNetworkFromScratch(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear_relu_stack): Sequential(\n",
              "    (0): Linear(in_features=3072, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "model_scratch.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.301011  [    0/50000]\n",
            "loss: 2.280377  [ 6400/50000]\n",
            "loss: 2.179036  [12800/50000]\n",
            "loss: 2.188025  [19200/50000]\n",
            "loss: 2.088279  [25600/50000]\n",
            "loss: 2.043051  [32000/50000]\n",
            "loss: 2.139561  [38400/50000]\n",
            "loss: 1.977216  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 29.6%, Avg loss: 1.957990 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.010353  [    0/50000]\n",
            "loss: 1.997623  [ 6400/50000]\n",
            "loss: 1.750316  [12800/50000]\n",
            "loss: 1.986206  [19200/50000]\n",
            "loss: 1.980092  [25600/50000]\n",
            "loss: 1.944378  [32000/50000]\n",
            "loss: 1.993266  [38400/50000]\n",
            "loss: 1.847662  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 33.9%, Avg loss: 1.859632 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.843515  [    0/50000]\n",
            "loss: 1.849531  [ 6400/50000]\n",
            "loss: 1.615582  [12800/50000]\n",
            "loss: 1.875607  [19200/50000]\n",
            "loss: 1.918449  [25600/50000]\n",
            "loss: 1.867430  [32000/50000]\n",
            "loss: 1.910365  [38400/50000]\n",
            "loss: 1.772202  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.789821 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.753593  [    0/50000]\n",
            "loss: 1.744315  [ 6400/50000]\n",
            "loss: 1.530156  [12800/50000]\n",
            "loss: 1.786296  [19200/50000]\n",
            "loss: 1.846535  [25600/50000]\n",
            "loss: 1.820713  [32000/50000]\n",
            "loss: 1.835381  [38400/50000]\n",
            "loss: 1.717980  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.734796 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.696965  [    0/50000]\n",
            "loss: 1.674847  [ 6400/50000]\n",
            "loss: 1.466949  [12800/50000]\n",
            "loss: 1.735217  [19200/50000]\n",
            "loss: 1.772075  [25600/50000]\n",
            "loss: 1.788455  [32000/50000]\n",
            "loss: 1.773721  [38400/50000]\n",
            "loss: 1.670758  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 38.9%, Avg loss: 1.709592 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.678291  [    0/50000]\n",
            "loss: 1.620398  [ 6400/50000]\n",
            "loss: 1.417396  [12800/50000]\n",
            "loss: 1.715549  [19200/50000]\n",
            "loss: 1.721499  [25600/50000]\n",
            "loss: 1.750999  [32000/50000]\n",
            "loss: 1.723045  [38400/50000]\n",
            "loss: 1.637580  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 38.6%, Avg loss: 1.710461 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.689156  [    0/50000]\n",
            "loss: 1.581641  [ 6400/50000]\n",
            "loss: 1.375366  [12800/50000]\n",
            "loss: 1.708321  [19200/50000]\n",
            "loss: 1.678132  [25600/50000]\n",
            "loss: 1.711470  [32000/50000]\n",
            "loss: 1.684911  [38400/50000]\n",
            "loss: 1.616179  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.724135 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.701730  [    0/50000]\n",
            "loss: 1.543647  [ 6400/50000]\n",
            "loss: 1.340398  [12800/50000]\n",
            "loss: 1.703505  [19200/50000]\n",
            "loss: 1.644573  [25600/50000]\n",
            "loss: 1.678116  [32000/50000]\n",
            "loss: 1.652506  [38400/50000]\n",
            "loss: 1.598854  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.742268 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.713517  [    0/50000]\n",
            "loss: 1.511761  [ 6400/50000]\n",
            "loss: 1.310522  [12800/50000]\n",
            "loss: 1.697483  [19200/50000]\n",
            "loss: 1.613671  [25600/50000]\n",
            "loss: 1.643969  [32000/50000]\n",
            "loss: 1.620442  [38400/50000]\n",
            "loss: 1.578914  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 36.8%, Avg loss: 1.758950 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.729911  [    0/50000]\n",
            "loss: 1.480646  [ 6400/50000]\n",
            "loss: 1.283485  [12800/50000]\n",
            "loss: 1.695598  [19200/50000]\n",
            "loss: 1.586849  [25600/50000]\n",
            "loss: 1.619450  [32000/50000]\n",
            "loss: 1.602862  [38400/50000]\n",
            "loss: 1.563669  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.748041 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model_scratch, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model_scratch, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inferencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Veamos ahora qué tal predice. Cogemos una muestra aletoria del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from random import randint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cogemos una muestra al azahar\n",
        "idx = randint(0, len(train_dataloader)-1)\n",
        "sample_dataloader = next(iter(train_dataloader))\n",
        "sample_dataloader_images, sample_dataloader_labels = sample_dataloader\n",
        "idx = randint(0, len(sample_dataloader_images)-1)\n",
        "sample_image = sample_dataloader_images[idx]\n",
        "sample_label = sample_dataloader_labels[idx]\n",
        "\n",
        "labels_map = {\n",
        "    0: \"airplane\",\n",
        "    1: \"automobile\",\n",
        "    2: \"bird\",\n",
        "    3: \"cat\",\n",
        "    4: \"deer\",\n",
        "    5: \"dog\",\n",
        "    6: \"frog\",\n",
        "    7: \"horse\",\n",
        "    8: \"ship\",\n",
        "    9: \"truck\",\n",
        "}\n",
        "\n",
        "figure = plt.figure(figsize=(5, 5))\n",
        "plt.title(labels_map[sample_label.item()])\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.imshow(sample_image.permute(1,2,0).squeeze())  \n",
        "# permute cambia el orden de las dimensiones, ya que las\n",
        "# imágenes tienen tamaño 3x32x32, pero necesitamos que\n",
        "# las dimensiones sean 32x32x3 para que matplotlib las pinte\n",
        "# así que se permutan poniendo primero las dos de 32 y \n",
        "# la última la de 3\n",
        "\n",
        "# squeeze elimina todas las dimensiones 1 de un tensor, \n",
        "# si se le mete un tensor de dimensiones (Ax1xBxCx1xD) \n",
        "# devuelve un tensor de dimensiones (AxBxCxD)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se la pasamos a la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La red cree que en la imagen hay ship\n"
          ]
        }
      ],
      "source": [
        "# Le pasamos la imagen a la red neuronal creada desde cero\n",
        "model_scratch.to(\"cpu\")\n",
        "logits_scratch = model_scratch(sample_dataloader_images)\n",
        "\n",
        "# La red ha devuelto 64 logits, por lo que nos quedamos con el \n",
        "# número idx que es el que se ha representado antes\n",
        "probs_scratch = logits_scratch.softmax(dim=1)\n",
        "label = probs_scratch[idx].argmax().item()\n",
        "print(f\"La red cree que en la imagen hay {labels_map[label]}\")"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "2da05e9852e3725e6cd29dd2f3d6ebaa07dda6697715ddc2b5ea77aa5959f695"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
