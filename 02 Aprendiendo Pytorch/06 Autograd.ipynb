{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Autograd"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos visto que para entrenar una red hay que calcular su función de pérdida (error entre la salida de la red y la etiqueta) y minimizar esta función de pérdida. Para minimizarla usamos los gradientes de la función de pérdida con respecto a los paŕametros de la red (ya que son estos los que podemos cambiar y harán que se minimice la función de pérdida).\n",
        "\n",
        "Si la red consiste en $z = wx + b$ y el error se calcula como $loss = (y-z)^2$ los gradientes se calculan mediante:\n",
        "\n",
        "$$\\frac{\\partial{loss}}{\\partial{w}} = 2(y-z)\\frac{\\partial{z}}{\\partial{w}} = 2(y-z)x$$\n",
        "$$\\frac{\\partial{loss}}{\\partial{b}} = 2(y-z)\\frac{\\partial{z}}{\\partial{b}} = 2(y-z)$$\n",
        "\n",
        "Pero si ahora la red cambia a $z = wx^2 + b$ el primer gradiente cambia a:\n",
        "\n",
        "$$\\frac{\\partial{loss}}{\\partial{w}} = 2(y-z)\\frac{\\partial{z}}{\\partial{w}} = 2(y-z)x^2$$\n",
        "\n",
        "Por tanto tiene que haber una manera automática de poder calcular los gradientes sin tener que calcularlos a mano para cada problema en particular"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pytorch resuelve esto mediante **Autograd**. Mediante `torch.autograd` Pytorch guarda las operaciones de las redes en un gráfico computacional\n",
        "\n",
        "En el caso de la red\n",
        "\n",
        "$$z = wx + b$$\n",
        "\n",
        "Pytorch guarda el siguiente gráfico computacional"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align:center;\">\n",
        "  <img src=\"https://pytorch.org/tutorials/_images/comp-graph.png\" alt=\"comp-graph\"> <!-- style=\"width:425px;height:626px;\"> -->\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En esta red, $w$ y $b$ son parámetros, que necesitamos optimizar. Por lo tanto, necesitamos poder calcular los gradientes de la función de pérdida con respecto a esas variables. Para hacer eso, establecemos la propiedad ``requires_grad`` de esos tensores.\n",
        "\n",
        " > **Nota**: Puede establecer el valor de ``requires_grad`` al crear un tensor, o más tarde mediante el método ``x.requires_grad_(True)``."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Veamos cómo lo hace Pytorch\n",
        "\n",
        "Definimos una entrada y su etiqueta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: tensor([-0.6014])\n",
            "y: tensor([-1.0122])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "seed = 1\n",
        "\n",
        "torch.manual_seed(10*seed)\n",
        "\n",
        "x = torch.randn(1)  # input tensor\n",
        "y = torch.randn(1)  # expected output\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y: {y}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos los parámetros $w$ y $b$ aleatoriamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w: tensor([0.6614], requires_grad=True)\n",
            "b: tensor([0.2669], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(seed)\n",
        "w = torch.randn(1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "print(f\"w: {w}\")\n",
        "print(f\"b: {b}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por último definimos la salida y su función de pérdida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z: tensor([-0.1308], grad_fn=<AddBackward0>)\n",
            "Loss: 0.776868462562561\n"
          ]
        }
      ],
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "loss = torch.nn.functional.mse_loss(z, y, reduction='mean')\n",
        "\n",
        "print(f\"z: {z}\")\n",
        "print(f\"Loss: {loss}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculemos a mano la salida y la función de pérdida para ver que está todo bien\n",
        "\n",
        "$$z = wx + b$$\n",
        "\n",
        "$$z = 0.6614·(-0.6014) + 0.2669 = -0.1309$$\n",
        "\n",
        "Ahora la función de pérdida\n",
        "\n",
        "$$loss = \\frac{\\sum_{i=1}^{N} \\left(z-y\\right)^2}{N}$$\n",
        "$$loss = \\frac{\\left(-0.1309-(-1.0122)\\right)^2}{1} = 0.7766$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ya tenemos la salida de la red y su fucnión de pérdida, para calcular los gradientes necesitamos calcular las derivadas de esta con respecto a $w$ y $b$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para hacer esto en Pytorch simplemente tenemos que llamar al método `loss.backward()`, que calcula las derivadas de la función de pérdida hacia atrás\n",
        "\n",
        "En el gráfico computacional cada nodo es una variable o parámetro y cada flecha es su salida, por lo que se calcula la derivada de cada salida con respecto las entradas o parámetros en función de las operaciones.\n",
        "\n",
        "Una vez hemos hecho esto obtenemos las derivadas parciales (o gradientes) de la función de pérdida con respecto $w$ y $b$ llamando a `w.grad` y `b.grad`. Aquí Pytorch lo que hará será calcular estas derivadas parciales mediante la regla de la cadena, ya que tiene calculadas todas las derivadas en su gráfico computacional. Es decir realiza\n",
        "\n",
        "$$\\frac{\\partial{loss}}{\\partial{w}} = \\frac{\\partial{loss}}{\\partial{z}}·\\frac{\\partial{z}}{\\partial{w}}$$\n",
        "$$\\frac{\\partial{loss}}{\\partial{b}} = \\frac{\\partial{loss}}{\\partial{z}}·\\frac{\\partial{z}}{\\partial{b}}$$\n",
        "\n",
        "Lo ejecutamos para ver que da"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gradiente de w: tensor([-1.0601])\n",
            "gradiente de b: tensor([1.7628])\n"
          ]
        }
      ],
      "source": [
        "loss.backward()\n",
        "print(f\"gradiente de w: {w.grad}\")\n",
        "print(f\"gradiente de b: {b.grad}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a calcularlo a mano para ver que obtenemos lo mismo\n",
        "\n",
        "Gradiente con respecto a $w$\n",
        "\n",
        "$$\\frac{\\partial{loss}}{\\partial{w}} = \\frac{\\partial{loss}}{\\partial{z}}·\\frac{\\partial{z}}{\\partial{w}}$$\n",
        "\n",
        "$$\\frac{\\partial{loss}}{\\partial{w}} = \\frac{2}{N}\\left(\\sum_{i=1}^{N} \\left(z-y\\right)\\right)·x$$\n",
        "\n",
        "$$\\frac{\\partial{loss}}{\\partial{w}} = 2\\left(-0.1309-(-1.0122)\\right)·(-0.6014) = -1.4793$$\n",
        "\n",
        "\n",
        "Gradiente con respecto a $b$\n",
        "\n",
        "\n",
        "$$\\frac{\\partial{loss}}{\\partial{b}} = \\frac{\\partial{loss}}{\\partial{z}}·\\frac{\\partial{z}}{\\partial{b}}$$\n",
        "$$\\frac{\\partial{loss}}{\\partial{b}} = \\frac{2}{N}\\left(\\sum_{i=1}^{N} \\left(z-y\\right)\\right)·1$$\n",
        "$$\\frac{\\partial{loss}}{\\partial{b}} = 2\\left(-0.1309-(-1.0122)\\right) = 1.7626$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se obtienen resultados similares.\n",
        "\n",
        " > Nota: No salen los mismos resultados, porque al hacer los cálculos a mano he redondeado, por lo que introduce errores"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como dijimos, si ahora la red cambia a $z = wx^2 + b$, habría que volver a hacer las derivadas y programar los cálculos de estas. Gracias a Autograd, llamando al método `loss.backgrd()` se realizan todas las derivadas gracias al gráfico computacional y ya solo nos falta obtener los gradientes mediante `w.grad` y `b.grad`"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d1c24abb23a313e1f9ae042292cd8e6e3c60c5818227ced3d46e3df2c65171ef"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
