{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Todo junto"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Volvemos a crear una red para entrenarla en el conjunto de datos `CIFAR-10` para hacer un resumen de lo que hemos aprendido y reforzar los conocimientos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datasets, Dataloaders y transformaciones"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Necesitamos crear un dataset, podemos hacerlo descargándo el conjunto de datos de Pytorch. O también podemos crear el dataset desde cero\n",
        "\n",
        "Una vez tenemos el dataset, podemos dividirlo en batches para el entrenamiento.\n",
        "\n",
        "Además a la hora de crear el dataset, podemos hacer transformaciones de los datos que nos vengan bien para el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BS = 64\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=BS, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BS, shuffle=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Red neuronal (modelo)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos e instanciamos nuestra red neuronal. Podemos crearla desde cero o meadiante transfer learning.\n",
        "\n",
        "Si tenemos una GPU podemos llevarnos la red a la GPU para que el entrenamiento sea más rápido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "# Creamos la red neuronal desde cero\n",
        "class NeuralNetworkFromScratch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetworkFromScratch, self).__init__()   # Se inicializa el módulo nn.Module\n",
        "        self.flatten = nn.Flatten()             # Se crea una primera capa que aplana la imagen de entrada\n",
        "        self.linear_relu_stack = nn.Sequential( # Se crea una módulo de arquitectura secuencial:\n",
        "            nn.Linear(3*32*32, 512),                # Se añade una primera capa lineal que está preparada \n",
        "                                                    # para que le entre un vector de 3x32x32 (3072)\n",
        "                                                    # y sacará un vector de 512\n",
        "            nn.ReLU(),                              # Se añade una no linealidad\n",
        "            nn.Linear(512, 512),                    # Se añade una segunda capa lineal que le entran 512 \n",
        "                                                    # datos y saca 512 datos\n",
        "            nn.ReLU(),                              # Se añade una no linealidad\n",
        "            nn.Linear(512, 10)                      # Se añade una tercera capa lineal que le entran 512 \n",
        "                                                    # datos y saca un array de tamaño 10 (el número\n",
        "                                                    # de etiquetas)\n",
        "        )\n",
        "        #self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)                         # Se pasa la imagen por la capa de aplanado\n",
        "        logits = self.linear_relu_stack(x)          # Se pasa el vector resultante por la red\n",
        "        #probs = self.softmax(logits)\n",
        "        return logits\n",
        "\n",
        "model_scratch = NeuralNetworkFromScratch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "NeuralNetworkFromScratch(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear_relu_stack): Sequential(\n",
              "    (0): Linear(in_features=3072, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "model_scratch.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Función de pérdida y optimizador"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos una función de pérdida y un optimizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "LR = 1e-2\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_scratch.parameters(), lr=LR)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ciclo de entrenamiento"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Entrenamos la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # X and y to device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            # X and y to device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            \n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.306402  [    0/50000]\n",
            "loss: 2.259025  [ 6400/50000]\n",
            "loss: 2.174863  [12800/50000]\n",
            "loss: 2.117852  [19200/50000]\n",
            "loss: 2.023432  [25600/50000]\n",
            "loss: 2.031960  [32000/50000]\n",
            "loss: 1.988318  [38400/50000]\n",
            "loss: 2.056397  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 28.1%, Avg loss: 1.965780 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.985560  [    0/50000]\n",
            "loss: 1.929681  [ 6400/50000]\n",
            "loss: 1.771994  [12800/50000]\n",
            "loss: 1.947874  [19200/50000]\n",
            "loss: 1.875200  [25600/50000]\n",
            "loss: 1.662335  [32000/50000]\n",
            "loss: 1.626034  [38400/50000]\n",
            "loss: 1.723350  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.846561 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.876759  [    0/50000]\n",
            "loss: 1.929100  [ 6400/50000]\n",
            "loss: 1.805240  [12800/50000]\n",
            "loss: 1.840645  [19200/50000]\n",
            "loss: 1.705923  [25600/50000]\n",
            "loss: 1.781176  [32000/50000]\n",
            "loss: 1.817811  [38400/50000]\n",
            "loss: 1.664451  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.807358 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.055019  [    0/50000]\n",
            "loss: 1.925963  [ 6400/50000]\n",
            "loss: 1.655501  [12800/50000]\n",
            "loss: 1.697683  [19200/50000]\n",
            "loss: 1.884704  [25600/50000]\n",
            "loss: 1.614487  [32000/50000]\n",
            "loss: 1.754172  [38400/50000]\n",
            "loss: 1.694825  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.749899 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.628714  [    0/50000]\n",
            "loss: 1.829815  [ 6400/50000]\n",
            "loss: 1.741920  [12800/50000]\n",
            "loss: 1.651102  [19200/50000]\n",
            "loss: 1.596733  [25600/50000]\n",
            "loss: 1.661462  [32000/50000]\n",
            "loss: 1.548611  [38400/50000]\n",
            "loss: 1.733263  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 39.8%, Avg loss: 1.696213 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.644011  [    0/50000]\n",
            "loss: 1.701767  [ 6400/50000]\n",
            "loss: 1.747609  [12800/50000]\n",
            "loss: 1.621609  [19200/50000]\n",
            "loss: 1.636948  [25600/50000]\n",
            "loss: 1.558566  [32000/50000]\n",
            "loss: 1.948719  [38400/50000]\n",
            "loss: 1.917163  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.730358 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.986383  [    0/50000]\n",
            "loss: 1.678196  [ 6400/50000]\n",
            "loss: 1.736730  [12800/50000]\n",
            "loss: 1.724779  [19200/50000]\n",
            "loss: 1.723107  [25600/50000]\n",
            "loss: 1.430659  [32000/50000]\n",
            "loss: 1.775043  [38400/50000]\n",
            "loss: 1.622715  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 39.5%, Avg loss: 1.705087 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.497827  [    0/50000]\n",
            "loss: 1.798438  [ 6400/50000]\n",
            "loss: 1.503389  [12800/50000]\n",
            "loss: 1.594225  [19200/50000]\n",
            "loss: 1.641100  [25600/50000]\n",
            "loss: 1.813148  [32000/50000]\n",
            "loss: 1.424981  [38400/50000]\n",
            "loss: 1.454917  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 43.2%, Avg loss: 1.593194 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.533015  [    0/50000]\n",
            "loss: 1.358321  [ 6400/50000]\n",
            "loss: 1.558413  [12800/50000]\n",
            "loss: 1.619255  [19200/50000]\n",
            "loss: 1.714670  [25600/50000]\n",
            "loss: 1.596634  [32000/50000]\n",
            "loss: 1.583645  [38400/50000]\n",
            "loss: 1.531806  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 44.2%, Avg loss: 1.574537 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.594965  [    0/50000]\n",
            "loss: 1.714708  [ 6400/50000]\n",
            "loss: 1.489157  [12800/50000]\n",
            "loss: 1.487188  [19200/50000]\n",
            "loss: 1.389452  [25600/50000]\n",
            "loss: 1.455378  [32000/50000]\n",
            "loss: 1.472629  [38400/50000]\n",
            "loss: 1.462139  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 41.3%, Avg loss: 1.621091 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model_scratch, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model_scratch, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Guardar o exportar el modelo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos guardar o exportar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.onnx as onnx\n",
        "\n",
        "# Pesos\n",
        "path = \"data/pesos.pth\"\n",
        "torch.save(model_scratch.state_dict(), path)\n",
        "\n",
        "# Red\n",
        "path = \"data/modelo.pth\"\n",
        "torch.save(model_scratch, path)\n",
        "\n",
        "# Zip\n",
        "path = \"data/modelo.zip\"\n",
        "torch.jit.save(torch.jit.script(model_scratch.cpu()), path)\n",
        "\n",
        "# ONNX\n",
        "path = \"data/modelo.onnx\"\n",
        "batch = 8\n",
        "input_image = torch.rand((batch,3,32,32))\n",
        "model_scratch.to('cpu')\n",
        "onnx.export(model_scratch, input_image, 'model.onnx')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cargar el modelo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos cargar el modelo guardado previamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import load\n",
        "\n",
        "# Pesos\n",
        "path = \"data/pesos.pth\"\n",
        "model_scratch.load_state_dict(load(path))\n",
        "\n",
        "# Red\n",
        "path = \"data/modelo.pth\"\n",
        "model_scratch = load(path)\n",
        "\n",
        "# Zip\n",
        "path = \"data/modelo.zip\"\n",
        "model_scratch = torch.jit.load(path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inferencia"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez tenemos nuestra red entrenada la podemos usar para hacer predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cogemos una muestra al azahar\n",
        "sample_dataloader = next(iter(train_dataloader))\n",
        "sample_dataloader_images, sample_dataloader_labels = sample_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f64ac56fa90>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from random import randint, seed\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generamos una semilla fija para poder repetir el ejemplo\n",
        "seed(10)\n",
        "\n",
        "idx = randint(0, len(sample_dataloader_images)-1)\n",
        "sample_image = sample_dataloader_images[idx]\n",
        "sample_label = sample_dataloader_labels[idx]\n",
        "\n",
        "labels_map = {\n",
        "    0: \"airplane\",\n",
        "    1: \"automobile\",\n",
        "    2: \"bird\",\n",
        "    3: \"cat\",\n",
        "    4: \"deer\",\n",
        "    5: \"dog\",\n",
        "    6: \"frog\",\n",
        "    7: \"horse\",\n",
        "    8: \"ship\",\n",
        "    9: \"truck\",\n",
        "}\n",
        "\n",
        "figure = plt.figure(figsize=(5, 5))\n",
        "plt.title(labels_map[sample_label.item()])\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.imshow(sample_image.permute(1,2,0).squeeze())  \n",
        "# permute cambia el orden de las dimensiones, ya que las\n",
        "# imágenes tienen tamaño 3x32x32, pero necesitamos que\n",
        "# las dimensiones sean 32x32x3 para que matplotlib las pinte\n",
        "# así que se permutan poniendo primero las dos de 32 y \n",
        "# la última la de 3\n",
        "\n",
        "# squeeze elimina todas las dimensiones 1 de un tensor, \n",
        "# si se le mete un tensor de dimensiones (Ax1xBxCx1xD) \n",
        "# devuelve un tensor de dimensiones (AxBxCxD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La red cree que en la imagen hay deer\n"
          ]
        }
      ],
      "source": [
        "# Le pasamos la imagen a la red neuronal creada desde cero\n",
        "model_scratch.to(\"cpu\")\n",
        "logits_scratch = model_scratch(sample_dataloader_images)\n",
        "\n",
        "# La red ha devuelto 64 logits, por lo que nos quedamos con el \n",
        "# número idx que es el que se ha representado antes\n",
        "probs_scratch = logits_scratch.softmax(dim=1)\n",
        "label = probs_scratch[idx].argmax().item()\n",
        "print(f\"La red cree que en la imagen hay {labels_map[label]}\")"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d1c24abb23a313e1f9ae042292cd8e6e3c60c5818227ced3d46e3df2c65171ef"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
