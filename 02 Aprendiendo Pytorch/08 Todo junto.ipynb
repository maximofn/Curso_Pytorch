{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Todo junto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Volvemos a crear una red para entrenarla en el conjunto de datos `CIFAR-10` para hacer un resumen de lo que hemos aprendido y reforzar los conocimientos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datasets, Dataloaders y transformaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Necesitamos crear un dataset, podemos hacerlo descargándo el conjunto de datos de Pytorch. O también podemos crear el dataset desde cero\n",
        "\n",
        "Una vez tenemos el dataset, podemos dividirlo en batches para el entrenamiento.\n",
        "\n",
        "Además a la hora de crear el dataset, podemos hacer transformaciones de los datos que nos vengan bien para el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BS = 64\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=BS, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BS, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Red neuronal (modelo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos e instanciamos nuestra red neuronal. Podemos crearla desde cero o meadiante transfer learning.\n",
        "\n",
        "Si tenemos una GPU podemos llevarnos la red a la GPU para que el entrenamiento sea más rápido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "# Creamos la red neuronal desde cero\n",
        "class NeuralNetworkFromScratch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetworkFromScratch, self).__init__()   # Se inicializa el módulo nn.Module\n",
        "        self.flatten = nn.Flatten()             # Se crea una primera capa que aplana la imagen de entrada\n",
        "        self.linear_relu_stack = nn.Sequential( # Se crea una módulo de arquitectura secuencial:\n",
        "            nn.Linear(3*32*32, 512),                # Se añade una primera capa lineal que está preparada \n",
        "                                                    # para que le entre un vector de 28*28 (784)\n",
        "                                                    # y sacará un vector de 512\n",
        "            nn.ReLU(),                              # Se añade una no linealidad\n",
        "            nn.Linear(512, 512),                    # Se añade una segunda capa lineal que le entran 512 \n",
        "                                                    # datos y saca 512 datos\n",
        "            nn.ReLU(),                              # Se añade una no linealidad\n",
        "            nn.Linear(512, 10)                      # Se añade una tercera capa lineal que le entran 512 \n",
        "                                                    # datos y saca un array de tamaño 10 (el número\n",
        "                                                    # de etiquetas)\n",
        "        )\n",
        "        #self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)                         # Se pasa la imagen por la capa de aplanado\n",
        "        logits = self.linear_relu_stack(x)          # Se pasa el vector resultante por la red\n",
        "        #probs = self.softmax(logits)\n",
        "        return logits\n",
        "\n",
        "model_scratch = NeuralNetworkFromScratch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "NeuralNetworkFromScratch(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear_relu_stack): Sequential(\n",
              "    (0): Linear(in_features=3072, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "model_scratch.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Función de pérdida y optimizador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos una función de pérdida y un optimizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "LR = 1e-2\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_scratch.parameters(), lr=LR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ciclo de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Entrenamos la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # X and y to device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            # X and y to device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            \n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.292881  [    0/50000]\n",
            "loss: 2.253162  [ 6400/50000]\n",
            "loss: 2.254493  [12800/50000]\n",
            "loss: 2.131873  [19200/50000]\n",
            "loss: 2.044054  [25600/50000]\n",
            "loss: 2.040840  [32000/50000]\n",
            "loss: 1.946755  [38400/50000]\n",
            "loss: 1.903936  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 30.6%, Avg loss: 1.949836 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.012805  [    0/50000]\n",
            "loss: 2.038391  [ 6400/50000]\n",
            "loss: 2.039904  [12800/50000]\n",
            "loss: 1.918222  [19200/50000]\n",
            "loss: 1.917239  [25600/50000]\n",
            "loss: 1.964177  [32000/50000]\n",
            "loss: 1.927772  [38400/50000]\n",
            "loss: 1.857523  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 34.9%, Avg loss: 1.843790 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.765827  [    0/50000]\n",
            "loss: 1.691749  [ 6400/50000]\n",
            "loss: 1.787539  [12800/50000]\n",
            "loss: 1.778993  [19200/50000]\n",
            "loss: 2.037781  [25600/50000]\n",
            "loss: 1.752187  [32000/50000]\n",
            "loss: 1.784580  [38400/50000]\n",
            "loss: 1.633242  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 31.1%, Avg loss: 1.931956 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.846174  [    0/50000]\n",
            "loss: 1.872676  [ 6400/50000]\n",
            "loss: 1.809991  [12800/50000]\n",
            "loss: 1.873386  [19200/50000]\n",
            "loss: 1.829244  [25600/50000]\n",
            "loss: 1.743187  [32000/50000]\n",
            "loss: 1.808505  [38400/50000]\n",
            "loss: 1.635011  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 38.8%, Avg loss: 1.726842 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.600091  [    0/50000]\n",
            "loss: 1.714626  [ 6400/50000]\n",
            "loss: 1.657216  [12800/50000]\n",
            "loss: 1.688834  [19200/50000]\n",
            "loss: 1.752804  [25600/50000]\n",
            "loss: 1.552619  [32000/50000]\n",
            "loss: 1.632835  [38400/50000]\n",
            "loss: 1.930504  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 39.8%, Avg loss: 1.679429 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.471980  [    0/50000]\n",
            "loss: 1.538952  [ 6400/50000]\n",
            "loss: 1.651030  [12800/50000]\n",
            "loss: 1.507904  [19200/50000]\n",
            "loss: 1.751692  [25600/50000]\n",
            "loss: 1.686795  [32000/50000]\n",
            "loss: 1.587971  [38400/50000]\n",
            "loss: 1.543423  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 42.4%, Avg loss: 1.621927 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.593557  [    0/50000]\n",
            "loss: 1.749055  [ 6400/50000]\n",
            "loss: 1.583334  [12800/50000]\n",
            "loss: 1.547219  [19200/50000]\n",
            "loss: 1.683659  [25600/50000]\n",
            "loss: 1.660721  [32000/50000]\n",
            "loss: 1.444562  [38400/50000]\n",
            "loss: 1.567297  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 42.8%, Avg loss: 1.610994 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.545299  [    0/50000]\n",
            "loss: 1.656602  [ 6400/50000]\n",
            "loss: 1.737031  [12800/50000]\n",
            "loss: 1.829681  [19200/50000]\n",
            "loss: 1.548696  [25600/50000]\n",
            "loss: 1.762576  [32000/50000]\n",
            "loss: 1.790601  [38400/50000]\n",
            "loss: 1.733546  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 44.1%, Avg loss: 1.595892 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.576528  [    0/50000]\n",
            "loss: 1.415443  [ 6400/50000]\n",
            "loss: 1.397622  [12800/50000]\n",
            "loss: 1.572268  [19200/50000]\n",
            "loss: 1.330171  [25600/50000]\n",
            "loss: 1.531623  [32000/50000]\n",
            "loss: 1.493511  [38400/50000]\n",
            "loss: 1.726095  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 40.1%, Avg loss: 1.675446 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.634020  [    0/50000]\n",
            "loss: 1.545090  [ 6400/50000]\n",
            "loss: 1.624678  [12800/50000]\n",
            "loss: 1.325535  [19200/50000]\n",
            "loss: 1.581432  [25600/50000]\n",
            "loss: 1.573768  [32000/50000]\n",
            "loss: 1.373697  [38400/50000]\n",
            "loss: 1.441515  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.6%, Avg loss: 1.539085 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model_scratch, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model_scratch, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Guardar o exportar el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos guardar o exportar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.onnx as onnx\n",
        "\n",
        "# Pesos\n",
        "path = \"data/pesos.pth\"\n",
        "torch.save(model_scratch.state_dict(), path)\n",
        "\n",
        "# Red\n",
        "path = \"data/modelo.pth\"\n",
        "torch.save(model_scratch, path)\n",
        "\n",
        "# Zip\n",
        "path = \"data/modelo.zip\"\n",
        "torch.jit.save(torch.jit.script(model_scratch.cpu()), path)\n",
        "\n",
        "# ONNX\n",
        "path = \"data/modelo.onnx\"\n",
        "batch = 8\n",
        "input_image = torch.rand((batch,3,32,32))\n",
        "model_scratch.to('cpu')\n",
        "onnx.export(model_scratch, input_image, 'model.onnx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cargar el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podmeos cargar el modelo guardado previamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import load\n",
        "\n",
        "# Pesos\n",
        "path = \"data/pesos.pth\"\n",
        "model_scratch.load_state_dict(load(path))\n",
        "\n",
        "# Red\n",
        "path = \"data/modelo.pth\"\n",
        "model_scratch = load(path)\n",
        "\n",
        "# Zip\n",
        "path = \"data/modelo.zip\"\n",
        "model_scratch = torch.jit.load(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inferencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez tenemos nuestra red entrenada la podemos usar para hacer predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1fd4648340>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from random import randint, seed\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generamos una semilla fija para poder repetir el ejemplo\n",
        "seed(10)\n",
        "\n",
        "# Cogemos una muestra al azahar\n",
        "idx = randint(0, len(train_dataloader)-1)\n",
        "sample_dataloader = next(iter(train_dataloader))\n",
        "sample_dataloader_images, sample_dataloader_labels = sample_dataloader\n",
        "idx = randint(0, len(sample_dataloader_images)-1)\n",
        "sample_image = sample_dataloader_images[idx]\n",
        "sample_label = sample_dataloader_labels[idx]\n",
        "\n",
        "labels_map = {\n",
        "    0: \"airplane\",\n",
        "    1: \"automobile\",\n",
        "    2: \"bird\",\n",
        "    3: \"cat\",\n",
        "    4: \"deer\",\n",
        "    5: \"dog\",\n",
        "    6: \"frog\",\n",
        "    7: \"horse\",\n",
        "    8: \"ship\",\n",
        "    9: \"truck\",\n",
        "}\n",
        "\n",
        "figure = plt.figure(figsize=(5, 5))\n",
        "plt.title(labels_map[sample_label.item()])\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.imshow(sample_image.permute(1,2,0).squeeze())  \n",
        "# permute cambia el orden de las dimensiones, ya que las\n",
        "# imágenes tienen tamaño 3x32x32, pero necesitamos que\n",
        "# las dimensiones sean 32x32x3 para que matplotlib las pinte\n",
        "# así que se permutan poniendo primero las dos de 32 y \n",
        "# la última la de 3\n",
        "\n",
        "# squeeze elimina todas las dimensiones 1 de un tensor, \n",
        "# si se le mete un tensor de dimensiones (Ax1xBxCx1xD) \n",
        "# devuelve un tensor de dimensiones (AxBxCxD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La red cree que en la imagen hay dog\n"
          ]
        }
      ],
      "source": [
        "# Le pasamos la imagen a la red neuronal creada desde cero\n",
        "model_scratch.to(\"cpu\")\n",
        "logits_scratch = model_scratch(sample_dataloader_images)\n",
        "\n",
        "# La red ha devuelto 64 logits, por lo que nos quedamos con el \n",
        "# número idx que es el que se ha representado antes\n",
        "probs_scratch = logits_scratch.softmax(dim=1)\n",
        "label = probs_scratch[idx].argmax().item()\n",
        "print(f\"La red cree que en la imagen hay {labels_map[label]}\")"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d1c24abb23a313e1f9ae042292cd8e6e3c60c5818227ced3d46e3df2c65171ef"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
