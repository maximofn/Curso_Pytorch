{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Todo junto"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Volvemos a crear una red para entrenarla en el conjunto de datos `CIFAR-10` para hacer un resumen de lo que hemos aprendido y reforzar los conocimientos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datasets, Dataloaders y transformaciones"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Necesitamos crear un dataset, podemos hacerlo descargándo el conjunto de datos de Pytorch. O también podemos crear el dataset desde cero\n",
        "\n",
        "Una vez tenemos el dataset, podemos dividirlo en batches para el entrenamiento.\n",
        "\n",
        "Además a la hora de crear el dataset, podemos hacer transformaciones de los datos que nos vengan bien para el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BS = 64\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=BS, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BS, shuffle=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Red neuronal (modelo)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos e instanciamos nuestra red neuronal. Podemos crearla desde cero o meadiante transfer learning.\n",
        "\n",
        "Si tenemos una GPU podemos llevarnos la red a la GPU para que el entrenamiento sea más rápido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "# Creamos la red neuronal desde cero\n",
        "class NeuralNetworkFromScratch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetworkFromScratch, self).__init__()   # Se inicializa el módulo nn.Module\n",
        "        self.flatten = nn.Flatten()             # Se crea una primera capa que aplana la imagen de entrada\n",
        "        self.linear_relu_stack = nn.Sequential( # Se crea una módulo de arquitectura secuencial:\n",
        "            nn.Linear(3*32*32, 512),                # Se añade una primera capa lineal que está preparada \n",
        "                                                    # para que le entre un vector de 3x32x32 (3072)\n",
        "                                                    # y sacará un vector de 512\n",
        "            nn.ReLU(),                              # Se añade una no linealidad\n",
        "            nn.Linear(512, 512),                    # Se añade una segunda capa lineal que le entran 512 \n",
        "                                                    # datos y saca 512 datos\n",
        "            nn.ReLU(),                              # Se añade una no linealidad\n",
        "            nn.Linear(512, 10)                      # Se añade una tercera capa lineal que le entran 512 \n",
        "                                                    # datos y saca un array de tamaño 10 (el número\n",
        "                                                    # de etiquetas)\n",
        "        )\n",
        "        #self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)                         # Se pasa la imagen por la capa de aplanado\n",
        "        logits = self.linear_relu_stack(x)          # Se pasa el vector resultante por la red\n",
        "        #probs = self.softmax(logits)\n",
        "        return logits\n",
        "\n",
        "model_scratch = NeuralNetworkFromScratch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "NeuralNetworkFromScratch(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear_relu_stack): Sequential(\n",
              "    (0): Linear(in_features=3072, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "model_scratch.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Función de pérdida y optimizador"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos una función de pérdida y un optimizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "LR = 1e-2\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_scratch.parameters(), lr=LR)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ciclo de entrenamiento"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Entrenamos la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # X and y to device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            # X and y to device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            \n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.297105  [    0/50000]\n",
            "loss: 2.259459  [ 6400/50000]\n",
            "loss: 2.190343  [12800/50000]\n",
            "loss: 2.113575  [19200/50000]\n",
            "loss: 2.063204  [25600/50000]\n",
            "loss: 1.974222  [32000/50000]\n",
            "loss: 1.969541  [38400/50000]\n",
            "loss: 1.892607  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 30.3%, Avg loss: 1.945082 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.806808  [    0/50000]\n",
            "loss: 1.954223  [ 6400/50000]\n",
            "loss: 1.974031  [12800/50000]\n",
            "loss: 1.898177  [19200/50000]\n",
            "loss: 1.945112  [25600/50000]\n",
            "loss: 1.938679  [32000/50000]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     train_loop(train_dataloader, model_scratch, loss_fn, optimizer)\n\u001b[1;32m      5\u001b[0m     test_loop(test_dataloader, model_scratch, loss_fn)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_loop\u001b[39m(dataloader, model, loss_fn, optimizer):\n\u001b[1;32m      2\u001b[0m     size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m      4\u001b[0m         \u001b[39m# X and y to device\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m         \u001b[39m# Compute prediction and loss\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
            "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torchvision/transforms/functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[0;32m--> 174\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model_scratch, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model_scratch, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Guardar o exportar el modelo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos guardar o exportar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.onnx as onnx\n",
        "\n",
        "# Pesos\n",
        "path = \"data/pesos.pth\"\n",
        "torch.save(model_scratch.state_dict(), path)\n",
        "\n",
        "# Red\n",
        "path = \"data/modelo.pth\"\n",
        "torch.save(model_scratch, path)\n",
        "\n",
        "# Zip\n",
        "path = \"data/modelo.zip\"\n",
        "torch.jit.save(torch.jit.script(model_scratch.cpu()), path)\n",
        "\n",
        "# ONNX\n",
        "path = \"data/modelo.onnx\"\n",
        "batch = 8\n",
        "input_image = torch.rand((batch,3,32,32))\n",
        "model_scratch.to('cpu')\n",
        "onnx.export(model_scratch, input_image, 'model.onnx')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cargar el modelo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podmeos cargar el modelo guardado previamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import load\n",
        "\n",
        "# Pesos\n",
        "path = \"data/pesos.pth\"\n",
        "model_scratch.load_state_dict(load(path))\n",
        "\n",
        "# Red\n",
        "path = \"data/modelo.pth\"\n",
        "model_scratch = load(path)\n",
        "\n",
        "# Zip\n",
        "path = \"data/modelo.zip\"\n",
        "model_scratch = torch.jit.load(path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inferencia"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez tenemos nuestra red entrenada la podemos usar para hacer predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7efac684d030>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAGpCAYAAACqIcDTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbyklEQVR4nO3dWZBdBZ3H8f+5+9LL7S3dSXc6nWAIS4yJBjIOA3EMiA4ZcIbSFFZhWfgwo75gaRUVy4L4oKjli0/j8IA1NVZZLlhYg4gsGSjEoCgMCaAsAbKRdHeSXu9+lnmwTE0XIven/xYm+X6qeCD87sm5y7m/Piw/giRJEgMA4C+UeqtPAABwdqBQAAAuKBQAgAsKBQDggkIBALigUAAALigUAIALCgUA4IJCAQC4oFBw1tmzZ48FQWAnT558q08FOKdQKAAAFxQK8Geo1+tv9SkAbzsUCs5ak5OTdsMNN1hvb68NDw/bTTfdZHNzc2f+eqPRsN27d9vatWstl8vZ6OiofeYzn7HZ2dklx5mYmLCdO3faj370I9uyZYsVCgX70pe+ZGZmP/jBD2zbtm3W29trpVLJ1q1bZzfddNOSx8/Pz9vnP//5Jb/PzTffbNVqddlfA+CvKfNWnwCwXK6//nrbtWuXffKTn7QDBw7Y7t27zczszjvvtCRJ7MMf/rA99NBDtnv3brv88stt//79dtttt9m+ffts3759ls/nzxzrySeftN/+9rf2xS9+0dauXWvlctn27dtnu3btsl27dtmePXusUCjYoUOHbO/evWceV6vVbPv27Xb06FH7whe+YJs2bbJnn33Wbr31Vjtw4IA9+OCDFgTBX/21AZZFApxlbrvttsTMkq9//etLfv3Tn/50UigUkjiOk/vuu++PZr73ve8lZpbccccdZ35tzZo1STqdTp5//vkl2W984xuJmSWzs7NveC633357kkqlkieeeGLJr//whz9MzCy59957/9ynCbzt8Le8cNa69tprl/z5pk2brNFo2NTU1Jm7iE984hNLMh/5yEesXC7bQw899LrHnn/++Ut+7ZJLLjEzs49+9KP2/e9/344dO/a6c7jnnnts48aNtnnzZgvD8MwfV199tQVBYA8//PBf+CyBtw8KBWetgYGBJX/+h7+FVa/X7dSpU5bJZGxoaGhJJggCGxkZsVOnTi359ZUrV77u+FdccYXdfffdFoahffzjH7exsTHbuHGjffe73z2TmZyctP3791s2m13yR3d3tyVJwr/ajLMK/wwF56SBgQELw9Cmp6eXlEqSJHbixIkzdx9/8Eb/nOO6666z6667zprNpj3++ON2++2328c+9jGbmJiw9773vTY4OGjFYtHuvPPOP/r4wcFBvycFvMW4Q8E5aceOHWZm9p3vfGfJr991111WrVbP/PVO5fN52759u33ta18zM7OnnnrKzMx27txpBw8etIGBAdu6devr/piYmPjLnwzwNsEdCs5JV111lV199dV2yy232Pz8vF122WVn/i2vLVu22I033vimx7j11lvt6NGjtmPHDhsbG7PZ2Vn75je/adls1rZv325mZjfffLPddddddsUVV9hnP/tZ27Rpk8VxbIcPH7b777/fPve5z9m2bduW++kCfxUUCs5JQRDY3XffbXv27LFvf/vb9uUvf9kGBwftxhtvtK985StL/pXhN7Jt2zb79a9/bbfccotNT09bpVKxrVu32t69e+3iiy82M7NyuWyPPvqoffWrX7U77rjDXnnlFSsWizY+Pm5XXnkldyg4qwRJkiRv9UkAAP7/45+hAABcUCgAABcUCgDABYUCAHBBoQAAXFAoAAAXFAoAwEXH/2Hjk//+KenAUUb7byaDuCnlMxZJ+aaVpfxPfvGClP/xw7+R8v90zRVS/gObV0v5rnBGylug/WxxZFHL/+dPfyXlj53QRhNvuOZSKX/peUNvHvo/cklbypuZhamilI+CnJTPJKGUTyfaNROLn4nE+P+6uBL/C0H1/6sTxbGUf8+//NubZrhDAQC4oFAAAC4oFACACwoFAOCCQgEAuKBQAAAuKBQAgAsKBQDggkIBALigUAAALigUAICLjge34kjbfYlSWj7R4pYEaSlfj7XuPDo9J+VffE3L/8fdP5fyE8PXS/ktE+NSPhJ3g6amZ6X8i5MtKf/M76ak/KoVr0j5C1YNS/nBfFbKm5klobadlaS0fGjaRROr41CJmD/HqK+OunSWiK9/StzyimP/95c7FACACwoFAOCCQgEAuKBQAAAuKBQAgAsKBQDggkIBALigUAAALigUAIALCgUA4IJCAQC46HjLKwm07lGbKh1oO0bZWNuGUnduNp83IuV/+XSflF970RYpv/5v/lHKD42PSnlLae9YuGZSyl/6al3KvzY1L+UXWzkp34oKUj6whpQ3M0uLa09xsMzrUIm6JoU/ZdlfTfE7S8+z5QUAeJuiUAAALigUAIALCgUA4IJCAQC4oFAAAC4oFACACwoFAOCCQgEAuKBQAAAuKBQAgIsgSZKOBl2e+NanpAOnUtpOTCppa3mLpXwcZKX8bFPr2vniOil/6VW7pHx3RdsKO/aatrWVyXQ862ZmZtmMtht06OWDUv7Fg4ek/KpB7fVJnXxeyhfrR6S8mVlXXvuMJqbt2cXymJT2mQ6Wf60Kf4K6tKXuFUax9vm85F+/9ebnIB0RAIA3QKEAAFxQKAAAFxQKAMAFhQIAcEGhAABcUCgAABcUCgDABYUCAHBBoQAAXFAoAAAXnQ84dTb5dUZaXKKJkrSUD1MFKd+IxR2jUlnKX37lB6V8ZXRMyjfqNSk/vKJfyoehtiOVzWrbaOvWb5DyXZVBKd9eXJTyYaC9nnOvTkt5M7P2woyU7ylr10Ag/jiYBOo6FFte0HCHAgBwQaEAAFxQKAAAFxQKAMAFhQIAcEGhAABcUCgAABcUCgDABYUCAHBBoQAAXFAoAAAXnW95iTtAgbj9FYjd1o5zUj5b0bazJi68RMoX+1ZL+XYYS/l0Vtsui1vaNlcrDLXjB9rxg3TnHzUzs1KxJOWPnTgu5du1ppQv9Q9LeTOzcK6t5U3LZ1PaZ0ie8gJE3KEAAFxQKAAAFxQKAMAFhQIAcEGhAABcUCgAABcUCgDABYUCAHBBoQAAXFAoAAAXFAoAwIUwsCTuBpk4HJSkpfhr0/NSPmxrW1WVjSulvC1oz3dh4YiUHx7WtqRabe39em1yWso/9tjPpfyDDzwg5bsLRSm/+aINUr6Qakn5d64XPw9mZoVeKR4n2mc6SepSPki0z4QFgZY3NY+zDXcoAAAXFAoAwAWFAgBwQaEAAFxQKAAAFxQKAMAFhQIAcEGhAABcUCgAABcUCgDABYUCAHAhbHlpW1VBEkl5udlibZtreuqklD/6mrZtVWlpO0kzJ49J+a6Stm1VrWo7T5PiltfzLxyU8i+89KqUXzeubWdVG9rzHR4fk/L57kEpb2ZWa56W8vVqW8rnhKvXzCwlXmXi8hfAHQoAwAeFAgBwQaEAAFxQKAAAFxQKAMAFhQIAcEGhAABcUCgAABcUCgDABYUCAHBBoQAAXHS8BhSoW16BtrUViMtBI8M9Un54YL2UHxjSjh+atl22ckjbhoqaTS3fbkn5UqEg5Te/a4uUX7v2PCnf060NVaXE7bjR0XVSvtGek/JmZvVQ2+YK2jUpX7a0lE+nc1JevOTNAjGPsw53KAAAFxQKAMAFhQIAcEGhAABcUCgAABcUCgDABYUCAHBBoQAAXFAoAAAXFAoAwAWFAgBwIQwmaUM9UZCV8mGgbTe1Y22XKJPWjh+JO0zNWOvmdlvb5pqZr0r5MNS2rdqhtr22enSllO/pKkr5+fkpKb9q1Sopny/kpXyjoW3NmZkVxH20dKS9RmlxvwxYbtyhAABcUCgAABcUCgDABYUCAHBBoQAAXFAoAAAXFAoAwAWFAgBwQaEAAFxQKAAAFxQKAMDFsm15qdtcTdO2leJsWcr3r9C2p0bHx6V8ta29PtOnT0v5WnVRyh8+8pKUP378mJQPkkTKh+2WlB8dH5byXV3a5yFsajtYuZy2TWdmlrS1xySB9vNdEGjvQZBon1Ez7fjwpV5j4lf0suAOBQDggkIBALigUAAALigUAIALCgUA4IJCAQC4oFAAAC4oFACACwoFAOCCQgEAuKBQAAAuOh7cSsRdnzBVlPJ1K0n5dPeglC909Ur5xYV5KX/w2Ckpv1BrSvlGvSrls9mclO8qaa//c8/sl/Jr14xJ+cG+HikfhW0pn8tp21/ZtPb6mJk1quI2lzzdpG5tqdtf4tGD5R6TenttiyXLPJ6lvp4p8XwSi6V8Z+cAAIADCgUA4IJCAQC4oFAAAC4oFACACwoFAOCCQgEAuKBQAAAuKBQAgAsKBQDggkIBALjoeMsr7jxqZmbHZyMp/6sXD0r5ykpttyY3tCjlX33yZ1L+xz95VMq/+5LtUj6nTXNZ2K5J+YMvPSvljx7W3q/usrYbdGpS24JbOToh5Yt9I1I+XtB3j7JRVsqnInH7K6VdY3GgPgftmg+SZd7aUsfFln37S/sOigPt/Y0TcStMPf4yvD7coQAAXFAoAAAXFAoAwAWFAgBwQaEAAFxQKAAAFxQKAMAFhQIAcEGhAABcUCgAABcUCgDARcdjPQ3TxqRePnREyt//00ek/Hu2VaV8X0nbVbrv3gek/MO/OiDlI3GmZ926cSm/uHBayjea81K+3J2X8tWadvz5eS0/PqGdT6Gg5aO2tmtlZhaJW1uBuK0UiD8PxuoWVhBqefn8NYH6iFjbLkvE80+J55NWz1/cRlOn2qJlmDrjDgUA4IJCAQC4oFAAAC4oFACACwoFAOCCQgEAuKBQAAAuKBQAgAsKBQDggkIBALigUAAALjofKBJ3ZVYN9Ur5DWuGpfyRgy9J+drigpR/4aWDUj5X1LbOjr32qpQfHR2Q8hvfebGUj8OWlJ+f1bbCerq6pPzYKm27rFzSPm9RJO5spdRdK7Mwqkv5rLq1ZWkprW6FJdYW89r5tGMtHwXa/lqm0C3lU5milE8S7TMUt7R9OmsvSvGC+H6lTP9Mv/kxAQBwQKEAAFxQKAAAFxQKAMAFhQIAcEGhAABcUCgAABcUCgDABYUCAHBBoQAAXFAoAAAXHW95FawhHXjtiLatdM2OS6X8A7/YL+VfPnJEyqdyBSlfyWv5YlHbttqyRXt9Bgf6pfzvnn1Wyk8en5PyAxeslPK9PSukvLprZRZL6SDQd4+iWNtHywba8QPTHhDEWj4Wpv7MzFpxVspbaUiKrzxvi5TvG14n5aN0Wco3WjUpv3D6ZSl/8tABKW/VKSmeTara8TvAHQoAwAWFAgBwQaEAAFxQKAAAFxQKAMAFhQIAcEGhAABcUCgAABcUCgDABYUCAHBBoQAAXHQ81pMJtF0iS7Rdn/NGSlJ+5Jp3SfnTM00pX60XpXx2bLOUH3jHJinfVdbOZ2Fe29qqVPqk/IGnn5byv9i3T8oPDGk7T/lubRstqbalfDGVSHkzszjQ9t1apl0zWatL+XQSSfk4yGnH79L211Zvep+UL/avkfJJWtt3SwXa65MRj1+oDEv5nuq8lK81tW2uuK3tM3aCOxQAgAsKBQDggkIBALigUAAALigUAIALCgUA4IJCAQC4oFAAAC4oFACACwoFAOCCQgEAuKBQAAAuOh6HTJK8dOB0onVVJtGGyiplKW6Vrl4pH8YDUr68/kIp3yhpx68uakNxcaQN3VUqPVL+8ssvk/KPPPKwlP+ve+6V8v98/S4pv2KwIuXTGW0I0MysskIbM5w5tiDla21t4DInPoVmoj1g5fiElC8NDEr5Fw8dkfLlsjYYWunSxjzrNe2abLW077h8XhvMraW07+i2+P52gjsUAIALCgUA4IJCAQC4oFAAAC4oFACACwoFAOCCQgEAuKBQAAAuKBQAgAsKBQDggkIBALjoeMvLLCsdOJVoJ5KY9oAoiaV8PRKPn9V2fXKB8FKaWb1el/LtMJTy4stvcaI9oq+vX8rv/IedUv7xX/5Gyv/33kek/Ac/9AEpXxrRttfMzAaHtMf0D6+Q8sdffkbKz85MSnlLi9d8t7bNVY+1z1yqoJ1POqtdkynl69DMEm1KzRbnta22TKLt8WW6KlJ+vjor5TvBHQoAwAWFAgBwQaEAAFxQKAAAFxQKAMAFhQIAcEGhAABcUCgAABcUCgDABYUCAHBBoQAAXHQ8XpMETenAgdhVadO2qsI4kPKW6ZXifSMXSPlsSdu2ChraEFAUabs+rVZLO36oHT+d0d7fvr4+KX/tdR+S8geee07K37/3Z1J++/b3S3kzs8Hhd0v5oaEJKd8/fJGUX5idlvKROFbVNaBteaVLJSm/pkc7fmNhTspPHT4k5U9PattokWnXWLmobZflSxUpH2e0178T3KEAAFxQKAAAFxQKAMAFhQIAcEGhAABcUCgAABcUCgDABYUCAHBBoQAAXFAoAAAXFAoAwEXHW15BkpYOHATa1lYY56V8kq1I+dENfyfliys3SfnpuZqUj+OqlC8UclK+0dB+VlCn0eKUdvxmEkv5IEmk/NatW6X82vO0naennnpKypuZzc1ov8eO9/+9lB/sG5DylYK2Z5fJadd8Oq9tT1lKe48T8TNULnRJ+Vy6KOXjtPadtTh/Wsqrr38uq70+qVKPlO/omO5HBACckygUAIALCgUA4IJCAQC4oFAAAC4oFACACwoFAOCCQgEAuKBQAAAuKBQAgAsKBQDgouMtLxO3udqBtusT57VdmVUbLpXy/edp+bmoLOWDTCTl02mty2vNhpSv17V8FGu7Sq1Iy6dCbWcon+/8o2lmlgq0rbN1a9ZJ+dWrVkt5M7MjR45I+QcfuE/Kr1+/XspfcPFFUj6fiNtWbe0ayGS0rSozLZ/NlqR8/7B2zfcNrZTyzeqslJ+fmZbyi7NTUr6rMijlO8EdCgDABYUCAHBBoQAAXFAoAAAXFAoAwAWFAgBwQaEAAFxQKAAAFxQKAMAFhQIAcEGhAABcdDyYFAbaFpPltF2c/jWbpPzAundL+SjbJ+WTthS3XEbbPSoWuqR8u6m9/um0tm2VFmeV0intAblsQcoXCtoOU7Govf5p8Ql3dWnvl5nZ+Pi4lJ+dm5Xyzz33nJR/7OePSfkNF14g5UdXjUp5cR7QzLQHqHt5Zto1Fov7d/mi9p04VNCu4XxWe30ac9r2Vye4QwEAuKBQAAAuKBQAgAsKBQDggkIBALigUAAALigUAIALCgUA4IJCAQC4oFAAAC4oFACAi463vKJE20rqGTlfyve/4z1Svp7XtrlakRS3OAilfCajbUNl0tq2VSbTlPJ5cUstibXzz2azUr63t1/KD/T1SvmusroVpuXzubyUNzPLF7THVCracx4ZGZHyB19+Wco/88wz2vEPHpTy69dr3xFjY2NSPpPRtq3iRMzH2pdKq63lU+LWWU7cCiuUu7XfoAPcoQAAXFAoAAAXFAoAwAWFAgBwQaEAAFxQKAAAFxQKAMAFhQIAcEGhAABcUCgAABcUCgDARcdbXunSgHTgyqqLpHw7qEj5sK7t4iRJW8rHcUvKp1KxlM9mc1K+VNJ2eqJIOx/1+ClxaCif17azyuWSlO/u1s5f3SLLZDq+VM5Ii49pi+9ZOqM9hw0bNkj5keEVUl7d8nrm6f+R8kcPvSrlV69eLeUHBwelfLlLvGYCbS+v1dS+g8Kmtj8YpPTP9JvhDgUA4IJCAQC4oFAAAC4oFACACwoFAOCCQgEAuKBQAAAuKBQAgAsKBQDggkIBALigUAAALjoec+keHpYOXKiMSPmFmrZjlEtpOze5jHb8eqzt4oShdvwk1rbI9O0sbSusUMhLeXULS82nUurPOuIOVlp7PdMZbYfJzCxJtHOK40TLJ1rexM9cWfwMveuiC6X8O9aMS/nTs7NSfvrEcSl//MhhKd/V3S3le3orUj6X1T5zUXNByrdb2ndcJ7hDAQC4oFAAAC4oFACACwoFAOCCQgEAuKBQAAAuKBQAgAsKBQDggkIBALigUAAALigUAICLjre8sllt66laW5TycdDxqfw+H2tbTO22tnvUjrUdpra45RVop2+ZjPb65PPa+6VSt7YC8QmrO1XtSNyCE3ez2q2GlDcza9Rq2gPE7a9MWnsP1M9QGGmvUbPZlPLqvtv42JiUHxsdlfKNRl3Kz89r33HNVlvKVxe1ba7pE4ekfHtxSspf+LdvnuEOBQDggkIBALigUAAALigUAIALCgUA4IJCAQC4oFAAAC4oFACACwoFAOCCQgEAuKBQAAAuOh73OXzomHTg1oy2JVXs1XZ3+ru6pXw6re0GxZmilM8XxXwuLeVV6tZWLG6XRWEk5VvtlpSv1cXtrEDbnRKn1CxqaTtPZmYzU5NSvlGdl/LlonaNdff2S/lsSbvG6g1ty6vd1ratwjCU8lGkfUaL4jU8ODgo5RPxUxdF2jXTXdSu+eqM9p3YCe5QAAAuKBQAgAsKBQDggkIBALigUAAALigUAIALCgUA4IJCAQC4oFAAAC4oFACACwoFAOCi4y2vsKXt6CzMabtEzVjb0ckm2nZTPqcdP1MuSHlVHGuvp7p7FATablAsvp4m5tXzabW0HaOZmZqUr2W0LbVmdUHKm5mdmjwq5RuLs1I+2/HV+3u9/SukfM/AKimv7uWlM9oTaDa1fbdcTts6U48vXjEWxtojwpZ2zadSOSlfqQxI+Y7Owf2IAIBzEoUCAHBBoQAAXFAoAAAXFAoAwAWFAgBwQaEAAFxQKAAAFxQKAMAFhQIAcEGhAABcdDymk09rOzS5QNtKSkJt66nR0HZuMhlt5yaJtG2oyamTUj6VaFteURxL+TDUjh+Lxy8WtK2zcleXlI9D7f2N2trztbS2IxW2I+34ZpbLa/txcdyU8s3mnJQ/NTst5Vuxdg0XimUpn8tp12Q2q22FhW1tD65R136+DmPxM2raNRbEJSnfldPyFi5q+Q5whwIAcEGhAABcUCgAABcUCgDABYUCAHBBoQAAXFAoAAAXFAoAwAWFAgBwQaEAAFxQKAAAFx0PGmUDbcsok9Lyc4szUr5V03aMTp/StsJCcevJAnFrq1mV8llx9ygw7fnWG3UpXyxqO1X5fF7Kt1vaDlMupx2/t6si5cUZJjMzaze1ba5Tp7RrIBT35vr7tf21ekM7flN8zzIZbSusu7tbytdq2v6gljbLZrXzjxLtOzGf0a7hJK3lFxe0bbdOcIcCAHBBoQAAXFAoAAAXFAoAwAWFAgBwQaEAAFxQKAAAFxQKAMAFhQIAcEGhAABcUCgAABcdD1YFibZL1F8pSfl0U9tiOvzqK1L+xNQJKX+6qu0YDQ0NSPl2Y1HKx7G2NJQWd5LiSNsZKha19zeby0p59fxTGe34fd3a+9Wjbn+ZWZBo71kup+2jldLaNZMk4p5dpF3zqSSU8nGi/Tw7O6dthSXi6x+G2vnXato1nMtqe3xDA9r5tBcnpfzi6Rel/IYOMtyhAABcUCgAABcUCgDABYUCAHBBoQAAXFAoAAAXFAoAwAWFAgBwQaEAAFxQKAAAFxQKAMBFkKiDNwAA/BHcoQAAXFAoAAAXFAoAwAWFAgBwQaEAAFxQKAAAFxQKAMAFhQIAcEGhAABc/C/Qlyd1r/Ib/AAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from random import randint, seed\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generamos una semilla fija para poder repetir el ejemplo\n",
        "seed(10)\n",
        "\n",
        "# Cogemos una muestra al azahar\n",
        "idx = randint(0, len(train_dataloader)-1)\n",
        "sample_dataloader = next(iter(train_dataloader))\n",
        "sample_dataloader_images, sample_dataloader_labels = sample_dataloader\n",
        "idx = randint(0, len(sample_dataloader_images)-1)\n",
        "sample_image = sample_dataloader_images[idx]\n",
        "sample_label = sample_dataloader_labels[idx]\n",
        "\n",
        "labels_map = {\n",
        "    0: \"airplane\",\n",
        "    1: \"automobile\",\n",
        "    2: \"bird\",\n",
        "    3: \"cat\",\n",
        "    4: \"deer\",\n",
        "    5: \"dog\",\n",
        "    6: \"frog\",\n",
        "    7: \"horse\",\n",
        "    8: \"ship\",\n",
        "    9: \"truck\",\n",
        "}\n",
        "\n",
        "figure = plt.figure(figsize=(5, 5))\n",
        "plt.title(labels_map[sample_label.item()])\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.imshow(sample_image.permute(1,2,0).squeeze())  \n",
        "# permute cambia el orden de las dimensiones, ya que las\n",
        "# imágenes tienen tamaño 3x32x32, pero necesitamos que\n",
        "# las dimensiones sean 32x32x3 para que matplotlib las pinte\n",
        "# así que se permutan poniendo primero las dos de 32 y \n",
        "# la última la de 3\n",
        "\n",
        "# squeeze elimina todas las dimensiones 1 de un tensor, \n",
        "# si se le mete un tensor de dimensiones (Ax1xBxCx1xD) \n",
        "# devuelve un tensor de dimensiones (AxBxCxD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La red cree que en la imagen hay dog\n"
          ]
        }
      ],
      "source": [
        "# Le pasamos la imagen a la red neuronal creada desde cero\n",
        "model_scratch.to(\"cpu\")\n",
        "logits_scratch = model_scratch(sample_dataloader_images)\n",
        "\n",
        "# La red ha devuelto 64 logits, por lo que nos quedamos con el \n",
        "# número idx que es el que se ha representado antes\n",
        "probs_scratch = logits_scratch.softmax(dim=1)\n",
        "label = probs_scratch[idx].argmax().item()\n",
        "print(f\"La red cree que en la imagen hay {labels_map[label]}\")"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d1c24abb23a313e1f9ae042292cd8e6e3c60c5818227ced3d46e3df2c65171ef"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
