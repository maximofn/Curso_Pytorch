{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de pérdida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque hay muchas más, vamos a ver las funciones de pérdida más comunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.nn.L1loss(size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "\n",
    "Calcula el error absoluto\n",
    "\n",
    "$ l\\left(x,y\\right) = \\left[l_1,...,l_N\\right]^T $, donde $l_n = \\left|x_n-y_n\\right|$\n",
    "\n",
    "`reduction` usa por defecto ``'mean'``, pero puede también usar ``'sum'`` y ``'none'``. Los parámetros ``size_average`` y ``reduce`` están obsoletos y Pytorch recomienda no usarlos y solo usar ``reduction``\n",
    "\n",
    "Cuando en ``reduction`` se usa ``'mean'`` se hace una media de todos los errores, cuando se usa ``'sum'`` se hace la suma de todos los errores y cuando se usa ``'none'`` no se hace nada\n",
    "\n",
    "Vamos a verlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos lo que sería la predicción de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2791, 0.5998, 0.5164, 0.8278, 0.5011],\n",
       "        [0.3532, 0.6893, 0.1909, 0.3920, 0.8956],\n",
       "        [0.2637, 0.1896, 0.1320, 0.6258, 0.6567]], requires_grad=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "preds = torch.rand(3, 5, requires_grad=True)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos lo que sería la verdadera salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8515, 0.4888, 0.0135, 0.0746, 0.9642],\n",
       "        [0.8065, 0.9731, 0.5105, 0.0795, 0.0852],\n",
       "        [0.2918, 0.7933, 0.2819, 0.1884, 0.8006]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.rand(3, 5)\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste con `reduction` con su valor predeterminado, es decir, `mean` y comparamos lo que da la función de coste con hacer nosotros la operación que dice la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3963553309440613, 0.3963553309440613)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean') # Predeterminado\n",
    "\n",
    "loss_fn = loss(preds, target)\n",
    "my_loss = abs(preds - target).mean()\n",
    "\n",
    "loss_fn.item(), my_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste ahora con `reduction` con valor `sum` y comparamos lo que da la función de coste con hacer nosotros la operación que dice la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.9453301429748535, 5.9453301429748535)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.L1Loss(size_average=None, reduce=None, reduction='sum')\n",
    "\n",
    "loss_fn = loss(preds, target)\n",
    "my_loss = abs(preds - target).sum()\n",
    "\n",
    "loss_fn.item(), my_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste ahora con `reduction` con valor `none` y comparamos lo que da la función de coste con hacer nosotros la operación que dice la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5724, 0.1111, 0.5029, 0.7532, 0.4632],\n",
       "         [0.4533, 0.2838, 0.3196, 0.3124, 0.8104],\n",
       "         [0.0282, 0.6037, 0.1498, 0.4374, 0.1439]], grad_fn=<L1LossBackward0>),\n",
       " tensor([[0.5724, 0.1111, 0.5029, 0.7532, 0.4632],\n",
       "         [0.4533, 0.2838, 0.3196, 0.3124, 0.8104],\n",
       "         [0.0282, 0.6037, 0.1498, 0.4374, 0.1439]], grad_fn=<AbsBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.L1Loss(size_average=None, reduce=None, reduction='none')\n",
    "\n",
    "loss_fn = loss(preds, target)\n",
    "my_loss = abs(preds - target)\n",
    "\n",
    "loss_fn, my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "\n",
    "Calcula el error cuadrático\n",
    "\n",
    "$ l\\left(x,y\\right) = \\left[l_1,...,l_N\\right]^T $, donde $l_n = \\left(x_n-y_n\\right)^2$\n",
    "\n",
    "`reduction` usa por defecto ``'mean'``, pero puede también usar ``'sum'`` y ``'none'``. Los parámetros ``size_average`` y ``reduce`` están obsoletos y Pytorch recomienda no usarlos y solo usar ``reduction``\n",
    "\n",
    "Cuando en ``reduction`` se usa ``'mean'`` se hace una media de todos los errores, cuando se usa ``'sum'`` se hace la suma de todos los errores y cuando se usa ``'none'`` no se hace nada\n",
    "\n",
    "Vamos a verlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos lo que sería la predicción de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5656, 0.6230, 0.8973, 0.8248, 0.6337],\n",
       "        [0.9425, 0.8919, 0.8190, 0.1976, 0.5075],\n",
       "        [0.7758, 0.3876, 0.1820, 0.7449, 0.9184]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "preds = torch.rand(3, 5, requires_grad=True)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos lo que sería la verdadera salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.2189e-01, 4.8484e-01, 4.3797e-01, 5.2775e-01, 1.2581e-01],\n",
       "        [8.9683e-01, 5.6446e-01, 4.3690e-01, 8.6725e-05, 3.3498e-01],\n",
       "        [3.2481e-01, 4.5485e-01, 9.4556e-01, 2.8550e-01, 6.8176e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.rand(3, 5)\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste con `reduction` con su valor predeterminado, es decir, `mean` y comparamos lo que da la función de coste con hacer nosotros la operación que dice la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13492344319820404, 0.13492344319820404)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean') # Predeterminado\n",
    "\n",
    "loss_fn = loss(preds, target)\n",
    "my_loss = ((preds - target)**2).mean()\n",
    "\n",
    "loss_fn.item(), my_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste ahora con `reduction` con valor `sum` y comparamos lo que da la función de coste con hacer nosotros la operación que dice la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0238516330718994, 2.0238516330718994)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.MSELoss(size_average=None, reduce=None, reduction='sum')\n",
    "\n",
    "loss_fn = loss(preds, target)\n",
    "my_loss = ((preds - target)**2).sum()\n",
    "\n",
    "loss_fn.item(), my_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste ahora con `reduction` con valor `none` y comparamos lo que da la función de coste con hacer nosotros la operación que dice la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0657, 0.0191, 0.2110, 0.0883, 0.2579],\n",
       "         [0.0021, 0.1072, 0.1460, 0.0390, 0.0297],\n",
       "         [0.2033, 0.0045, 0.5830, 0.2110, 0.0560]], grad_fn=<MseLossBackward0>),\n",
       " tensor([[0.0657, 0.0191, 0.2110, 0.0883, 0.2579],\n",
       "         [0.0021, 0.1072, 0.1460, 0.0390, 0.0297],\n",
       "         [0.2033, 0.0045, 0.5830, 0.2110, 0.0560]], grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.MSELoss(size_average=None, reduce=None, reduction='none')\n",
    "\n",
    "loss_fn = loss(preds, target)\n",
    "my_loss = (preds - target)**2\n",
    "\n",
    "loss_fn, my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "\n",
    "Calcula la entropía cruzada binaria entre lo predicho por la red y el target. Es útil cuando se entrena un problema de 2 clases\n",
    "\n",
    "Si se proporciona, el argumento opcional `weight` debe ser un tensor 1D que asigne peso a cada una de las clases. Esto es particularmente útil cuando se tiene un conjunto de entrenamiento desbalanceado.\n",
    "\n",
    "Se calcula como\n",
    "\n",
    "$$l_n = -\\omega_n\\left[y_n·log\\left(x_n\\right) + \\left(1-y_n\\right)·log\\left(1-x_n\\right) \\right]$$\n",
    "\n",
    "Donde $l_n$ es la loss para cada clase, $\\omega_n$ corresponde al valor del peso explicado en el párrafo anterior, $y_n$ corresponde al target y $x_n$ a lo predicho por el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduction` usa por defecto ``'mean'``, pero puede también usar ``'sum'`` y ``'none'``. Los parámetros ``size_average`` y ``reduce`` están obsoletos y Pytorch recomienda no usarlos y solo usar ``reduction``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando en ``reduction`` se usa ``'mean'`` se hace una media de todos los errores, cuando se usa ``'sum'`` se hace la suma de todos los errores y cuando se usa ``'none'`` no se hace nada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay que tener en cuenta que tanto $x$ como $y$ tienen que tener valores entre 0 y 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También hay que tener en cuenta que si $x_n$ vale 0 o 1 vamos a tener el caso de $log\\left(0\\right) = - \\infty$. Esto no es deseable por dos razones\n",
    "\n",
    " * Como $y_n$ va a ser 0 o 1, entonces vamos a tener $y_n = 0$ o $\\left(1 - y_n\\right) = 0$. Es decir, vamos a tener una multiplicación de $0$ por $\\infty$\n",
    " * Si tenemos un valor de $\\infty$ en la función de pérdida vamos a tener también un valor de $\\infty$ en el gradiente, de manera que cuando vayamos a actualizar los parámetros de la red vamos a tener problemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a verlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos lo que sería la predicción de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6786, -1.4587,  0.0754], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "logits = torch.randn(3, requires_grad=True)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos que crear probabilidades a partir de estos valores, por lo que usamos la función `Sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "pytorch_sigmoid = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.3366, 0.1887, 0.5188], grad_fn=<MulBackward0>),\n",
       " tensor([0.3366, 0.1887, 0.5188], grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_preds = my_sigmoid(logits)\n",
    "pytorch_preds = pytorch_sigmoid(logits)\n",
    "\n",
    "my_preds, pytorch_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos lo que sería la verdadera salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.empty(3).random_(2)\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos también la matriz de pesos para cuando la queramos usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([53., 82., 37.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.empty(3).random_(100)\n",
    "weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste con `reduction` con su valor predeterminado, es decir, `mean` y comparamos lo que da la función de coste con hacer nosotros la operación que dice la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6765, grad_fn=<BinaryCrossEntropyBackward0>),\n",
       " tensor(0.6765, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean') # Predeterminado\n",
    "\n",
    "loss_fn = loss(pytorch_preds, target)\n",
    "my_loss = (-(target*torch.log(my_preds) + (1-target)*torch.log(1-my_preds))).mean()\n",
    "\n",
    "loss_fn, my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste ahora con `reduction` con valor `sum` y comparamos lo que da la función de coste con hacer nosotros la operación que dice la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0295441150665283, 2.0295441150665283)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='sum')\n",
    "\n",
    "loss_fn = loss(pytorch_preds, target)\n",
    "my_loss = (-(target*torch.log(my_preds) + (1-target)*torch.log(1-my_preds))).sum()\n",
    "\n",
    "loss_fn.item(), my_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste ahora con `reduction` con valor `none` y comparamos lo que da la función de coste con hacer nosotros la operación que dice la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0889, 0.2091, 0.7315], grad_fn=<BinaryCrossEntropyBackward0>),\n",
       " tensor([1.0889, 0.2091, 0.7315], grad_fn=<NegBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='none')\n",
    "\n",
    "loss_fn = loss(pytorch_preds, target)\n",
    "my_loss = -(target*torch.log(my_preds) + (1-target)*torch.log(1-my_preds))\n",
    "\n",
    "loss_fn, my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos ahora el efecto de meter la matriz de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([57.7129, 17.1440, 27.0672], grad_fn=<BinaryCrossEntropyBackward0>),\n",
       " tensor([57.7129, 17.1440, 27.0672], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.BCELoss(weight=weight, size_average=None, reduce=None, reduction='none')\n",
    "\n",
    "loss_fn = loss(pytorch_preds, target)\n",
    "my_loss = -weight*(target*torch.log(my_preds) + (1-target)*torch.log(1-my_preds))\n",
    "\n",
    "loss_fn, my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary cross entropy with logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)\n",
    "```\n",
    "\n",
    "Esta pérdida combina una capa Sigmoide y la BCELoss en una sola clase. Esta versión es más estable numéricamente que usar un Sigmoide simple seguida de BCELoss ya que, al combinar las operaciones en una sola capa, aprovechamos el truco log-sum-exp para la estabilidad numérica.\n",
    "\n",
    "Es útil cuando se entrena un problema de clasificación con 2 clases. Si se proporciona, el argumento opcional `weight` debe ser un tensor 1D que asigne peso a cada una de las clases. Esto es particularmente útil cuando se tiene un conjunto de entrenamiento desbalanceado.\n",
    "\n",
    "$$l_n = -\\omega_n\\left[y_n·log\\left(\\sigma\\left(x_n\\right)\\right) + \\left(1-y_n\\right)·log\\left(1-\\sigma\\left(x_n\\right)\\right) \\right]$$\n",
    "\n",
    "Donde $l_n$ es la loss para cada clase, $\\omega_n$ corresponde al valor del peso explicado en el párrafo anterior, $y_n$ corresponde al target y $x_n$ a lo predicho por el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduction` usa por defecto ``'mean'``, pero puede también usar ``'sum'`` y ``'none'``. Los parámetros ``size_average`` y ``reduce`` están obsoletos y Pytorch recomienda no usarlos y solo usar ``reduction``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando en ``reduction`` se usa ``'mean'`` se hace una media de todos los errores, cuando se usa ``'sum'`` se hace la suma de todos los errores y cuando se usa ``'none'`` no se hace nada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay que tener en cuenta que tanto $x$ como $y$ tienen que tener valores entre 0 y 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible compensar el recall y la precisión agregando pesos a los ejemplos positivos.\n",
    "\n",
    "$ l\\left(x,y\\right) = \\left[l_1,...,l_N\\right]^T $, donde $l_n = -\\omega_n\\left[p·y_n·log\\left(\\sigma\\left(x_n\\right)\\right) + \\left(1-y_n\\right)·log\\left(1-\\sigma\\left(x_n\\right)\\right) \\right]$\n",
    "\n",
    "p>1 aumenta el recall, mientars que p<1 aumenta la precisión.\n",
    "\n",
    "Por ejemplo, si un conjunto de datos contiene 100 ejemplos positivos y 300 negativos de una sola clase, entonces pos_weight para la clase debe ser igual a $\\frac{300}{100}=3$. La pérdida actuaría como si el conjunto de datos contuviera $3×100=300$ ejemplos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a verlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos lo que sería la predicción de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2686,  0.1552,  1.0885], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "logits = torch.randn(3, requires_grad=True)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya no tenemos que calcular las probabilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos lo que sería la verdadera salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.empty(3).random_(2)\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos también la matriz de pesos para cuando la queramos usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([70., 94., 22.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.empty(3).random_(100)\n",
    "weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la matriz de pesos positivos para cuando la queramos usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_weight = torch.ones([3])\n",
    "pos_weight[1] = 5\n",
    "pos_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste con `reduction` con su valor predeterminado, es decir, `mean` y comparamos lo que da la función de coste con hacer nosotros la operación que dice la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9963, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " tensor(0.9963, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None) # Predeterminado\n",
    "\n",
    "loss_fn = loss(logits, target)\n",
    "my_loss = (-(target*torch.log(torch.sigmoid(logits)) + (1-target)*torch.log(1-torch.sigmoid(logits)))).mean()\n",
    "\n",
    "loss_fn, my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste ahora con `reduction` con valor `sum` y comparamos lo que da la función de coste con hacer nosotros la operación que dice la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.9889793395996094, 2.9889793395996094)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='sum', pos_weight=None)\n",
    "\n",
    "loss_fn = loss(logits, target)\n",
    "my_loss = (-(target*torch.log(torch.sigmoid(logits)) + (1-target)*torch.log(1-torch.sigmoid(logits)))).sum()\n",
    "\n",
    "loss_fn.item(), my_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste ahora con `reduction` con valor `none` y comparamos lo que da la función de coste con hacer nosotros la operación que dice la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.8365, 0.7738, 1.3787],\n",
       "        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " tensor([0.8365, 0.7738, 1.3787], grad_fn=<NegBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='none', pos_weight=None)\n",
    "\n",
    "loss_fn = loss(logits, target)\n",
    "my_loss = -(target*torch.log(torch.sigmoid(logits)) + (1-target)*torch.log(1-torch.sigmoid(logits)))\n",
    "\n",
    "loss_fn, my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos ahora el efecto de meter la matriz de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([58.5524, 72.7355, 30.3322],\n",
       "        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " tensor([58.5523, 72.7355, 30.3322], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.BCEWithLogitsLoss(weight=weight, size_average=None, reduce=None, reduction='none', pos_weight=None)\n",
    "\n",
    "loss_fn = loss(logits, target)\n",
    "my_loss = -weight*(target*torch.log(torch.sigmoid(logits)) + (1-target)*torch.log(1-torch.sigmoid(logits)))\n",
    "\n",
    "loss_fn, my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos ahora el efecto de meter la matriz de pesos positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([58.5524, 72.7355, 30.3322],\n",
       "        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " tensor([58.5523, 72.7355, 30.3322], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.BCEWithLogitsLoss(weight=weight, size_average=None, reduce=None, reduction='none', pos_weight=pos_weight)\n",
    "\n",
    "loss_fn = loss(logits, target)\n",
    "my_loss = -weight*(pos_weight*target*torch.log(torch.sigmoid(logits)) + (1-target)*torch.log(1-torch.sigmoid(logits)))\n",
    "\n",
    "loss_fn, my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0)\n",
    "```\n",
    "\n",
    "Calcula la entropía cruzada entre lo predicho por la red y el target. Es útil cuando se entrena un problema de C clases\n",
    "\n",
    "Si se proporciona, el argumento opcional `weight` debe ser un tensor 1D que asigne peso a cada una de las clases. Esto es particularmente útil cuando se tiene un conjunto de entrenamiento desbalanceado.\n",
    "\n",
    "Se calcula como\n",
    "\n",
    "$l_n = -\\omega_nlog\\left(\\frac{e^{x_n}}{\\sum_{c=1}^{C}{e^{x_n}}}\\right)$\n",
    "\n",
    "Donde $l_n$ es la loss para cada clase, $\\omega_n$ corresponde al valor del peso explicado en el párrafo anterior, $y_n$ corresponde al target y $x_n$ a lo predicho por el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduction` usa por defecto ``'mean'``, pero puede también usar ``'sum'`` y ``'none'``. Los parámetros ``size_average`` y ``reduce`` están obsoletos y Pytorch recomienda no usarlos y solo usar ``reduction``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando en ``reduction`` se usa ``'mean'`` se hace una media de todos los errores, cuando se usa ``'sum'`` se hace la suma de todos los errores y cuando se usa ``'none'`` no se hace nada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro `ignore_index` hace que se ignoren las salidas tienen como label el valor de `ignore_index`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro `label_smoothing` puede valer entre `0.0` y `1.0`, y tiene que ver con el suavizado de la loss, que no vamos a ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a verlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos lo que sería la predicción de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8584,  1.0926, -0.3535],\n",
       "        [-0.1641,  0.4056, -0.1291],\n",
       "        [ 0.2495,  0.4327, -0.7101],\n",
       "        [ 0.9400, -1.2176, -0.7795],\n",
       "        [-1.4444, -0.7460,  0.8954]], requires_grad=True)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = 5\n",
    "clases = 3\n",
    "\n",
    "preds = torch.randn(examples, clases, requires_grad=True)\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos lo que sería la verdadera salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 0, 2, 1, 1]),\n",
       " tensor([[0, 0, 1],\n",
       "         [1, 0, 0],\n",
       "         [0, 0, 1],\n",
       "         [0, 1, 0],\n",
       "         [0, 1, 0]]))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.empty(examples, dtype=torch.long).random_(clases)\n",
    "target = torch.nn.functional.one_hot(labels, num_classes=clases)\n",
    "\n",
    "labels, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos también la matriz de pesos para cuando la queramos usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 1., 1., 1.])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_weight = torch.ones([examples])\n",
    "pos_weight[1] = 5\n",
    "pos_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste con `reduction` con su valor predeterminado, es decir, `mean` y comparamos lo que da la función de coste con hacer nosotros la operación que dice la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.8647, grad_fn=<MeanBackward0>),\n",
       " tensor(1.8647, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0)\n",
    "\n",
    "loss_fn = loss(preds, labels)\n",
    "\n",
    "exp = torch.exp(preds)\n",
    "exp_sum = exp.sum(dim=1, keepdim=True)\n",
    "my_loss = (-target*torch.log(exp/exp_sum)).sum(dim=1).mean()\n",
    "\n",
    "my_loss, loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste ahora con `reduction` con valor `sum` y comparamos lo que da la función de coste con hacer nosotros la operación que dice la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.3233, grad_fn=<SumBackward0>),\n",
       " tensor(9.3233, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='sum', label_smoothing=0.0)\n",
    "\n",
    "loss_fn = loss(preds, labels)\n",
    "\n",
    "exp = torch.exp(preds)\n",
    "exp_sum = exp.sum(dim=1, keepdim=True)\n",
    "my_loss = (-target*torch.log(exp/exp_sum)).sum(dim=1).sum()\n",
    "\n",
    "my_loss, loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste ahora con `reduction` con valor `none` y comparamos lo que da la función de coste con hacer nosotros la operación que dice la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.7664, 1.3359, 1.9090, 2.4159, 1.8961], grad_fn=<SumBackward1>),\n",
       " tensor([1.7664, 1.3359, 1.9090, 2.4159, 1.8961], grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='none', label_smoothing=0.0)\n",
    "\n",
    "loss_fn = loss(preds, labels)\n",
    "\n",
    "exp = torch.exp(preds)\n",
    "exp_sum = exp.sum(dim=1, keepdim=True)\n",
    "my_loss = (-target*torch.log(exp/exp_sum)).sum(dim=1)\n",
    "\n",
    "my_loss, loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos ahora el efecto de meter la matriz de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 38.8611,  93.5102,  41.9981, 227.0984, 178.2310],\n",
       "        grad_fn=<SumBackward1>),\n",
       " tensor([ 38.8611,  93.5102,  41.9981, 227.0984, 178.2310],\n",
       "        grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(weight=weight, size_average=None, ignore_index=-100, reduce=None, reduction='none', label_smoothing=0.0)\n",
    "\n",
    "loss_fn = loss(preds, labels)\n",
    "\n",
    "exp = torch.exp(preds)\n",
    "exp_sum = exp.sum(dim=1, keepdim=True)\n",
    "my_loss = (-weight*target*torch.log(exp/exp_sum)).sum(dim=1)\n",
    "\n",
    "my_loss, loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora el efecto de ignorar una clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 0, 2, 1, 1]),\n",
       " tensor([[0, 0, 1],\n",
       "         [1, 0, 0],\n",
       "         [0, 0, 1],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]),\n",
       " tensor([1.7664, 1.3359, 1.9090, 0.0000, 0.0000], grad_fn=<SumBackward1>),\n",
       " tensor([1.7664, 1.3359, 1.9090, 0.0000, 0.0000], grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore_index = 1\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=ignore_index, reduce=None, reduction='none', label_smoothing=0.0)\n",
    "\n",
    "loss_fn = loss(preds, labels)\n",
    "\n",
    "num_ignored = 0\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == ignore_index:\n",
    "        target[i][ignore_index] = 0\n",
    "        num_ignored += 1\n",
    "\n",
    "exp = torch.exp(preds)\n",
    "exp_sum = exp.sum(dim=1, keepdim=True)\n",
    "my_loss = (-target*torch.log(exp/exp_sum)).sum(dim=1)\n",
    "\n",
    "labels, target, my_loss, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.0113, grad_fn=<SumBackward0>),\n",
       " tensor(5.0113, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=ignore_index, reduce=None, reduction='sum', label_smoothing=0.0)\n",
    "\n",
    "loss_fn = loss(preds, labels)\n",
    "\n",
    "exp = torch.exp(preds)\n",
    "exp_sum = exp.sum(dim=1, keepdim=True)\n",
    "my_loss = (-target*torch.log(exp/exp_sum)).sum(dim=1).sum()\n",
    "\n",
    "my_loss, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.6704, grad_fn=<DivBackward0>),\n",
       " tensor(1.6704, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=ignore_index, reduce=None, reduction='mean', label_smoothing=0.0)\n",
    "\n",
    "loss_fn = loss(preds, labels)\n",
    "\n",
    "exp = torch.exp(preds)\n",
    "exp_sum = exp.sum(dim=1, keepdim=True)\n",
    "my_loss = (-target*torch.log(exp/exp_sum)).sum(dim=1).sum()/(examples-num_ignored)\n",
    "\n",
    "my_loss, loss_fn"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e4d58f53b4b3ced286559ef92073773937aa87eedd0536c036fd264999b02c5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
