{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos explicado, durante el entrenamiento necesitamos una función para saber cómo de bien o mal lo está haciendo nuestra red y así poder entrenarla mediante el algoritmo del descenso del gradiente. Pero estas son funciones para nuestra red, que a nosotros nos puede llegar a costar entender, por lo tanto necesitamos métricas para nosotoros, para que podamos valorar cómo de bien lo está haciendo nuestro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas para probemas de regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de los problemas de regresión podemos usar el error cuadrático medio (MSE - Mean Square Error) o el error absoluto medio o L1 (MAE - Mean Absolute Error), que como ya las hemos visto en el tema de las funciones de pérdida no las vamos a volver a ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas para problemas de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los problemas de clasificación hemos visto la entropía cruzada binaria (BCE - Binary Cross Entropy) como función de pérdida, que no nos aporta mucha información. Por lo que aquí si necesitamos funciones para medir lo bien o mal que lo está haciendo nuestro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de confusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que tenemos una clasificador que tiene que clasificar perros, gatos y pájaros y obtenemos la siguiente tabla de resultados\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th></th>\n",
    "            <th colspan=3><center>Real</center></th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th></th>\n",
    "            <th><center>Perro</center></th>\n",
    "            <th><center>Gato</center></th>\n",
    "            <th><center>Pájaro</center></th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th rowspan=3>Predicho</th>\n",
    "            <th>Perro</th>\n",
    "            <td><center>112</center></td>\n",
    "            <td><center>28</center></td>\n",
    "            <td><center>19</center></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Gato</th>\n",
    "            <td><center>21</center></td>\n",
    "            <td><center>134</center></td>\n",
    "            <td><center>7</center></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Pájaro</th>\n",
    "            <td><center>14</center></td>\n",
    "            <td><center>25</center></td>\n",
    "            <td><center>103</center></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A esta tabla se le llama matriz de confusión, y muestra los aciertos y los fallos de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando tenemos la matriz de confución podemos obtener los verdaderos positivos, falsos positivos, falsos negativos y verdaderos negativos de cada clase. Ahora vemos qué es cada cosa:\n",
    " * TP (True Positive - Verdadero positivo): Es cuando la red acierta con la predicción. La red predice que un dato pertenece a una clase y de verdad ese dato pertenece a esa clase\n",
    " * FP (False Positive - Falso positivo): Es cuando la red predice que un dato pertenece a una clase, pero no pertenece a esa clase\n",
    " * FN (False Negative - Falso negativo): Es cuando la red predice que un dato no pertenece a una clase, pero sí pertenece a esa clase\n",
    " * TN (True Negative - Verdadero negativo): Es cuando la red predice que un dato no pertenece a una clase y de verdad no pertenece a esa clase\n",
    "\n",
    "Vamos a verlo con la anterior tabla\n",
    " * Clase perro:\n",
    "   + TP = 112\n",
    "   + FP = 28 + 19 = 47\n",
    "   + FN = 21 + 14 = 35\n",
    "   + TN = 134 + 7 + 25 + 103 = 269\n",
    " * Clase Gato:\n",
    "   + TP = 134\n",
    "   + FP = 21 + 7 = 28\n",
    "   + FN = 28 + 25 = 53\n",
    "   + TN = 112 + 19 + 17 + 103 = 251\n",
    " * Clase Pájaro:\n",
    "   + TP = 103\n",
    "   + FP = 14 + 25 = 39\n",
    "   + FN = 19 + 7 = 26\n",
    "   + TN = 112 + 28 + 21 + 134 = 295\n",
    "  \n",
    "Vamos a definir esto en variables para luego usarlo en métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_perro = 112\n",
    "FP_perro = 47\n",
    "FN_perro = 35\n",
    "TN_perro = 269\n",
    "\n",
    "TP_gato = 134\n",
    "FP_gato = 28\n",
    "FN_gato = 53\n",
    "TN_gato = 251\n",
    "\n",
    "TP_pajaro = 103\n",
    "FP_pajaro = 39\n",
    "FN_pajaro = 26\n",
    "TN_pajaro = 295\n",
    "\n",
    "total_perro = TP_perro + FP_perro + FN_perro + TN_perro\n",
    "total_gato = TP_gato + FP_gato + FN_gato + TN_gato\n",
    "total_pajaro = TP_pajaro + FP_pajaro + FN_pajaro + TN_pajaro\n",
    "\n",
    "total = total_perro + total_gato + total_pajaro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es la relación entre el número de predicciones correctas frente al total de predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2507183908045977"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (TP_perro + TP_gato + TP_pajaro) / total\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando el número de muestras no es similar entre clases, se dice que el problema está desbalancecado. Supongamos un problema en el que hay 1000000 muestras de perro, 10 de gato y 10 de pájaro, si usamos una red que le entre lo que le entre siempre diga que es perro va a tener una accuracy de \n",
    "\n",
    "$$\\frac{1000}{1000+10+10}=0.98$$\n",
    "\n",
    "Tenemos un accuracy del 98% pero sabemos que la red no está intentando clasificar las muestras. Así que en estos casos se suele usar la precisión que se mide mediante\n",
    "\n",
    "$$Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "En el ejemplo anterior sería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7537796976241901,\n",
       " 0.7044025157232704,\n",
       " 0.8271604938271605,\n",
       " 0.7253521126760564)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_general = (TP_perro+TP_gato+TP_pajaro) / (TP_perro+TP_gato+TP_pajaro + FP_perro+FP_gato+FP_pajaro)\n",
    "\n",
    "precision_perro = TP_perro / (TP_perro + FP_perro)\n",
    "precision_gato = TP_gato / (TP_gato + FP_gato)\n",
    "precision_pajaro = TP_pajaro / (TP_pajaro + FP_pajaro)\n",
    "\n",
    "precision_general, precision_perro, precision_gato, precision_pajaro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define como la fracción de muestras de una clase que el modelo predice correctamente. Se calcula mediante\n",
    "\n",
    "$$Recall = \\frac{TP}{TP+FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7619047619047619, 0.7165775401069518, 0.7984496124031008)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_perro = TP_perro / (TP_perro + FN_perro)\n",
    "recall_gato = TP_gato / (TP_gato + FN_gato)\n",
    "recall_pajaro = TP_pajaro / (TP_pajaro + FN_pajaro)\n",
    "\n",
    "recall_perro, recall_gato, recall_pajaro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según la aplicación, es posible que desee dar mayor prioridad a la Precision o la Recall. Pero hay muchas aplicaciones en las que tanto la Precision como la Recall son importantes. Por lo tanto, es natural pensar en una forma de combinar estos dos en una sola métrica\n",
    "\n",
    "$$F1 score = 2\\frac{Precision·Recall}{Precision+Recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7320261437908496, 0.7679083094555874, 0.7601476014760148)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_perro = 2 * (precision_perro * recall_perro) / (precision_perro + recall_perro)\n",
    "f1_gato = 2 * (precision_gato * recall_gato) / (precision_gato + recall_gato)\n",
    "f1_pajaro = 2 * (precision_pajaro * recall_pajaro) / (precision_pajaro + recall_pajaro)\n",
    "\n",
    "f1_perro, f1_gato, f1_pajaro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Especifidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Especifidad = \\frac{TN}{TN+FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8512658227848101, 0.899641577060932, 0.8832335329341318)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "especifidad_perro = TN_perro / (TN_perro + FP_perro)\n",
    "especifidad_gato = TN_gato / (TN_gato + FP_gato)\n",
    "especifidad_pajaro = TN_pajaro / (TN_pajaro + FP_pajaro)\n",
    "\n",
    "especifidad_perro, especifidad_gato, especifidad_pajaro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area bajo la curva ROC (AUC - Area Under Curve ROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La curva ROC se usa solo con clasificadores binarios. Muestra la tasa de TP contra la tasa de FP para varios valores umbral.\n",
    "\n",
    "Cuando una red de clasificación predice una clase, lo que hace es darte un valor entre 0 y 1. Si da 0 es que pertenece a una clase y si da 1 es que pertenece a otra clase, pero para valores intermedios depende de un valor umbral que establecemos, y que normalmente es 0,5. Pero este valor umbral se puede modificar.\n",
    "\n",
    "Por ejemplo una red que prediga si una imagen corresponde a un perro (1) o no (0) puede dar las siguientes salidas [0,4; 0,6; 0,7; 0,3] para distintos valores de umbral estas serían sus salidas\n",
    " * Umbral de 0,5: [0; 1; 1; 0]\n",
    " * Umbral de 0,2: [1; 1; 1; 1]\n",
    " * Umbral de 0,8: [0; 0; 0; 0]\n",
    "\n",
    "Al cambiar el valor umbral cambia la clase predicha. Por lo tanto, se puede entender, que cambiando el valor umbrarl cambiará el número de TP y el de FP, por lo que también cambia el ratio de TPs y FPs.\n",
    "\n",
    "La curva ROC representa los ratios entre TPs y FPs\n",
    "\n",
    "![curva ROC](Imagenes/ROC_courve.png)\n",
    "\n",
    "Lo ideal sería una curva lo más pegada a la izquierda en el eje del ratio de los FPs y lo más arriba posible en el eje de los TPs, ya que si la curva está encima de los ejes, quiere decir que para todos los umbrales el ratio de FPs es siempre 0, es decir, nunca hay falsos positivos, y que para todos los umbrales el ratio de TPs es siempre 1, es decir, siempre acierta la red para todos los humbrales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como métrica por tanto se usa el área bajo la curva ROC, de manera que mejor será el sistema cuanto más cercano a 1 sea este área\n",
    "\n",
    "![área bajo la curva ROC](Imagenes/Area_ROC_curve.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e4d58f53b4b3ced286559ef92073773937aa87eedd0536c036fd264999b02c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
