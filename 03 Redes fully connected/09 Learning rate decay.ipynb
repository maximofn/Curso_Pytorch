{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una cosa que se puede hacer para mejorar el entrenamiento es ir disminuyendo el valor del learning rate a medida que entrenamos. Veamos por qué"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vemos el efecto de entrenar con un valor de learning rate de 0.01 en el ejemplo anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento para lr = 0.025\n",
      "i=10: error=3.5332415076132806, gradiente=4.434751401390874, a=1.945937013155588\n",
      "i=20: error=3.463169558318404, gradiente=0.11728176690111386, a=1.990209371273261\n",
      "i=30: error=3.46312055035669, gradiente=0.0031016423701155796, a=1.9913802013638249\n",
      "i=40: error=3.463120516080773, gradiente=8.202626584997337e-05, a=1.991411165223878\n",
      "i=50: error=3.4631205160568, gradiente=2.1692727570984022e-06, a=1.9914119840964342\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "x = np.array( [ 0.        ,  0.34482759,  0.68965517,  1.03448276,  1.37931034,\n",
    "        1.72413793,  2.06896552,  2.4137931 ,  2.75862069,  3.10344828,\n",
    "        3.44827586,  3.79310345,  4.13793103,  4.48275862,  4.82758621,\n",
    "        5.17241379,  5.51724138,  5.86206897,  6.20689655,  6.55172414,\n",
    "        6.89655172,  7.24137931,  7.5862069 ,  7.93103448,  8.27586207,\n",
    "        8.62068966,  8.96551724,  9.31034483,  9.65517241, 10.        ])\n",
    "\n",
    "y = np.array( [-0.16281253,  1.88707606,  0.39649312,  0.03857752,  4.0148778 ,\n",
    "        0.58866234,  3.35711859,  1.94314906,  6.96106424,  5.89792585,\n",
    "        8.47226615,  3.67698542, 12.05958678,  9.85234481,  9.82181679,\n",
    "        6.07652248, 14.17536744, 12.67825433, 12.97499286, 11.76098542,\n",
    "       12.7843083 , 16.42241036, 13.67913705, 15.55066478, 17.45979602,\n",
    "       16.41982806, 17.01977617, 20.28151197, 19.38148414, 19.41029831])\n",
    "\n",
    "random.seed(45)\n",
    "a = random.random()\n",
    "\n",
    "def loss_fn(y, z):\n",
    "    n = len(y)\n",
    "    loss = np.sum((z-y) ** 2) / n\n",
    "    return loss\n",
    "\n",
    "posibles_a = np.linspace(0, 4, 300)\n",
    "perdidas = np.empty_like(posibles_a)\n",
    "\n",
    "for i in range (len(posibles_a)):\n",
    "    z = posibles_a[i]*x\n",
    "    perdidas[i] = loss_fn(y, z)\n",
    "\n",
    "def gradiente (a, x, y):\n",
    "    # Función que calcula el valor de una derivada en un punto\n",
    "    n = len(y)\n",
    "    return 2*np.sum((a*x - y)*x)/n\n",
    "\n",
    "def gradiente_linea (i, a=None, error=None, gradiente=None, posibles_w=None, \n",
    "    losses=None, gradientes=None):\n",
    "    # Función que devuleve los puntos de la linea que supone la derivada de una \n",
    "    # función en un punto dado\n",
    "    if a is None:\n",
    "        x1 = posibles_w[i]-0.7\n",
    "        x2 = posibles_w[i]\n",
    "        x3 = posibles_w[i]+0.7\n",
    "\n",
    "        b = losses[i] - gradientes[i]*posibles_w[i]\n",
    "\n",
    "        y1 = gradientes[i]*x1 + b\n",
    "        y2 = losses[i]\n",
    "        y3 = gradientes[i]*x3 + b\n",
    "    else:\n",
    "        x1 = a-0.7\n",
    "        x2 = a\n",
    "        x3 = a+0.7\n",
    "\n",
    "        b = error - gradiente*a\n",
    "\n",
    "        y1 = gradiente*x1 + b\n",
    "        y2 = error\n",
    "        y3 = gradiente*x3 + b\n",
    "\n",
    "    x_linea = np.array([x1, x2, x3])\n",
    "    y_linea = np.array([y1, y2, y3])\n",
    "\n",
    "    return x_linea, y_linea\n",
    "\n",
    "LRs = [2.5e-2]    # Tasa de aprendizaje o learning rate\n",
    "steps = 50  # Numero de veces que se realiza el bucle de enrtenamiento\n",
    "\n",
    "# Matrices donde se guardarán los datos para luego ver la evolución del entrenamiento en una gráfica\n",
    "Zs = np.empty([len(LRs), steps, len(x)])\n",
    "Xs_linea_gradiente = np.empty([len(LRs), steps, 3])\n",
    "Ys_linea_gradiente = np.empty([len(LRs), steps, 3])\n",
    "As = np.empty([len(LRs), steps])\n",
    "Errores = np.empty([len(LRs), steps])\n",
    "\n",
    "for l, lr in enumerate(LRs):\n",
    "    # Inicialización aleatoria de a\n",
    "    random.seed(45)\n",
    "    a = random.random()\n",
    "    \n",
    "    print(f\"Entrenamiento para lr = {lr}\")\n",
    "    for i in range(steps):\n",
    "        # Calculamos el gradiente\n",
    "        dl = gradiente(a, x, y)\n",
    "\n",
    "        # Corregimos el valor de a\n",
    "        a = a - lr*dl\n",
    "\n",
    "        # Calculamos los valores que obtiene la red neuronal\n",
    "        z = a*x\n",
    "\n",
    "        # Obtenemos el error\n",
    "        error = loss_fn(y, z)\n",
    "\n",
    "        # Obtenemos las rectas de los gradientes para representarlas\n",
    "        x_linea_gradiente, y_linea_gradiente = gradiente_linea(3, a=a, error=error, gradiente=dl)\n",
    "\n",
    "        # Guardamos los valores para luego ver la evolución del entrenamiento en una gráfica\n",
    "        As[l][i] = a\n",
    "        Zs[l][i] = z\n",
    "        Errores[l][i] = error\n",
    "        Xs_linea_gradiente[l][i] = x_linea_gradiente\n",
    "        Ys_linea_gradiente[l][i] = y_linea_gradiente\n",
    "\n",
    "        # Imprimimos la evolución del entrenamiento\n",
    "        if (i+1)%10 == 0:\n",
    "            print(f\"i={i+1}: error={error}, gradiente={dl}, a={a}\")\n",
    "\n",
    "\n",
    "# Creamos GIF con la evolución del entrenamiento\n",
    "rectas = []\n",
    "gradientes = []\n",
    "puntos = []\n",
    "a_texts = []\n",
    "error_texts = []\n",
    "lr_texts = []\n",
    "\n",
    "fontsize = 12\n",
    "\n",
    "# Creamos la gráfica inicial\n",
    "fig, ax = plt.subplots(len(LRs),2, figsize=(15, 5*len(LRs)))\n",
    "ax = ax.reshape(len(LRs),ax.shape[0])\n",
    "fig.set_tight_layout(True)\n",
    "for i in range(len(LRs)):\n",
    "    ax[i][0].set_xlabel('X')\n",
    "    ax[i][0].set_ylabel('Y  ', rotation=0)\n",
    "    ax[i][1].set_xlabel('a')\n",
    "    ax[i][1].set_ylabel('loss  ', rotation=0)\n",
    "    ax[i][0].set_xlim(-0.5, 10.5)\n",
    "    ax[i][0].set_ylim(-0.5, 20.5)\n",
    "    ax[i][1].set_xlim(-0.5, 4.5)\n",
    "    ax[i][1].set_ylim(-0.5, 140.5)\n",
    "\n",
    "    # Se dibujan los datos que persistiran en toda la evolución de la gráfica\n",
    "    ax[i][0].scatter(x, y)\n",
    "    ax[i][1].plot(posibles_a, perdidas, linewidth = 3)\n",
    "\n",
    "    # Se dibuja el resto de lineas que irán cambiando durante el entrenamiento\n",
    "    line1, = ax[i][0].plot(x, Zs[i][0], 'k', linewidth=2)                             # Recta generada con la pendiente a aprendida\n",
    "    line2, = ax[i][1].plot(Xs_linea_gradiente[i][0], Ys_linea_gradiente[i][0], 'g')   # Gradiente de la función de error\n",
    "    punto, = ax[i][1].plot(As[i][0], Errores[i][0], 'r*')                             # Punto donde se calcula el gradiente\n",
    "    rectas.append(line1)\n",
    "    gradientes.append(line2)\n",
    "    puntos.append(punto)\n",
    "\n",
    "    # Se dibujan textos dentro de la segunda figura del subplot\n",
    "    lr_text = ax[i][1].text(1, 100, f'lr = {LRs[i]}', fontsize = fontsize)\n",
    "    a_text = ax[i][1].text(1, 90, f'pesos = {As[i][0]:.5f}', fontsize = fontsize)\n",
    "    error_text = ax[i][1].text(1, 80, f'loss = {Errores[i][0]:.5f}', fontsize = fontsize)\n",
    "    a_texts.append(a_text)\n",
    "    error_texts.append(error_text)\n",
    "    lr_texts.append(lr_text)\n",
    "    \n",
    "# Se dibuja un título\n",
    "titulo = fig.suptitle(f'step: {0}', fontsize=fontsize)\n",
    "\n",
    "# Se define la función que va a modificar la gráfica con la evolución del entrenamiento\n",
    "def update(i):\n",
    "\n",
    "    for l, _ in enumerate(LRs):\n",
    "        # Se actualiza la recta generada con la pendiente a aprendida\n",
    "        rectas[l].set_ydata(Zs[l][i])\n",
    "\n",
    "        # Se actualiza el gradiente de la función de error\n",
    "        gradientes[l].set_xdata(Xs_linea_gradiente[l][i])\n",
    "        gradientes[l].set_ydata(Ys_linea_gradiente[l][i])\n",
    "\n",
    "        # Se actualiza el punto 2. Punto donde se calcula el gradiente\n",
    "        puntos[l].set_xdata(As[l][i])\n",
    "        puntos[l].set_ydata(Errores[l][i])\n",
    "\n",
    "        # Se actualizan los textos\n",
    "        a_texts[l].set_text(f'pesos = {As[l][i]:.5f}')\n",
    "        error_texts[l].set_text(f'loss = {Errores[l][i]:.5f}')\n",
    "        lr_texts[l].set_text(f'lr = {LRs[l]}')\n",
    "    \n",
    "    titulo.set_text(f'step: {i}')\n",
    "\n",
    "    return line1, # ax1, #line2, punto2, ax2, a_text, error_text\n",
    "\n",
    "# Se crea la animación con un refresco cada 200 ms\n",
    "interval = 500 # ms\n",
    "anim = FuncAnimation(fig, update, frames=np.arange(0, steps), interval=interval)\n",
    "\n",
    "# Se guarda en un GIF\n",
    "gif_name = \"GIFs/LR_noDecay.gif\"\n",
    "anim.save(gif_name, dpi=80, writer='imagemagick')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LR sin decaimiento](GIFs/LR_noDecay.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver desde el step 4 el error no cambia mucho, en 4 steps se ha llegado a la mejor solución posible, pero vamos a ver si hacemos zoom en la zona del mínimo error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    }
   ],
   "source": [
    "# Creamos GIF con la evolución del entrenamiento\n",
    "rectas = []\n",
    "gradientes1 = []\n",
    "gradientes2 = []\n",
    "puntos1 = []\n",
    "puntos2 = []\n",
    "a_texts1 = []\n",
    "a_texts2 = []\n",
    "error_texts1 = []\n",
    "error_texts2 = []\n",
    "lr_texts1 = []\n",
    "lr_texts2 = []\n",
    "\n",
    "\n",
    "fontsize = 12\n",
    "\n",
    "# Creamos la gráfica inicial\n",
    "fig, ax = plt.subplots(len(LRs),3, figsize=(15, 5*len(LRs)))\n",
    "ax = ax.reshape(len(LRs),ax.shape[0])\n",
    "fig.set_tight_layout(True)\n",
    "for i in range(len(LRs)):\n",
    "    ax[i][0].set_xlabel('X')\n",
    "    ax[i][0].set_ylabel('Y  ', rotation=0)\n",
    "    ax[i][1].set_xlabel('a')\n",
    "    ax[i][1].set_ylabel('loss  ', rotation=0)\n",
    "    ax[i][2].set_xlabel('a')\n",
    "    ax[i][2].set_ylabel('loss  ', rotation=0)\n",
    "    ax[i][0].set_xlim(-0.5, 10.5)\n",
    "    ax[i][0].set_ylim(-0.5, 20.5)\n",
    "    ax[i][1].set_xlim(-0.5, 4.5)\n",
    "    ax[i][1].set_ylim(-0.5, 140.5)\n",
    "    ax[i][2].set_xlim(1.8, 2.2)\n",
    "    ax[i][2].set_ylim(3, 5.0)\n",
    "\n",
    "    # Se dibujan los datos que persistiran en toda la evolución de la gráfica\n",
    "    ax[i][0].scatter(x, y)\n",
    "    ax[i][1].plot(posibles_a, perdidas, linewidth = 3)\n",
    "    ax[i][2].plot(posibles_a, perdidas, linewidth = 3)\n",
    "\n",
    "    # Se dibuja el resto de lineas que irán cambiando durante el entrenamiento\n",
    "    recta, = ax[i][0].plot(x, Zs[i][0], 'k', linewidth=2)                             # Recta generada con la pendiente a aprendida\n",
    "    gradiente1, = ax[i][1].plot(Xs_linea_gradiente[i][0], Ys_linea_gradiente[i][0], 'g')   # Gradiente de la función de error\n",
    "    gradiente2, = ax[i][2].plot(Xs_linea_gradiente[i][0], Ys_linea_gradiente[i][0], 'g')   # Gradiente de la función de error\n",
    "    punto1, = ax[i][1].plot(As[i][0], Errores[i][0], 'r*')                             # Punto donde se calcula el gradiente\n",
    "    punto2, = ax[i][2].plot(As[i][0], Errores[i][0], 'r*')                             # Punto donde se calcula el gradiente\n",
    "    rectas.append(recta)\n",
    "    gradientes1.append(gradiente1)\n",
    "    gradientes2.append(gradiente2)\n",
    "    puntos1.append(punto1)\n",
    "    puntos2.append(punto2)\n",
    "\n",
    "    # Se dibujan textos dentro de la segunda figura del subplot\n",
    "    lr_text1 = ax[i][1].text(1, 100, f'lr = {LRs[i]}', fontsize = fontsize)\n",
    "    lr_text2 = ax[i][2].text(1.9, 4.75, f'lr = {LRs[i]}', fontsize = fontsize)\n",
    "    a_text1 = ax[i][1].text(1, 90, f'pesos = {As[i][0]:.5f}', fontsize = fontsize)\n",
    "    a_text2 = ax[i][2].text(1.9, 4.65, f'pesos = {As[i][0]:.5f}', fontsize = fontsize)\n",
    "    error_text1 = ax[i][1].text(1, 80, f'loss = {Errores[i][0]:.5f}', fontsize = fontsize)\n",
    "    error_text2 = ax[i][2].text(1.9, 4.55, f'loss = {Errores[i][0]:.5f}', fontsize = fontsize)\n",
    "    a_texts1.append(a_text1)\n",
    "    a_texts2.append(a_text2)\n",
    "    error_texts1.append(error_text1)\n",
    "    error_texts2.append(error_text2)\n",
    "    lr_texts1.append(lr_text1)\n",
    "    lr_texts2.append(lr_text2)\n",
    "    \n",
    "# Se dibuja un título\n",
    "titulo = fig.suptitle(f'step: {0}', fontsize=fontsize)\n",
    "\n",
    "# Se define la función que va a modificar la gráfica con la evolución del entrenamiento\n",
    "def update(i):\n",
    "\n",
    "    for l, _ in enumerate(LRs):\n",
    "        # Se actualiza la recta generada con la pendiente a aprendida\n",
    "        rectas[l].set_ydata(Zs[l][i])\n",
    "\n",
    "        # Se actualiza el gradiente de la función de error\n",
    "        gradientes1[l].set_xdata(Xs_linea_gradiente[l][i])\n",
    "        gradientes1[l].set_ydata(Ys_linea_gradiente[l][i])\n",
    "        gradientes2[l].set_xdata(Xs_linea_gradiente[l][i])\n",
    "        gradientes2[l].set_ydata(Ys_linea_gradiente[l][i])\n",
    "\n",
    "        # Se actualiza el punto 2. Punto donde se calcula el gradiente\n",
    "        puntos1[l].set_xdata(As[l][i])\n",
    "        puntos1[l].set_ydata(Errores[l][i])\n",
    "        puntos2[l].set_xdata(As[l][i])\n",
    "        puntos2[l].set_ydata(Errores[l][i])\n",
    "\n",
    "        # Se actualizan los textos\n",
    "        a_texts1[l].set_text(f'pesos = {As[l][i]:.5f}')\n",
    "        a_texts2[l].set_text(f'pesos = {As[l][i]:.5f}')\n",
    "        error_texts1[l].set_text(f'loss = {Errores[l][i]:.5f}')\n",
    "        error_texts2[l].set_text(f'loss = {Errores[l][i]:.5f}')\n",
    "        lr_texts1[l].set_text(f'lr = {LRs[l]}')\n",
    "        lr_texts2[l].set_text(f'lr = {LRs[l]}')\n",
    "    \n",
    "    titulo.set_text(f'step: {i}')\n",
    "\n",
    "    return line1, # ax1, #line2, punto2, ax2, a_text, error_text\n",
    "\n",
    "# Se crea la animación con un refresco cada 200 ms\n",
    "interval = 500 # ms\n",
    "anim = FuncAnimation(fig, update, frames=np.arange(0, steps), interval=interval)\n",
    "\n",
    "# Se guarda en un GIF\n",
    "gif_name = \"GIFs/LR_noDecay_zoom.gif\"\n",
    "anim.save(gif_name, dpi=80, writer='imagemagick')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LR sin decaimiento](GIFs/LR_noDecay_zoom.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, si se hace zoom, el punto va dando saltos al rededor del mínimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos qué pasa si vamos disminuyendo el valor del learning rate a medida que vamos entrenando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento para lr = 0.025\n",
      "i=10: error=3.5332415076132806, gradiente=4.434751401390874, a=1.945937013155588\n",
      "i=20: error=3.463169558318404, gradiente=0.11728176690111386, a=1.990209371273261\n",
      "\n",
      "Entrenamiento para lr = 0.025\n",
      "i=10: error=3.46312053385814, gradiente=0.010202610187635781, a=1.9914349189821918\n",
      "i=20: error=3.4631205160568017, gradiente=2.662718301434571e-06, a=1.9914120289624948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LRs = [2.5e-2, 2.5e-2]    # Tasa de aprendizaje o learning rate\n",
    "steps = 20  # Numero de veces que se realiza el bucle de enrtenamiento\n",
    "\n",
    "# Matrices donde se guardarán los datos para luego ver la evolución del entrenamiento en una gráfica\n",
    "Zs = np.empty([len(LRs), steps, len(x)])\n",
    "Xs_linea_gradiente = np.empty([len(LRs), steps, 3])\n",
    "Ys_linea_gradiente = np.empty([len(LRs), steps, 3])\n",
    "As = np.empty([len(LRs), steps])\n",
    "Errores = np.empty([len(LRs), steps])\n",
    "LR_values = np.empty([len(LRs), steps])\n",
    "lr_decay = 0\n",
    "\n",
    "for l, lr in enumerate(LRs):\n",
    "    # Inicialización aleatoria de a\n",
    "    random.seed(45)\n",
    "    a = random.random()\n",
    "    \n",
    "    print(f\"Entrenamiento para lr = {lr}\")\n",
    "    for i in range(steps):\n",
    "        # Calculamos el gradiente\n",
    "        dl = gradiente(a, x, y)\n",
    "\n",
    "        # Corregimos el valor de a\n",
    "        if l == 0:\n",
    "            lr_decay = lr\n",
    "            a -= lr * dl\n",
    "        else:\n",
    "            if i >= 0 and i < 5:\n",
    "                lr_decay = lr\n",
    "            elif i >= 5 and i < 10:\n",
    "                lr_decay = lr/2\n",
    "            elif i >= 10 and i < 15:\n",
    "                lr_decay = lr/3\n",
    "            elif i >= 15 and i < 20:\n",
    "                lr_decay = lr/4\n",
    "            a = a - lr_decay*dl\n",
    "\n",
    "        # Calculamos los valores que obtiene la red neuronal\n",
    "        z = a*x\n",
    "\n",
    "        # Obtenemos el error\n",
    "        error = loss_fn(y, z)\n",
    "\n",
    "        # Obtenemos las rectas de los gradientes para representarlas\n",
    "        x_linea_gradiente, y_linea_gradiente = gradiente_linea(3, a=a, error=error, gradiente=dl)\n",
    "\n",
    "        # Guardamos los valores para luego ver la evolución del entrenamiento en una gráfica\n",
    "        As[l][i] = a\n",
    "        Zs[l][i] = z\n",
    "        Errores[l][i] = error\n",
    "        Xs_linea_gradiente[l][i] = x_linea_gradiente\n",
    "        Ys_linea_gradiente[l][i] = y_linea_gradiente\n",
    "        LR_values[l][i] = lr_decay\n",
    "\n",
    "        # Imprimimos la evolución del entrenamiento\n",
    "        if (i+1)%10 == 0:\n",
    "            print(f\"i={i+1}: error={error}, gradiente={dl}, a={a}\")\n",
    "        \n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "# Creamos GIF con la evolución del entrenamiento\n",
    "rectas = []\n",
    "gradientes1 = []\n",
    "gradientes2 = []\n",
    "puntos1 = []\n",
    "puntos2 = []\n",
    "a_texts1 = []\n",
    "a_texts2 = []\n",
    "error_texts1 = []\n",
    "error_texts2 = []\n",
    "lr_texts1 = []\n",
    "lr_texts2 = []\n",
    "\n",
    "\n",
    "fontsize = 12\n",
    "\n",
    "# Creamos la gráfica inicial\n",
    "fig, ax = plt.subplots(len(LRs),3, figsize=(15, 5*len(LRs)))\n",
    "fig.set_tight_layout(True)\n",
    "for i in range(len(LRs)):\n",
    "    ax[i][0].set_xlabel('X')\n",
    "    ax[i][0].set_ylabel('Y  ', rotation=0)\n",
    "    ax[i][1].set_xlabel('a')\n",
    "    ax[i][1].set_ylabel('loss  ', rotation=0)\n",
    "    ax[i][2].set_xlabel('a')\n",
    "    ax[i][2].set_ylabel('loss  ', rotation=0)\n",
    "    ax[i][0].set_xlim(-0.5, 10.5)\n",
    "    ax[i][0].set_ylim(-0.5, 20.5)\n",
    "    ax[i][1].set_xlim(-0.5, 4.5)\n",
    "    ax[i][1].set_ylim(-0.5, 140.5)\n",
    "    ax[i][2].set_xlim(1.8, 2.2)\n",
    "    ax[i][2].set_ylim(3, 5.0)\n",
    "\n",
    "    # Se dibujan los datos que persistiran en toda la evolución de la gráfica\n",
    "    ax[i][0].scatter(x, y)\n",
    "    ax[i][1].plot(posibles_a, perdidas, linewidth = 3)\n",
    "    ax[i][2].plot(posibles_a, perdidas, linewidth = 3)\n",
    "\n",
    "    # Se dibuja el resto de lineas que irán cambiando durante el entrenamiento\n",
    "    recta, = ax[i][0].plot(x, Zs[i][0], 'k', linewidth=2)                             # Recta generada con la pendiente a aprendida\n",
    "    gradiente1, = ax[i][1].plot(Xs_linea_gradiente[i][0], Ys_linea_gradiente[i][0], 'g')   # Gradiente de la función de error\n",
    "    gradiente2, = ax[i][2].plot(Xs_linea_gradiente[i][0], Ys_linea_gradiente[i][0], 'g')   # Gradiente de la función de error\n",
    "    punto1, = ax[i][1].plot(As[i][0], Errores[i][0], 'r*')                             # Punto donde se calcula el gradiente\n",
    "    punto2, = ax[i][2].plot(As[i][0], Errores[i][0], 'r*')                             # Punto donde se calcula el gradiente\n",
    "    rectas.append(recta)\n",
    "    gradientes1.append(gradiente1)\n",
    "    gradientes2.append(gradiente2)\n",
    "    puntos1.append(punto1)\n",
    "    puntos2.append(punto2)\n",
    "\n",
    "    # Se dibujan textos dentro de la segunda figura del subplot\n",
    "    lr_text1 = ax[i][1].text(1, 100, f'lr = {LR_values[i][0]:.5f}', fontsize = fontsize)\n",
    "    lr_text2 = ax[i][2].text(1.9, 4.75, f'lr = {LR_values[i][0]:.5f}', fontsize = fontsize)\n",
    "    a_text1 = ax[i][1].text(1, 90, f'pesos = {As[i][0]:.5f}', fontsize = fontsize)\n",
    "    a_text2 = ax[i][2].text(1.9, 4.65, f'pesos = {As[i][0]:.5f}', fontsize = fontsize)\n",
    "    error_text1 = ax[i][1].text(1, 80, f'loss = {Errores[i][0]:.5f}', fontsize = fontsize)\n",
    "    error_text2 = ax[i][2].text(1.9, 4.55, f'loss = {Errores[i][0]:.5f}', fontsize = fontsize)\n",
    "    a_texts1.append(a_text1)\n",
    "    a_texts2.append(a_text2)\n",
    "    error_texts1.append(error_text1)\n",
    "    error_texts2.append(error_text2)\n",
    "    lr_texts1.append(lr_text1)\n",
    "    lr_texts2.append(lr_text2)\n",
    "    \n",
    "# Se dibuja un título\n",
    "titulo = fig.suptitle(f'step: {0}', fontsize=fontsize)\n",
    "\n",
    "# Se define la función que va a modificar la gráfica con la evolución del entrenamiento\n",
    "def update(i):\n",
    "\n",
    "    for l, _ in enumerate(LRs):\n",
    "        # Se actualiza la recta generada con la pendiente a aprendida\n",
    "        rectas[l].set_ydata(Zs[l][i])\n",
    "\n",
    "        # Se actualiza el gradiente de la función de error\n",
    "        gradientes1[l].set_xdata(Xs_linea_gradiente[l][i])\n",
    "        gradientes1[l].set_ydata(Ys_linea_gradiente[l][i])\n",
    "        gradientes2[l].set_xdata(Xs_linea_gradiente[l][i])\n",
    "        gradientes2[l].set_ydata(Ys_linea_gradiente[l][i])\n",
    "\n",
    "        # Se actualiza el punto 2. Punto donde se calcula el gradiente\n",
    "        puntos1[l].set_xdata(As[l][i])\n",
    "        puntos1[l].set_ydata(Errores[l][i])\n",
    "        puntos2[l].set_xdata(As[l][i])\n",
    "        puntos2[l].set_ydata(Errores[l][i])\n",
    "\n",
    "        # Se actualizan los textos\n",
    "        a_texts1[l].set_text(f'pesos = {As[l][i]:.5f}')\n",
    "        a_texts2[l].set_text(f'pesos = {As[l][i]:.5f}')\n",
    "        error_texts1[l].set_text(f'loss = {Errores[l][i]:.5f}')\n",
    "        error_texts2[l].set_text(f'loss = {Errores[l][i]:.5f}')\n",
    "        lr_texts1[l].set_text(f'lr = {LR_values[l][i]:.5f}')\n",
    "        lr_texts2[l].set_text(f'lr = {LR_values[l][i]:.5f}')\n",
    "    \n",
    "    titulo.set_text(f'step: {i}')\n",
    "\n",
    "    return line1, # ax1, #line2, punto2, ax2, a_text, error_text\n",
    "\n",
    "# Se crea la animación con un refresco cada 200 ms\n",
    "interval = 500 # ms\n",
    "anim = FuncAnimation(fig, update, frames=np.arange(0, steps), interval=interval)\n",
    "\n",
    "# Se guarda en un GIF\n",
    "gif_name = \"GIFs/LR_decay.gif\"\n",
    "anim.save(gif_name, dpi=80, writer='imagemagick')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LR sin decaimiento](GIFs/LR_decay.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, cuando estamos cerca del mínimo error, es buena idea ir disminuyendo el valor del learning rate para que se ajuste mejor al problema.\n",
    "\n",
    "Aquí se ha mostrado un ejemplo en el que se reduce el valor del learnig rate cada 5 steps, para que sea ilustrativo. Pero en la realidad, lo que se suele hacer es que si despues de varias épocas, el error, o la métrica que se esté usando, no mejora, se reduce el valor del learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación en Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cómo se hace esto en Pytorch. Vamos a verlo con el ejemplo del dataset de Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  type  \n",
       "0          0.4601                  0.11890     0  \n",
       "1          0.2750                  0.08902     0  \n",
       "2          0.3613                  0.08758     0  \n",
       "3          0.6638                  0.17300     0  \n",
       "4          0.2364                  0.07678     0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cancer_df = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n",
    "cancer_df['type'] = cancer['target']\n",
    "cancer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el dataset y el dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CancerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        cols = [col for col in dataframe.columns if col != 'target']\n",
    "        self.parameters = torch.from_numpy(dataframe[cols].values).type(torch.float32)\n",
    "        self.targets = torch.from_numpy(dataframe['type'].values).type(torch.float32)\n",
    "        self.targets = self.targets.reshape((len(self.targets), 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parameters)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        parameters = self.parameters[idx]\n",
    "        target = self.targets[idx]\n",
    "        return parameters, target\n",
    "\n",
    "ds = CancerDataset(cancer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso no se va a dividir el dataset en uno de entrenamiento y otro de validación, porque el dataset tiene tan pocos datos, que para poder hacer el ejemplo es necesario usar todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = ds\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BS_train = 64\n",
    "train_dl = DataLoader(train_ds, batch_size=BS_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CancerNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layers=[100, 50, 20]):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_inputs, hidden_layers[0]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[0], hidden_layers[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[1], hidden_layers[2]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[2], num_outputs),\n",
    "        )\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.network(x)\n",
    "        probs = self.activation(logits)\n",
    "        return logits, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos la red y la llevamos a la GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Se define una semilla para que la inicialización de los pesos aleatoria sea siempre la misma\n",
    "seed = 27\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "model = CancerNeuralNetwork(31, 1)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaramos la función de pérdida y el optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos ahora el método que irá reduciendo el valor del learning rate. Hay varios métodos de reducir el learning rate, pero vamos a ver solo dos. Uno en el que se reduce continuamente el learnirg rate de una manera exponencial, y otro en el que se reduce si la métrica que estemos usando no mejora\n",
    "\n",
    " 1. Metodo de reducción exponencial (ExponentialLR)\n",
    "  * El primer método multiplica el learning rate por un valor gamma, en este caso 0.9.\n",
    "  * Además ponemos el parámetro verbose a True para que se muestre el valor del learning rate\n",
    "  * El primer parámetro que se le pasa es optimizer, es decir, le tenemos que pasar el optimizador que hemos definido\n",
    "\n",
    "2. Método de reducción si la métrica no mejora (ReduceLROnPlateau)\n",
    "  * El primer parámetro es el optimizer, es decir, le tenemos que pasar el optimizador que hemos definido\n",
    "  * El tercer parámetro es el factor de reducción, en este caso 0.1\n",
    "  * El cuarto parámetro es el número de epocas que se tienen que pasar antes de reducir el learning rate, en este caso 10\n",
    "  * El quinto parámetro es el verbose, en este caso True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "# El primer método multiplica el learning rate por un valor gamma, en este caso 0.9.\n",
    "# Además ponemos el parámetro verbose a True para que se muestre el valor del learning rate\n",
    "# El primer parámetro que se le pasa es optimizer, es decir, le tenemos que pasar el optimizador \n",
    "# que hemos definido\n",
    "schedulerExponential = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, verbose=True)\n",
    "\n",
    "# El segundo método comprueba si en 10 épocas (patience=2), lam métrica no disminuye (mode='min')\n",
    "# Si no lo hace, se reduce el learning rate multiplicándolo por un valor factor, en este caso 0.1\n",
    "# Al igual que antes, ponemos el parámetro verbose a True para que se muestre el valor del learning rate\n",
    "schedulerOnPlateau = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el ciclo de entrenamiento. Ahora durante el ciclo de entrenamiento tendremos que llamar al método `step()` de cada uno de los optimizadores, pero en el caso de usar el método de reducción si la métrica no mejora, dentro del método `step()` hay que pasarle la métrica que estemos usando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último definimos la función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_prints = 4\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, scheduler=None, onPlateau=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # X and y to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        logits, probs = model(X)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % int(len(dataloader)/num_prints) == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    if scheduler is not None:\n",
    "        if onPlateau:\n",
    "            scheduler.step(loss)\n",
    "        else:\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comparar el entrenamiento sin reducción del learning rate, con cada uno de los métodos que hemos visto y con los dos a la vez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sin reducción del learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos todo desde cero y entrenamos la red sin reducir el learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Se define una semilla para que la inicialización de los pesos aleatoria sea siempre la misma\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "model = CancerNeuralNetwork(31, 1)\n",
    "model.to(device)\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "LR = 1e-3\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrena durante 500 épocas para ver mejor el efecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.010835  [    0/  569]\n",
      "loss: 4.976839  [  128/  569]\n",
      "loss: 1.088202  [  256/  569]\n",
      "loss: 0.989754  [  384/  569]\n",
      "loss: 2.056240  [  456/  569]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.452749  [    0/  569]\n",
      "loss: 0.852072  [  128/  569]\n",
      "loss: 1.305426  [  256/  569]\n",
      "loss: 1.547930  [  384/  569]\n",
      "loss: 0.906874  [  456/  569]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.809822  [    0/  569]\n",
      "loss: 0.795044  [  128/  569]\n",
      "loss: 0.638141  [  256/  569]\n",
      "loss: 0.690192  [  384/  569]\n",
      "loss: 1.183605  [  456/  569]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.689182  [    0/  569]\n",
      "loss: 0.708076  [  128/  569]\n",
      "loss: 0.727928  [  256/  569]\n",
      "loss: 0.612084  [  384/  569]\n",
      "loss: 0.569775  [  456/  569]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.583048  [    0/  569]\n",
      "loss: 0.605605  [  128/  569]\n",
      "loss: 0.597755  [  256/  569]\n",
      "loss: 0.593264  [  384/  569]\n",
      "loss: 0.555762  [  456/  569]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.560533  [    0/  569]\n",
      "loss: 0.606022  [  128/  569]\n",
      "loss: 0.649214  [  256/  569]\n",
      "loss: 0.582997  [  384/  569]\n",
      "loss: 0.674636  [  456/  569]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.589584  [    0/  569]\n",
      "loss: 0.567771  [  128/  569]\n",
      "loss: 0.540669  [  256/  569]\n",
      "loss: 0.572264  [  384/  569]\n",
      "loss: 0.547533  [  456/  569]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.549541  [    0/  569]\n",
      "loss: 0.527712  [  128/  569]\n",
      "loss: 0.706295  [  256/  569]\n",
      "loss: 0.676892  [  384/  569]\n",
      "loss: 0.608055  [  456/  569]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.576598  [    0/  569]\n",
      "loss: 0.563597  [  128/  569]\n",
      "loss: 0.565747  [  256/  569]\n",
      "loss: 0.560480  [  384/  569]\n",
      "loss: 0.542406  [  456/  569]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.533795  [    0/  569]\n",
      "loss: 0.524325  [  128/  569]\n",
      "loss: 0.508124  [  256/  569]\n",
      "loss: 0.552691  [  384/  569]\n",
      "loss: 0.673658  [  456/  569]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.740433  [    0/  569]\n",
      "loss: 0.502843  [  128/  569]\n",
      "loss: 0.480381  [  256/  569]\n",
      "loss: 0.501871  [  384/  569]\n",
      "loss: 0.523998  [  456/  569]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.462362  [    0/  569]\n",
      "loss: 0.687931  [  128/  569]\n",
      "loss: 0.607976  [  256/  569]\n",
      "loss: 0.617812  [  384/  569]\n",
      "loss: 0.524706  [  456/  569]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.649603  [    0/  569]\n",
      "loss: 0.719671  [  128/  569]\n",
      "loss: 0.527713  [  256/  569]\n",
      "loss: 0.476069  [  384/  569]\n",
      "loss: 0.582269  [  456/  569]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.595615  [    0/  569]\n",
      "loss: 0.548386  [  128/  569]\n",
      "loss: 0.668005  [  256/  569]\n",
      "loss: 0.610247  [  384/  569]\n",
      "loss: 0.571391  [  456/  569]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.648396  [    0/  569]\n",
      "loss: 0.442423  [  128/  569]\n",
      "loss: 0.425231  [  256/  569]\n",
      "loss: 0.532290  [  384/  569]\n",
      "loss: 0.506401  [  456/  569]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.522067  [    0/  569]\n",
      "loss: 0.485718  [  128/  569]\n",
      "loss: 0.497100  [  256/  569]\n",
      "loss: 0.680997  [  384/  569]\n",
      "loss: 0.562090  [  456/  569]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.529039  [    0/  569]\n",
      "loss: 0.449875  [  128/  569]\n",
      "loss: 0.484772  [  256/  569]\n",
      "loss: 0.465413  [  384/  569]\n",
      "loss: 0.405232  [  456/  569]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.508134  [    0/  569]\n",
      "loss: 0.465621  [  128/  569]\n",
      "loss: 0.483011  [  256/  569]\n",
      "loss: 0.436114  [  384/  569]\n",
      "loss: 0.419241  [  456/  569]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.444139  [    0/  569]\n",
      "loss: 0.469899  [  128/  569]\n",
      "loss: 0.420347  [  256/  569]\n",
      "loss: 0.415841  [  384/  569]\n",
      "loss: 0.468701  [  456/  569]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.404192  [    0/  569]\n",
      "loss: 0.448077  [  128/  569]\n",
      "loss: 0.495588  [  256/  569]\n",
      "loss: 0.531647  [  384/  569]\n",
      "loss: 0.425645  [  456/  569]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.396829  [    0/  569]\n",
      "loss: 0.604932  [  128/  569]\n",
      "loss: 0.737976  [  256/  569]\n",
      "loss: 0.791596  [  384/  569]\n",
      "loss: 0.433093  [  456/  569]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.479557  [    0/  569]\n",
      "loss: 0.442613  [  128/  569]\n",
      "loss: 0.462668  [  256/  569]\n",
      "loss: 0.396762  [  384/  569]\n",
      "loss: 0.456745  [  456/  569]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.447425  [    0/  569]\n",
      "loss: 0.498830  [  128/  569]\n",
      "loss: 0.384478  [  256/  569]\n",
      "loss: 0.507664  [  384/  569]\n",
      "loss: 0.487293  [  456/  569]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.528589  [    0/  569]\n",
      "loss: 0.455708  [  128/  569]\n",
      "loss: 0.454495  [  256/  569]\n",
      "loss: 0.456561  [  384/  569]\n",
      "loss: 0.414833  [  456/  569]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.460361  [    0/  569]\n",
      "loss: 0.422549  [  128/  569]\n",
      "loss: 0.443054  [  256/  569]\n",
      "loss: 0.474521  [  384/  569]\n",
      "loss: 0.525737  [  456/  569]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.464506  [    0/  569]\n",
      "loss: 0.376312  [  128/  569]\n",
      "loss: 0.418592  [  256/  569]\n",
      "loss: 0.373305  [  384/  569]\n",
      "loss: 0.538292  [  456/  569]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.468397  [    0/  569]\n",
      "loss: 0.503836  [  128/  569]\n",
      "loss: 0.418757  [  256/  569]\n",
      "loss: 0.520395  [  384/  569]\n",
      "loss: 0.389233  [  456/  569]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.525356  [    0/  569]\n",
      "loss: 0.403796  [  128/  569]\n",
      "loss: 0.420031  [  256/  569]\n",
      "loss: 0.373586  [  384/  569]\n",
      "loss: 0.296368  [  456/  569]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.374042  [    0/  569]\n",
      "loss: 0.398272  [  128/  569]\n",
      "loss: 0.429078  [  256/  569]\n",
      "loss: 0.406237  [  384/  569]\n",
      "loss: 0.374877  [  456/  569]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.398619  [    0/  569]\n",
      "loss: 0.412277  [  128/  569]\n",
      "loss: 0.402910  [  256/  569]\n",
      "loss: 0.349278  [  384/  569]\n",
      "loss: 0.450172  [  456/  569]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.369628  [    0/  569]\n",
      "loss: 0.372794  [  128/  569]\n",
      "loss: 0.312439  [  256/  569]\n",
      "loss: 0.974680  [  384/  569]\n",
      "loss: 0.638555  [  456/  569]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.757864  [    0/  569]\n",
      "loss: 0.429475  [  128/  569]\n",
      "loss: 0.364063  [  256/  569]\n",
      "loss: 0.333338  [  384/  569]\n",
      "loss: 0.420487  [  456/  569]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.371355  [    0/  569]\n",
      "loss: 0.373163  [  128/  569]\n",
      "loss: 0.487672  [  256/  569]\n",
      "loss: 0.495208  [  384/  569]\n",
      "loss: 0.661653  [  456/  569]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.451180  [    0/  569]\n",
      "loss: 0.521518  [  128/  569]\n",
      "loss: 0.615372  [  256/  569]\n",
      "loss: 0.487347  [  384/  569]\n",
      "loss: 0.328617  [  456/  569]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.431906  [    0/  569]\n",
      "loss: 0.327230  [  128/  569]\n",
      "loss: 0.392563  [  256/  569]\n",
      "loss: 0.352681  [  384/  569]\n",
      "loss: 0.443610  [  456/  569]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.498166  [    0/  569]\n",
      "loss: 0.368514  [  128/  569]\n",
      "loss: 0.465028  [  256/  569]\n",
      "loss: 0.411254  [  384/  569]\n",
      "loss: 0.326883  [  456/  569]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.412262  [    0/  569]\n",
      "loss: 0.412846  [  128/  569]\n",
      "loss: 0.421528  [  256/  569]\n",
      "loss: 0.510371  [  384/  569]\n",
      "loss: 0.416416  [  456/  569]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.448007  [    0/  569]\n",
      "loss: 0.346762  [  128/  569]\n",
      "loss: 0.339076  [  256/  569]\n",
      "loss: 0.327275  [  384/  569]\n",
      "loss: 0.601628  [  456/  569]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.429789  [    0/  569]\n",
      "loss: 0.376573  [  128/  569]\n",
      "loss: 0.351658  [  256/  569]\n",
      "loss: 0.345978  [  384/  569]\n",
      "loss: 0.439912  [  456/  569]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.566837  [    0/  569]\n",
      "loss: 0.580809  [  128/  569]\n",
      "loss: 0.446230  [  256/  569]\n",
      "loss: 0.482847  [  384/  569]\n",
      "loss: 0.366733  [  456/  569]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.320465  [    0/  569]\n",
      "loss: 0.369644  [  128/  569]\n",
      "loss: 0.315557  [  256/  569]\n",
      "loss: 0.488051  [  384/  569]\n",
      "loss: 0.373719  [  456/  569]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.395093  [    0/  569]\n",
      "loss: 0.355604  [  128/  569]\n",
      "loss: 0.366967  [  256/  569]\n",
      "loss: 0.323426  [  384/  569]\n",
      "loss: 0.560787  [  456/  569]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.822384  [    0/  569]\n",
      "loss: 0.558762  [  128/  569]\n",
      "loss: 0.379692  [  256/  569]\n",
      "loss: 0.371678  [  384/  569]\n",
      "loss: 0.281890  [  456/  569]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.325100  [    0/  569]\n",
      "loss: 0.304679  [  128/  569]\n",
      "loss: 0.448145  [  256/  569]\n",
      "loss: 0.762375  [  384/  569]\n",
      "loss: 0.360255  [  456/  569]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.441455  [    0/  569]\n",
      "loss: 0.401501  [  128/  569]\n",
      "loss: 0.354148  [  256/  569]\n",
      "loss: 0.332165  [  384/  569]\n",
      "loss: 0.406671  [  456/  569]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.395483  [    0/  569]\n",
      "loss: 0.298914  [  128/  569]\n",
      "loss: 0.327653  [  256/  569]\n",
      "loss: 0.463650  [  384/  569]\n",
      "loss: 0.398400  [  456/  569]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.422759  [    0/  569]\n",
      "loss: 0.340575  [  128/  569]\n",
      "loss: 0.357982  [  256/  569]\n",
      "loss: 0.336079  [  384/  569]\n",
      "loss: 0.273035  [  456/  569]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.427489  [    0/  569]\n",
      "loss: 0.361500  [  128/  569]\n",
      "loss: 0.296777  [  256/  569]\n",
      "loss: 0.326437  [  384/  569]\n",
      "loss: 0.387276  [  456/  569]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.427048  [    0/  569]\n",
      "loss: 0.386923  [  128/  569]\n",
      "loss: 0.335921  [  256/  569]\n",
      "loss: 0.437920  [  384/  569]\n",
      "loss: 0.245789  [  456/  569]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.307767  [    0/  569]\n",
      "loss: 0.314793  [  128/  569]\n",
      "loss: 0.346795  [  256/  569]\n",
      "loss: 0.561763  [  384/  569]\n",
      "loss: 0.430959  [  456/  569]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.351759  [    0/  569]\n",
      "loss: 0.363869  [  128/  569]\n",
      "loss: 0.439790  [  256/  569]\n",
      "loss: 0.485214  [  384/  569]\n",
      "loss: 0.363782  [  456/  569]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.387074  [    0/  569]\n",
      "loss: 0.351140  [  128/  569]\n",
      "loss: 0.582570  [  256/  569]\n",
      "loss: 0.310448  [  384/  569]\n",
      "loss: 0.426368  [  456/  569]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.275914  [    0/  569]\n",
      "loss: 0.286914  [  128/  569]\n",
      "loss: 0.397029  [  256/  569]\n",
      "loss: 0.311112  [  384/  569]\n",
      "loss: 0.399712  [  456/  569]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.265134  [    0/  569]\n",
      "loss: 0.335340  [  128/  569]\n",
      "loss: 0.299282  [  256/  569]\n",
      "loss: 0.300487  [  384/  569]\n",
      "loss: 0.517391  [  456/  569]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.377828  [    0/  569]\n",
      "loss: 0.334805  [  128/  569]\n",
      "loss: 0.336603  [  256/  569]\n",
      "loss: 0.348463  [  384/  569]\n",
      "loss: 0.482074  [  456/  569]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.357948  [    0/  569]\n",
      "loss: 0.367571  [  128/  569]\n",
      "loss: 0.437003  [  256/  569]\n",
      "loss: 0.465446  [  384/  569]\n",
      "loss: 0.397422  [  456/  569]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.298156  [    0/  569]\n",
      "loss: 0.410890  [  128/  569]\n",
      "loss: 0.299794  [  256/  569]\n",
      "loss: 0.184742  [  384/  569]\n",
      "loss: 0.294561  [  456/  569]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.363669  [    0/  569]\n",
      "loss: 0.373482  [  128/  569]\n",
      "loss: 0.351102  [  256/  569]\n",
      "loss: 0.503179  [  384/  569]\n",
      "loss: 0.204101  [  456/  569]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.271586  [    0/  569]\n",
      "loss: 0.296106  [  128/  569]\n",
      "loss: 0.344924  [  256/  569]\n",
      "loss: 0.332796  [  384/  569]\n",
      "loss: 0.413188  [  456/  569]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.484128  [    0/  569]\n",
      "loss: 0.391651  [  128/  569]\n",
      "loss: 0.637090  [  256/  569]\n",
      "loss: 0.244601  [  384/  569]\n",
      "loss: 0.246648  [  456/  569]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.433333  [    0/  569]\n",
      "loss: 0.417240  [  128/  569]\n",
      "loss: 0.321835  [  256/  569]\n",
      "loss: 0.349543  [  384/  569]\n",
      "loss: 0.283197  [  456/  569]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.394350  [    0/  569]\n",
      "loss: 0.312635  [  128/  569]\n",
      "loss: 0.350653  [  256/  569]\n",
      "loss: 0.296825  [  384/  569]\n",
      "loss: 0.319483  [  456/  569]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.382688  [    0/  569]\n",
      "loss: 0.326714  [  128/  569]\n",
      "loss: 0.444594  [  256/  569]\n",
      "loss: 0.635294  [  384/  569]\n",
      "loss: 0.428483  [  456/  569]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.571031  [    0/  569]\n",
      "loss: 0.489605  [  128/  569]\n",
      "loss: 0.323239  [  256/  569]\n",
      "loss: 0.340979  [  384/  569]\n",
      "loss: 0.314178  [  456/  569]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.394764  [    0/  569]\n",
      "loss: 0.211003  [  128/  569]\n",
      "loss: 0.479519  [  256/  569]\n",
      "loss: 0.379092  [  384/  569]\n",
      "loss: 0.326809  [  456/  569]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.340406  [    0/  569]\n",
      "loss: 0.381917  [  128/  569]\n",
      "loss: 0.481293  [  256/  569]\n",
      "loss: 0.223969  [  384/  569]\n",
      "loss: 0.304898  [  456/  569]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.276351  [    0/  569]\n",
      "loss: 0.249720  [  128/  569]\n",
      "loss: 0.440471  [  256/  569]\n",
      "loss: 0.276076  [  384/  569]\n",
      "loss: 0.299679  [  456/  569]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.422340  [    0/  569]\n",
      "loss: 0.360213  [  128/  569]\n",
      "loss: 0.566905  [  256/  569]\n",
      "loss: 0.310209  [  384/  569]\n",
      "loss: 0.320129  [  456/  569]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.339729  [    0/  569]\n",
      "loss: 0.286729  [  128/  569]\n",
      "loss: 0.284991  [  256/  569]\n",
      "loss: 0.312614  [  384/  569]\n",
      "loss: 0.188800  [  456/  569]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.372011  [    0/  569]\n",
      "loss: 0.242710  [  128/  569]\n",
      "loss: 0.198987  [  256/  569]\n",
      "loss: 0.271752  [  384/  569]\n",
      "loss: 0.450308  [  456/  569]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.523236  [    0/  569]\n",
      "loss: 0.535613  [  128/  569]\n",
      "loss: 0.222070  [  256/  569]\n",
      "loss: 0.237293  [  384/  569]\n",
      "loss: 0.292103  [  456/  569]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.384782  [    0/  569]\n",
      "loss: 0.287506  [  128/  569]\n",
      "loss: 0.261895  [  256/  569]\n",
      "loss: 0.358964  [  384/  569]\n",
      "loss: 0.409685  [  456/  569]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.327584  [    0/  569]\n",
      "loss: 0.391365  [  128/  569]\n",
      "loss: 0.267008  [  256/  569]\n",
      "loss: 0.333909  [  384/  569]\n",
      "loss: 0.297184  [  456/  569]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.370885  [    0/  569]\n",
      "loss: 0.243410  [  128/  569]\n",
      "loss: 0.262236  [  256/  569]\n",
      "loss: 0.260415  [  384/  569]\n",
      "loss: 0.282938  [  456/  569]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.329564  [    0/  569]\n",
      "loss: 0.404589  [  128/  569]\n",
      "loss: 0.599157  [  256/  569]\n",
      "loss: 0.795434  [  384/  569]\n",
      "loss: 0.644656  [  456/  569]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.478250  [    0/  569]\n",
      "loss: 0.250359  [  128/  569]\n",
      "loss: 0.284856  [  256/  569]\n",
      "loss: 0.368522  [  384/  569]\n",
      "loss: 0.311136  [  456/  569]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.312476  [    0/  569]\n",
      "loss: 0.250742  [  128/  569]\n",
      "loss: 0.223929  [  256/  569]\n",
      "loss: 0.341413  [  384/  569]\n",
      "loss: 0.274912  [  456/  569]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.311684  [    0/  569]\n",
      "loss: 0.282274  [  128/  569]\n",
      "loss: 0.268962  [  256/  569]\n",
      "loss: 0.288464  [  384/  569]\n",
      "loss: 0.317302  [  456/  569]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.339277  [    0/  569]\n",
      "loss: 0.405622  [  128/  569]\n",
      "loss: 0.360986  [  256/  569]\n",
      "loss: 0.502467  [  384/  569]\n",
      "loss: 0.658299  [  456/  569]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.524878  [    0/  569]\n",
      "loss: 0.291648  [  128/  569]\n",
      "loss: 0.245605  [  256/  569]\n",
      "loss: 0.282116  [  384/  569]\n",
      "loss: 0.395342  [  456/  569]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.254948  [    0/  569]\n",
      "loss: 0.293575  [  128/  569]\n",
      "loss: 0.263267  [  256/  569]\n",
      "loss: 0.348588  [  384/  569]\n",
      "loss: 0.320042  [  456/  569]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.271582  [    0/  569]\n",
      "loss: 0.210806  [  128/  569]\n",
      "loss: 0.283186  [  256/  569]\n",
      "loss: 0.806429  [  384/  569]\n",
      "loss: 0.458773  [  456/  569]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.270665  [    0/  569]\n",
      "loss: 0.492506  [  128/  569]\n",
      "loss: 0.226559  [  256/  569]\n",
      "loss: 0.368289  [  384/  569]\n",
      "loss: 0.437482  [  456/  569]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.347916  [    0/  569]\n",
      "loss: 0.284390  [  128/  569]\n",
      "loss: 0.339371  [  256/  569]\n",
      "loss: 0.311789  [  384/  569]\n",
      "loss: 0.320155  [  456/  569]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.238343  [    0/  569]\n",
      "loss: 0.332886  [  128/  569]\n",
      "loss: 0.318494  [  256/  569]\n",
      "loss: 0.260010  [  384/  569]\n",
      "loss: 0.351755  [  456/  569]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.448772  [    0/  569]\n",
      "loss: 0.346485  [  128/  569]\n",
      "loss: 0.226870  [  256/  569]\n",
      "loss: 0.323635  [  384/  569]\n",
      "loss: 0.318982  [  456/  569]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.210269  [    0/  569]\n",
      "loss: 0.271473  [  128/  569]\n",
      "loss: 0.251564  [  256/  569]\n",
      "loss: 0.229993  [  384/  569]\n",
      "loss: 0.271127  [  456/  569]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.253912  [    0/  569]\n",
      "loss: 0.376028  [  128/  569]\n",
      "loss: 0.739728  [  256/  569]\n",
      "loss: 0.481949  [  384/  569]\n",
      "loss: 0.281921  [  456/  569]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.271934  [    0/  569]\n",
      "loss: 0.272491  [  128/  569]\n",
      "loss: 0.260391  [  256/  569]\n",
      "loss: 0.358108  [  384/  569]\n",
      "loss: 0.318612  [  456/  569]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.292143  [    0/  569]\n",
      "loss: 0.326552  [  128/  569]\n",
      "loss: 0.187692  [  256/  569]\n",
      "loss: 0.373652  [  384/  569]\n",
      "loss: 0.273690  [  456/  569]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.312569  [    0/  569]\n",
      "loss: 0.254018  [  128/  569]\n",
      "loss: 0.265783  [  256/  569]\n",
      "loss: 0.170494  [  384/  569]\n",
      "loss: 0.288068  [  456/  569]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.283353  [    0/  569]\n",
      "loss: 0.208758  [  128/  569]\n",
      "loss: 0.276254  [  256/  569]\n",
      "loss: 0.290616  [  384/  569]\n",
      "loss: 0.244794  [  456/  569]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.222723  [    0/  569]\n",
      "loss: 0.272707  [  128/  569]\n",
      "loss: 0.301469  [  256/  569]\n",
      "loss: 0.395834  [  384/  569]\n",
      "loss: 0.193248  [  456/  569]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.281818  [    0/  569]\n",
      "loss: 0.168009  [  128/  569]\n",
      "loss: 0.279965  [  256/  569]\n",
      "loss: 0.203394  [  384/  569]\n",
      "loss: 0.321722  [  456/  569]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.397546  [    0/  569]\n",
      "loss: 0.532076  [  128/  569]\n",
      "loss: 0.334829  [  256/  569]\n",
      "loss: 0.252595  [  384/  569]\n",
      "loss: 0.231048  [  456/  569]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.163616  [    0/  569]\n",
      "loss: 0.237313  [  128/  569]\n",
      "loss: 0.310385  [  256/  569]\n",
      "loss: 0.387544  [  384/  569]\n",
      "loss: 0.257214  [  456/  569]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.309882  [    0/  569]\n",
      "loss: 0.328976  [  128/  569]\n",
      "loss: 0.406513  [  256/  569]\n",
      "loss: 0.323749  [  384/  569]\n",
      "loss: 0.230283  [  456/  569]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.309022  [    0/  569]\n",
      "loss: 0.207438  [  128/  569]\n",
      "loss: 0.366904  [  256/  569]\n",
      "loss: 0.394300  [  384/  569]\n",
      "loss: 0.506156  [  456/  569]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.387440  [    0/  569]\n",
      "loss: 0.232871  [  128/  569]\n",
      "loss: 0.217073  [  256/  569]\n",
      "loss: 0.323378  [  384/  569]\n",
      "loss: 0.357409  [  456/  569]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.481874  [    0/  569]\n",
      "loss: 0.607848  [  128/  569]\n",
      "loss: 0.396597  [  256/  569]\n",
      "loss: 0.301140  [  384/  569]\n",
      "loss: 0.294191  [  456/  569]\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.381265  [    0/  569]\n",
      "loss: 0.737795  [  128/  569]\n",
      "loss: 0.280064  [  256/  569]\n",
      "loss: 0.209956  [  384/  569]\n",
      "loss: 0.221385  [  456/  569]\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.268704  [    0/  569]\n",
      "loss: 0.248224  [  128/  569]\n",
      "loss: 0.218588  [  256/  569]\n",
      "loss: 0.201723  [  384/  569]\n",
      "loss: 0.303281  [  456/  569]\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.311045  [    0/  569]\n",
      "loss: 0.211050  [  128/  569]\n",
      "loss: 0.332838  [  256/  569]\n",
      "loss: 0.214874  [  384/  569]\n",
      "loss: 0.190515  [  456/  569]\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.234068  [    0/  569]\n",
      "loss: 0.299272  [  128/  569]\n",
      "loss: 0.468784  [  256/  569]\n",
      "loss: 0.302603  [  384/  569]\n",
      "loss: 0.234850  [  456/  569]\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.279707  [    0/  569]\n",
      "loss: 0.248469  [  128/  569]\n",
      "loss: 0.170531  [  256/  569]\n",
      "loss: 0.186995  [  384/  569]\n",
      "loss: 0.303153  [  456/  569]\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.236554  [    0/  569]\n",
      "loss: 0.289560  [  128/  569]\n",
      "loss: 0.209664  [  256/  569]\n",
      "loss: 0.139634  [  384/  569]\n",
      "loss: 0.278382  [  456/  569]\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.389272  [    0/  569]\n",
      "loss: 0.240945  [  128/  569]\n",
      "loss: 0.364737  [  256/  569]\n",
      "loss: 0.238526  [  384/  569]\n",
      "loss: 0.205083  [  456/  569]\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.245850  [    0/  569]\n",
      "loss: 0.321377  [  128/  569]\n",
      "loss: 0.352302  [  256/  569]\n",
      "loss: 0.292399  [  384/  569]\n",
      "loss: 0.821183  [  456/  569]\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.916706  [    0/  569]\n",
      "loss: 0.294811  [  128/  569]\n",
      "loss: 0.303491  [  256/  569]\n",
      "loss: 0.232366  [  384/  569]\n",
      "loss: 0.305242  [  456/  569]\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.227396  [    0/  569]\n",
      "loss: 0.278268  [  128/  569]\n",
      "loss: 0.387771  [  256/  569]\n",
      "loss: 0.182942  [  384/  569]\n",
      "loss: 0.234564  [  456/  569]\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.419033  [    0/  569]\n",
      "loss: 0.243289  [  128/  569]\n",
      "loss: 0.199207  [  256/  569]\n",
      "loss: 0.166755  [  384/  569]\n",
      "loss: 0.249132  [  456/  569]\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.296086  [    0/  569]\n",
      "loss: 0.321330  [  128/  569]\n",
      "loss: 0.416583  [  256/  569]\n",
      "loss: 0.267192  [  384/  569]\n",
      "loss: 0.234591  [  456/  569]\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.300500  [    0/  569]\n",
      "loss: 0.237498  [  128/  569]\n",
      "loss: 0.190156  [  256/  569]\n",
      "loss: 0.155744  [  384/  569]\n",
      "loss: 0.312886  [  456/  569]\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.326716  [    0/  569]\n",
      "loss: 0.248971  [  128/  569]\n",
      "loss: 0.284990  [  256/  569]\n",
      "loss: 0.248837  [  384/  569]\n",
      "loss: 0.266557  [  456/  569]\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.283078  [    0/  569]\n",
      "loss: 0.267730  [  128/  569]\n",
      "loss: 0.215607  [  256/  569]\n",
      "loss: 0.213470  [  384/  569]\n",
      "loss: 0.326364  [  456/  569]\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.346475  [    0/  569]\n",
      "loss: 0.263342  [  128/  569]\n",
      "loss: 0.260516  [  256/  569]\n",
      "loss: 0.162039  [  384/  569]\n",
      "loss: 0.331005  [  456/  569]\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.228234  [    0/  569]\n",
      "loss: 0.214155  [  128/  569]\n",
      "loss: 0.178977  [  256/  569]\n",
      "loss: 0.254624  [  384/  569]\n",
      "loss: 0.267090  [  456/  569]\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.214736  [    0/  569]\n",
      "loss: 0.500814  [  128/  569]\n",
      "loss: 0.325380  [  256/  569]\n",
      "loss: 0.260653  [  384/  569]\n",
      "loss: 0.257528  [  456/  569]\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.170431  [    0/  569]\n",
      "loss: 0.261270  [  128/  569]\n",
      "loss: 0.394282  [  256/  569]\n",
      "loss: 0.229973  [  384/  569]\n",
      "loss: 0.198646  [  456/  569]\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.355270  [    0/  569]\n",
      "loss: 0.235230  [  128/  569]\n",
      "loss: 0.383586  [  256/  569]\n",
      "loss: 0.301594  [  384/  569]\n",
      "loss: 0.267096  [  456/  569]\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.314457  [    0/  569]\n",
      "loss: 0.183009  [  128/  569]\n",
      "loss: 0.244516  [  256/  569]\n",
      "loss: 0.305295  [  384/  569]\n",
      "loss: 0.176457  [  456/  569]\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.296553  [    0/  569]\n",
      "loss: 0.179157  [  128/  569]\n",
      "loss: 0.245451  [  256/  569]\n",
      "loss: 0.328517  [  384/  569]\n",
      "loss: 0.614875  [  456/  569]\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.379264  [    0/  569]\n",
      "loss: 0.255908  [  128/  569]\n",
      "loss: 0.231535  [  256/  569]\n",
      "loss: 0.194038  [  384/  569]\n",
      "loss: 0.312170  [  456/  569]\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.219400  [    0/  569]\n",
      "loss: 0.224271  [  128/  569]\n",
      "loss: 0.196154  [  256/  569]\n",
      "loss: 0.252550  [  384/  569]\n",
      "loss: 0.083999  [  456/  569]\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.253772  [    0/  569]\n",
      "loss: 0.359022  [  128/  569]\n",
      "loss: 0.516997  [  256/  569]\n",
      "loss: 0.247472  [  384/  569]\n",
      "loss: 0.277286  [  456/  569]\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.343093  [    0/  569]\n",
      "loss: 0.177307  [  128/  569]\n",
      "loss: 0.238787  [  256/  569]\n",
      "loss: 0.236120  [  384/  569]\n",
      "loss: 0.346511  [  456/  569]\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.434717  [    0/  569]\n",
      "loss: 0.326754  [  128/  569]\n",
      "loss: 0.202813  [  256/  569]\n",
      "loss: 0.182066  [  384/  569]\n",
      "loss: 0.173318  [  456/  569]\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.248804  [    0/  569]\n",
      "loss: 0.214178  [  128/  569]\n",
      "loss: 0.233981  [  256/  569]\n",
      "loss: 0.257437  [  384/  569]\n",
      "loss: 0.280783  [  456/  569]\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.135404  [    0/  569]\n",
      "loss: 0.220256  [  128/  569]\n",
      "loss: 0.219219  [  256/  569]\n",
      "loss: 0.394990  [  384/  569]\n",
      "loss: 0.703868  [  456/  569]\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.658666  [    0/  569]\n",
      "loss: 0.327577  [  128/  569]\n",
      "loss: 0.235344  [  256/  569]\n",
      "loss: 0.287113  [  384/  569]\n",
      "loss: 0.195777  [  456/  569]\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.193732  [    0/  569]\n",
      "loss: 0.198490  [  128/  569]\n",
      "loss: 0.357025  [  256/  569]\n",
      "loss: 0.208325  [  384/  569]\n",
      "loss: 0.172392  [  456/  569]\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.218021  [    0/  569]\n",
      "loss: 0.204376  [  128/  569]\n",
      "loss: 0.208618  [  256/  569]\n",
      "loss: 0.193447  [  384/  569]\n",
      "loss: 0.381902  [  456/  569]\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.254665  [    0/  569]\n",
      "loss: 0.388136  [  128/  569]\n",
      "loss: 0.180051  [  256/  569]\n",
      "loss: 0.175094  [  384/  569]\n",
      "loss: 0.288438  [  456/  569]\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.465274  [    0/  569]\n",
      "loss: 0.141486  [  128/  569]\n",
      "loss: 0.185332  [  256/  569]\n",
      "loss: 0.275330  [  384/  569]\n",
      "loss: 0.295553  [  456/  569]\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.143024  [    0/  569]\n",
      "loss: 0.240371  [  128/  569]\n",
      "loss: 0.396607  [  256/  569]\n",
      "loss: 0.475303  [  384/  569]\n",
      "loss: 0.698130  [  456/  569]\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.263675  [    0/  569]\n",
      "loss: 0.242474  [  128/  569]\n",
      "loss: 0.156475  [  256/  569]\n",
      "loss: 0.228352  [  384/  569]\n",
      "loss: 0.154018  [  456/  569]\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.224891  [    0/  569]\n",
      "loss: 0.396626  [  128/  569]\n",
      "loss: 0.288242  [  256/  569]\n",
      "loss: 0.213187  [  384/  569]\n",
      "loss: 0.175226  [  456/  569]\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.240215  [    0/  569]\n",
      "loss: 0.271169  [  128/  569]\n",
      "loss: 0.204018  [  256/  569]\n",
      "loss: 0.194527  [  384/  569]\n",
      "loss: 0.241609  [  456/  569]\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.332916  [    0/  569]\n",
      "loss: 0.289151  [  128/  569]\n",
      "loss: 0.132748  [  256/  569]\n",
      "loss: 0.296374  [  384/  569]\n",
      "loss: 0.364371  [  456/  569]\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.244899  [    0/  569]\n",
      "loss: 0.444259  [  128/  569]\n",
      "loss: 0.190638  [  256/  569]\n",
      "loss: 0.171429  [  384/  569]\n",
      "loss: 0.300473  [  456/  569]\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.288702  [    0/  569]\n",
      "loss: 0.381510  [  128/  569]\n",
      "loss: 0.445070  [  256/  569]\n",
      "loss: 0.150632  [  384/  569]\n",
      "loss: 0.225644  [  456/  569]\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.264883  [    0/  569]\n",
      "loss: 0.285048  [  128/  569]\n",
      "loss: 0.271360  [  256/  569]\n",
      "loss: 0.167814  [  384/  569]\n",
      "loss: 0.156618  [  456/  569]\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.230699  [    0/  569]\n",
      "loss: 0.391112  [  128/  569]\n",
      "loss: 0.148977  [  256/  569]\n",
      "loss: 0.283546  [  384/  569]\n",
      "loss: 0.274474  [  456/  569]\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.329768  [    0/  569]\n",
      "loss: 0.246510  [  128/  569]\n",
      "loss: 0.247949  [  256/  569]\n",
      "loss: 0.200275  [  384/  569]\n",
      "loss: 0.260904  [  456/  569]\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.194042  [    0/  569]\n",
      "loss: 0.518583  [  128/  569]\n",
      "loss: 1.722648  [  256/  569]\n",
      "loss: 0.298283  [  384/  569]\n",
      "loss: 0.285817  [  456/  569]\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.281872  [    0/  569]\n",
      "loss: 0.276067  [  128/  569]\n",
      "loss: 0.207438  [  256/  569]\n",
      "loss: 0.243187  [  384/  569]\n",
      "loss: 0.206037  [  456/  569]\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.327582  [    0/  569]\n",
      "loss: 0.190032  [  128/  569]\n",
      "loss: 0.194794  [  256/  569]\n",
      "loss: 0.250714  [  384/  569]\n",
      "loss: 0.226957  [  456/  569]\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.151272  [    0/  569]\n",
      "loss: 0.262480  [  128/  569]\n",
      "loss: 0.154597  [  256/  569]\n",
      "loss: 0.270675  [  384/  569]\n",
      "loss: 0.218072  [  456/  569]\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.204548  [    0/  569]\n",
      "loss: 0.229919  [  128/  569]\n",
      "loss: 0.192861  [  256/  569]\n",
      "loss: 0.271984  [  384/  569]\n",
      "loss: 0.326056  [  456/  569]\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.240088  [    0/  569]\n",
      "loss: 0.217252  [  128/  569]\n",
      "loss: 0.244345  [  256/  569]\n",
      "loss: 0.181875  [  384/  569]\n",
      "loss: 0.267123  [  456/  569]\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.171519  [    0/  569]\n",
      "loss: 0.207162  [  128/  569]\n",
      "loss: 0.281522  [  256/  569]\n",
      "loss: 0.282896  [  384/  569]\n",
      "loss: 0.356730  [  456/  569]\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.310464  [    0/  569]\n",
      "loss: 0.295412  [  128/  569]\n",
      "loss: 0.377239  [  256/  569]\n",
      "loss: 0.117674  [  384/  569]\n",
      "loss: 0.350985  [  456/  569]\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.309267  [    0/  569]\n",
      "loss: 0.367217  [  128/  569]\n",
      "loss: 0.142863  [  256/  569]\n",
      "loss: 0.235451  [  384/  569]\n",
      "loss: 0.236969  [  456/  569]\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.246380  [    0/  569]\n",
      "loss: 0.352679  [  128/  569]\n",
      "loss: 0.382496  [  256/  569]\n",
      "loss: 0.264879  [  384/  569]\n",
      "loss: 0.364053  [  456/  569]\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.343801  [    0/  569]\n",
      "loss: 0.238149  [  128/  569]\n",
      "loss: 0.218155  [  256/  569]\n",
      "loss: 0.204712  [  384/  569]\n",
      "loss: 0.180020  [  456/  569]\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.257447  [    0/  569]\n",
      "loss: 0.187398  [  128/  569]\n",
      "loss: 0.283691  [  256/  569]\n",
      "loss: 0.178762  [  384/  569]\n",
      "loss: 0.291968  [  456/  569]\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.259315  [    0/  569]\n",
      "loss: 0.244056  [  128/  569]\n",
      "loss: 0.203419  [  256/  569]\n",
      "loss: 0.222356  [  384/  569]\n",
      "loss: 0.161016  [  456/  569]\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.219294  [    0/  569]\n",
      "loss: 0.216350  [  128/  569]\n",
      "loss: 0.276268  [  256/  569]\n",
      "loss: 0.268866  [  384/  569]\n",
      "loss: 0.156634  [  456/  569]\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.189385  [    0/  569]\n",
      "loss: 0.271581  [  128/  569]\n",
      "loss: 0.243617  [  256/  569]\n",
      "loss: 0.211458  [  384/  569]\n",
      "loss: 0.236215  [  456/  569]\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.300998  [    0/  569]\n",
      "loss: 0.149633  [  128/  569]\n",
      "loss: 0.242864  [  256/  569]\n",
      "loss: 0.256878  [  384/  569]\n",
      "loss: 0.157673  [  456/  569]\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.203928  [    0/  569]\n",
      "loss: 0.201050  [  128/  569]\n",
      "loss: 0.409210  [  256/  569]\n",
      "loss: 0.218577  [  384/  569]\n",
      "loss: 0.360151  [  456/  569]\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.287795  [    0/  569]\n",
      "loss: 0.404771  [  128/  569]\n",
      "loss: 0.158965  [  256/  569]\n",
      "loss: 0.707101  [  384/  569]\n",
      "loss: 0.242152  [  456/  569]\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.151255  [    0/  569]\n",
      "loss: 0.215456  [  128/  569]\n",
      "loss: 0.162326  [  256/  569]\n",
      "loss: 0.321903  [  384/  569]\n",
      "loss: 0.280888  [  456/  569]\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.248194  [    0/  569]\n",
      "loss: 0.355221  [  128/  569]\n",
      "loss: 0.256477  [  256/  569]\n",
      "loss: 0.230327  [  384/  569]\n",
      "loss: 0.180636  [  456/  569]\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.193181  [    0/  569]\n",
      "loss: 0.274440  [  128/  569]\n",
      "loss: 0.220287  [  256/  569]\n",
      "loss: 0.210443  [  384/  569]\n",
      "loss: 0.287395  [  456/  569]\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.211118  [    0/  569]\n",
      "loss: 0.262104  [  128/  569]\n",
      "loss: 0.254990  [  256/  569]\n",
      "loss: 0.243102  [  384/  569]\n",
      "loss: 0.142163  [  456/  569]\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.284562  [    0/  569]\n",
      "loss: 0.177188  [  128/  569]\n",
      "loss: 0.160431  [  256/  569]\n",
      "loss: 0.299807  [  384/  569]\n",
      "loss: 0.228645  [  456/  569]\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.124025  [    0/  569]\n",
      "loss: 0.259706  [  128/  569]\n",
      "loss: 0.213444  [  256/  569]\n",
      "loss: 0.327808  [  384/  569]\n",
      "loss: 0.281419  [  456/  569]\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.290611  [    0/  569]\n",
      "loss: 0.418240  [  128/  569]\n",
      "loss: 0.359720  [  256/  569]\n",
      "loss: 0.187157  [  384/  569]\n",
      "loss: 0.226734  [  456/  569]\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.265596  [    0/  569]\n",
      "loss: 0.325863  [  128/  569]\n",
      "loss: 0.364734  [  256/  569]\n",
      "loss: 0.197244  [  384/  569]\n",
      "loss: 0.205293  [  456/  569]\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.188275  [    0/  569]\n",
      "loss: 0.313722  [  128/  569]\n",
      "loss: 0.248463  [  256/  569]\n",
      "loss: 0.244931  [  384/  569]\n",
      "loss: 0.205981  [  456/  569]\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.155194  [    0/  569]\n",
      "loss: 0.204892  [  128/  569]\n",
      "loss: 0.237994  [  256/  569]\n",
      "loss: 0.323471  [  384/  569]\n",
      "loss: 0.143390  [  456/  569]\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.278527  [    0/  569]\n",
      "loss: 0.253532  [  128/  569]\n",
      "loss: 0.281452  [  256/  569]\n",
      "loss: 0.287703  [  384/  569]\n",
      "loss: 0.270633  [  456/  569]\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.168075  [    0/  569]\n",
      "loss: 0.301506  [  128/  569]\n",
      "loss: 0.202258  [  256/  569]\n",
      "loss: 0.166887  [  384/  569]\n",
      "loss: 0.249747  [  456/  569]\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.166657  [    0/  569]\n",
      "loss: 0.208326  [  128/  569]\n",
      "loss: 0.166090  [  256/  569]\n",
      "loss: 0.411861  [  384/  569]\n",
      "loss: 0.107739  [  456/  569]\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.391349  [    0/  569]\n",
      "loss: 0.260246  [  128/  569]\n",
      "loss: 0.181196  [  256/  569]\n",
      "loss: 0.230803  [  384/  569]\n",
      "loss: 0.276407  [  456/  569]\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.468270  [    0/  569]\n",
      "loss: 0.325449  [  128/  569]\n",
      "loss: 0.286767  [  256/  569]\n",
      "loss: 0.220735  [  384/  569]\n",
      "loss: 0.197833  [  456/  569]\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.263745  [    0/  569]\n",
      "loss: 0.308789  [  128/  569]\n",
      "loss: 0.190753  [  256/  569]\n",
      "loss: 0.176965  [  384/  569]\n",
      "loss: 0.253683  [  456/  569]\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.201064  [    0/  569]\n",
      "loss: 0.611922  [  128/  569]\n",
      "loss: 0.422976  [  256/  569]\n",
      "loss: 0.229094  [  384/  569]\n",
      "loss: 0.167228  [  456/  569]\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.259664  [    0/  569]\n",
      "loss: 0.236801  [  128/  569]\n",
      "loss: 0.523377  [  256/  569]\n",
      "loss: 0.538473  [  384/  569]\n",
      "loss: 0.464273  [  456/  569]\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.273329  [    0/  569]\n",
      "loss: 0.320544  [  128/  569]\n",
      "loss: 0.286394  [  256/  569]\n",
      "loss: 0.199871  [  384/  569]\n",
      "loss: 0.438095  [  456/  569]\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.566425  [    0/  569]\n",
      "loss: 0.264955  [  128/  569]\n",
      "loss: 0.292714  [  256/  569]\n",
      "loss: 0.161090  [  384/  569]\n",
      "loss: 0.208849  [  456/  569]\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.297303  [    0/  569]\n",
      "loss: 0.218957  [  128/  569]\n",
      "loss: 0.166391  [  256/  569]\n",
      "loss: 0.198560  [  384/  569]\n",
      "loss: 0.252203  [  456/  569]\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.122799  [    0/  569]\n",
      "loss: 0.279460  [  128/  569]\n",
      "loss: 0.229013  [  256/  569]\n",
      "loss: 0.282917  [  384/  569]\n",
      "loss: 0.168804  [  456/  569]\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.245984  [    0/  569]\n",
      "loss: 0.211677  [  128/  569]\n",
      "loss: 0.321933  [  256/  569]\n",
      "loss: 0.286667  [  384/  569]\n",
      "loss: 0.209530  [  456/  569]\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.217090  [    0/  569]\n",
      "loss: 0.145297  [  128/  569]\n",
      "loss: 0.175831  [  256/  569]\n",
      "loss: 0.264400  [  384/  569]\n",
      "loss: 0.241675  [  456/  569]\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.191631  [    0/  569]\n",
      "loss: 0.126954  [  128/  569]\n",
      "loss: 0.345068  [  256/  569]\n",
      "loss: 0.332162  [  384/  569]\n",
      "loss: 0.339458  [  456/  569]\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.192285  [    0/  569]\n",
      "loss: 0.209637  [  128/  569]\n",
      "loss: 0.207035  [  256/  569]\n",
      "loss: 0.260653  [  384/  569]\n",
      "loss: 0.235112  [  456/  569]\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.206378  [    0/  569]\n",
      "loss: 0.269067  [  128/  569]\n",
      "loss: 0.183341  [  256/  569]\n",
      "loss: 0.265275  [  384/  569]\n",
      "loss: 0.180424  [  456/  569]\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.287351  [    0/  569]\n",
      "loss: 0.100757  [  128/  569]\n",
      "loss: 0.241423  [  256/  569]\n",
      "loss: 0.257533  [  384/  569]\n",
      "loss: 0.174414  [  456/  569]\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.165546  [    0/  569]\n",
      "loss: 0.380478  [  128/  569]\n",
      "loss: 0.218231  [  256/  569]\n",
      "loss: 0.198305  [  384/  569]\n",
      "loss: 0.187186  [  456/  569]\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.188627  [    0/  569]\n",
      "loss: 0.211070  [  128/  569]\n",
      "loss: 0.217578  [  256/  569]\n",
      "loss: 0.215661  [  384/  569]\n",
      "loss: 0.316501  [  456/  569]\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.324701  [    0/  569]\n",
      "loss: 0.168161  [  128/  569]\n",
      "loss: 0.194439  [  256/  569]\n",
      "loss: 0.235250  [  384/  569]\n",
      "loss: 0.191551  [  456/  569]\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.200141  [    0/  569]\n",
      "loss: 0.199363  [  128/  569]\n",
      "loss: 0.347962  [  256/  569]\n",
      "loss: 0.264672  [  384/  569]\n",
      "loss: 0.521323  [  456/  569]\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.424229  [    0/  569]\n",
      "loss: 0.249752  [  128/  569]\n",
      "loss: 0.282908  [  256/  569]\n",
      "loss: 0.263421  [  384/  569]\n",
      "loss: 0.282294  [  456/  569]\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.180183  [    0/  569]\n",
      "loss: 0.172730  [  128/  569]\n",
      "loss: 0.215825  [  256/  569]\n",
      "loss: 0.225427  [  384/  569]\n",
      "loss: 0.216165  [  456/  569]\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.204760  [    0/  569]\n",
      "loss: 0.142634  [  128/  569]\n",
      "loss: 0.421790  [  256/  569]\n",
      "loss: 0.136537  [  384/  569]\n",
      "loss: 0.256358  [  456/  569]\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.232612  [    0/  569]\n",
      "loss: 0.336892  [  128/  569]\n",
      "loss: 0.175105  [  256/  569]\n",
      "loss: 0.190370  [  384/  569]\n",
      "loss: 0.162583  [  456/  569]\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.242657  [    0/  569]\n",
      "loss: 0.212100  [  128/  569]\n",
      "loss: 0.454273  [  256/  569]\n",
      "loss: 0.120175  [  384/  569]\n",
      "loss: 0.127243  [  456/  569]\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.127905  [    0/  569]\n",
      "loss: 0.323547  [  128/  569]\n",
      "loss: 0.271706  [  256/  569]\n",
      "loss: 0.154200  [  384/  569]\n",
      "loss: 0.187375  [  456/  569]\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.223683  [    0/  569]\n",
      "loss: 0.232063  [  128/  569]\n",
      "loss: 0.165596  [  256/  569]\n",
      "loss: 0.322246  [  384/  569]\n",
      "loss: 0.345793  [  456/  569]\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.279206  [    0/  569]\n",
      "loss: 0.117613  [  128/  569]\n",
      "loss: 0.294590  [  256/  569]\n",
      "loss: 0.099208  [  384/  569]\n",
      "loss: 0.280672  [  456/  569]\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.285264  [    0/  569]\n",
      "loss: 0.230543  [  128/  569]\n",
      "loss: 0.186840  [  256/  569]\n",
      "loss: 0.185573  [  384/  569]\n",
      "loss: 0.217225  [  456/  569]\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.199111  [    0/  569]\n",
      "loss: 0.260630  [  128/  569]\n",
      "loss: 0.396482  [  256/  569]\n",
      "loss: 0.323223  [  384/  569]\n",
      "loss: 0.231476  [  456/  569]\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.176225  [    0/  569]\n",
      "loss: 0.224707  [  128/  569]\n",
      "loss: 0.167968  [  256/  569]\n",
      "loss: 0.263547  [  384/  569]\n",
      "loss: 0.264271  [  456/  569]\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.161263  [    0/  569]\n",
      "loss: 0.187897  [  128/  569]\n",
      "loss: 0.295340  [  256/  569]\n",
      "loss: 0.237744  [  384/  569]\n",
      "loss: 0.153700  [  456/  569]\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.169233  [    0/  569]\n",
      "loss: 0.152913  [  128/  569]\n",
      "loss: 0.233810  [  256/  569]\n",
      "loss: 0.305664  [  384/  569]\n",
      "loss: 0.163747  [  456/  569]\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.151900  [    0/  569]\n",
      "loss: 0.299954  [  128/  569]\n",
      "loss: 0.344779  [  256/  569]\n",
      "loss: 0.205063  [  384/  569]\n",
      "loss: 0.121988  [  456/  569]\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.315634  [    0/  569]\n",
      "loss: 0.326050  [  128/  569]\n",
      "loss: 0.190146  [  256/  569]\n",
      "loss: 0.255605  [  384/  569]\n",
      "loss: 0.226442  [  456/  569]\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.165121  [    0/  569]\n",
      "loss: 0.203795  [  128/  569]\n",
      "loss: 0.405690  [  256/  569]\n",
      "loss: 0.312325  [  384/  569]\n",
      "loss: 0.243182  [  456/  569]\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.161630  [    0/  569]\n",
      "loss: 0.253209  [  128/  569]\n",
      "loss: 0.204672  [  256/  569]\n",
      "loss: 0.218442  [  384/  569]\n",
      "loss: 0.152070  [  456/  569]\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.233575  [    0/  569]\n",
      "loss: 0.225934  [  128/  569]\n",
      "loss: 0.277390  [  256/  569]\n",
      "loss: 0.096934  [  384/  569]\n",
      "loss: 0.187261  [  456/  569]\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.293307  [    0/  569]\n",
      "loss: 0.342715  [  128/  569]\n",
      "loss: 0.318939  [  256/  569]\n",
      "loss: 0.508225  [  384/  569]\n",
      "loss: 0.222444  [  456/  569]\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.147572  [    0/  569]\n",
      "loss: 0.194170  [  128/  569]\n",
      "loss: 0.185828  [  256/  569]\n",
      "loss: 0.257601  [  384/  569]\n",
      "loss: 0.167047  [  456/  569]\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.168362  [    0/  569]\n",
      "loss: 0.351782  [  128/  569]\n",
      "loss: 0.318425  [  256/  569]\n",
      "loss: 0.177648  [  384/  569]\n",
      "loss: 0.157700  [  456/  569]\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.180656  [    0/  569]\n",
      "loss: 0.191725  [  128/  569]\n",
      "loss: 0.361187  [  256/  569]\n",
      "loss: 0.163330  [  384/  569]\n",
      "loss: 0.160302  [  456/  569]\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.260382  [    0/  569]\n",
      "loss: 0.191438  [  128/  569]\n",
      "loss: 0.310052  [  256/  569]\n",
      "loss: 0.081616  [  384/  569]\n",
      "loss: 0.235894  [  456/  569]\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.276874  [    0/  569]\n",
      "loss: 0.460201  [  128/  569]\n",
      "loss: 0.357364  [  256/  569]\n",
      "loss: 0.110517  [  384/  569]\n",
      "loss: 0.156998  [  456/  569]\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.219770  [    0/  569]\n",
      "loss: 0.238644  [  128/  569]\n",
      "loss: 0.279635  [  256/  569]\n",
      "loss: 0.162576  [  384/  569]\n",
      "loss: 0.202295  [  456/  569]\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.296541  [    0/  569]\n",
      "loss: 0.244216  [  128/  569]\n",
      "loss: 0.223440  [  256/  569]\n",
      "loss: 0.137273  [  384/  569]\n",
      "loss: 0.397268  [  456/  569]\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.677348  [    0/  569]\n",
      "loss: 0.410329  [  128/  569]\n",
      "loss: 0.264596  [  256/  569]\n",
      "loss: 0.256369  [  384/  569]\n",
      "loss: 0.171740  [  456/  569]\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.260909  [    0/  569]\n",
      "loss: 0.258911  [  128/  569]\n",
      "loss: 0.268393  [  256/  569]\n",
      "loss: 0.192666  [  384/  569]\n",
      "loss: 0.226622  [  456/  569]\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.206668  [    0/  569]\n",
      "loss: 0.205254  [  128/  569]\n",
      "loss: 0.221982  [  256/  569]\n",
      "loss: 0.289379  [  384/  569]\n",
      "loss: 0.259444  [  456/  569]\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.187331  [    0/  569]\n",
      "loss: 0.393108  [  128/  569]\n",
      "loss: 0.348858  [  256/  569]\n",
      "loss: 0.274334  [  384/  569]\n",
      "loss: 0.226692  [  456/  569]\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.143365  [    0/  569]\n",
      "loss: 0.169285  [  128/  569]\n",
      "loss: 0.112838  [  256/  569]\n",
      "loss: 0.425921  [  384/  569]\n",
      "loss: 0.356486  [  456/  569]\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.109134  [    0/  569]\n",
      "loss: 0.235849  [  128/  569]\n",
      "loss: 0.181299  [  256/  569]\n",
      "loss: 0.182513  [  384/  569]\n",
      "loss: 0.287262  [  456/  569]\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.245728  [    0/  569]\n",
      "loss: 0.173069  [  128/  569]\n",
      "loss: 0.512711  [  256/  569]\n",
      "loss: 0.303599  [  384/  569]\n",
      "loss: 0.164646  [  456/  569]\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.203290  [    0/  569]\n",
      "loss: 0.242828  [  128/  569]\n",
      "loss: 0.262461  [  256/  569]\n",
      "loss: 0.190945  [  384/  569]\n",
      "loss: 0.286414  [  456/  569]\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.249121  [    0/  569]\n",
      "loss: 0.205320  [  128/  569]\n",
      "loss: 0.294301  [  256/  569]\n",
      "loss: 0.230189  [  384/  569]\n",
      "loss: 0.146031  [  456/  569]\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.254694  [    0/  569]\n",
      "loss: 0.231288  [  128/  569]\n",
      "loss: 0.181105  [  256/  569]\n",
      "loss: 0.158078  [  384/  569]\n",
      "loss: 0.149707  [  456/  569]\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.299389  [    0/  569]\n",
      "loss: 0.320371  [  128/  569]\n",
      "loss: 0.342869  [  256/  569]\n",
      "loss: 0.214147  [  384/  569]\n",
      "loss: 0.083359  [  456/  569]\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.166914  [    0/  569]\n",
      "loss: 0.247858  [  128/  569]\n",
      "loss: 0.280463  [  256/  569]\n",
      "loss: 0.309897  [  384/  569]\n",
      "loss: 0.160238  [  456/  569]\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.256635  [    0/  569]\n",
      "loss: 0.248773  [  128/  569]\n",
      "loss: 0.156112  [  256/  569]\n",
      "loss: 0.194139  [  384/  569]\n",
      "loss: 0.311108  [  456/  569]\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.187498  [    0/  569]\n",
      "loss: 0.171221  [  128/  569]\n",
      "loss: 0.280348  [  256/  569]\n",
      "loss: 0.279294  [  384/  569]\n",
      "loss: 0.257039  [  456/  569]\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.318764  [    0/  569]\n",
      "loss: 0.421397  [  128/  569]\n",
      "loss: 0.204763  [  256/  569]\n",
      "loss: 0.202126  [  384/  569]\n",
      "loss: 0.285949  [  456/  569]\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.281502  [    0/  569]\n",
      "loss: 0.354170  [  128/  569]\n",
      "loss: 0.182140  [  256/  569]\n",
      "loss: 0.300635  [  384/  569]\n",
      "loss: 0.190641  [  456/  569]\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.211962  [    0/  569]\n",
      "loss: 0.328133  [  128/  569]\n",
      "loss: 0.196278  [  256/  569]\n",
      "loss: 0.303882  [  384/  569]\n",
      "loss: 0.104889  [  456/  569]\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.210565  [    0/  569]\n",
      "loss: 0.265215  [  128/  569]\n",
      "loss: 0.343207  [  256/  569]\n",
      "loss: 0.176689  [  384/  569]\n",
      "loss: 0.264636  [  456/  569]\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.208806  [    0/  569]\n",
      "loss: 0.197577  [  128/  569]\n",
      "loss: 0.171695  [  256/  569]\n",
      "loss: 0.154566  [  384/  569]\n",
      "loss: 0.161278  [  456/  569]\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.296547  [    0/  569]\n",
      "loss: 0.296188  [  128/  569]\n",
      "loss: 0.332966  [  256/  569]\n",
      "loss: 0.282301  [  384/  569]\n",
      "loss: 0.212872  [  456/  569]\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.302676  [    0/  569]\n",
      "loss: 0.430795  [  128/  569]\n",
      "loss: 0.310251  [  256/  569]\n",
      "loss: 0.329379  [  384/  569]\n",
      "loss: 0.152004  [  456/  569]\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.237536  [    0/  569]\n",
      "loss: 0.208711  [  128/  569]\n",
      "loss: 0.224988  [  256/  569]\n",
      "loss: 0.240075  [  384/  569]\n",
      "loss: 0.210586  [  456/  569]\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.243459  [    0/  569]\n",
      "loss: 0.217698  [  128/  569]\n",
      "loss: 0.286586  [  256/  569]\n",
      "loss: 0.119877  [  384/  569]\n",
      "loss: 0.188732  [  456/  569]\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.252366  [    0/  569]\n",
      "loss: 0.236689  [  128/  569]\n",
      "loss: 0.234937  [  256/  569]\n",
      "loss: 0.190497  [  384/  569]\n",
      "loss: 0.167861  [  456/  569]\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.256209  [    0/  569]\n",
      "loss: 0.119634  [  128/  569]\n",
      "loss: 0.206464  [  256/  569]\n",
      "loss: 0.306656  [  384/  569]\n",
      "loss: 0.221863  [  456/  569]\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.207474  [    0/  569]\n",
      "loss: 0.196515  [  128/  569]\n",
      "loss: 0.226142  [  256/  569]\n",
      "loss: 0.278305  [  384/  569]\n",
      "loss: 0.185163  [  456/  569]\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.136452  [    0/  569]\n",
      "loss: 0.230466  [  128/  569]\n",
      "loss: 0.321440  [  256/  569]\n",
      "loss: 0.318558  [  384/  569]\n",
      "loss: 0.178605  [  456/  569]\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.174076  [    0/  569]\n",
      "loss: 0.197915  [  128/  569]\n",
      "loss: 0.334383  [  256/  569]\n",
      "loss: 0.294888  [  384/  569]\n",
      "loss: 0.216862  [  456/  569]\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.193540  [    0/  569]\n",
      "loss: 0.296803  [  128/  569]\n",
      "loss: 0.284297  [  256/  569]\n",
      "loss: 0.267851  [  384/  569]\n",
      "loss: 0.167254  [  456/  569]\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.222281  [    0/  569]\n",
      "loss: 0.155087  [  128/  569]\n",
      "loss: 0.316531  [  256/  569]\n",
      "loss: 0.178452  [  384/  569]\n",
      "loss: 0.170373  [  456/  569]\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.189137  [    0/  569]\n",
      "loss: 0.186171  [  128/  569]\n",
      "loss: 0.219336  [  256/  569]\n",
      "loss: 0.256293  [  384/  569]\n",
      "loss: 0.339315  [  456/  569]\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.302609  [    0/  569]\n",
      "loss: 0.179117  [  128/  569]\n",
      "loss: 0.204370  [  256/  569]\n",
      "loss: 0.176471  [  384/  569]\n",
      "loss: 0.369603  [  456/  569]\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.449318  [    0/  569]\n",
      "loss: 0.209409  [  128/  569]\n",
      "loss: 0.202489  [  256/  569]\n",
      "loss: 0.258485  [  384/  569]\n",
      "loss: 0.118976  [  456/  569]\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.478766  [    0/  569]\n",
      "loss: 0.217398  [  128/  569]\n",
      "loss: 0.207916  [  256/  569]\n",
      "loss: 0.241990  [  384/  569]\n",
      "loss: 0.244060  [  456/  569]\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.180668  [    0/  569]\n",
      "loss: 0.279544  [  128/  569]\n",
      "loss: 0.230549  [  256/  569]\n",
      "loss: 0.204626  [  384/  569]\n",
      "loss: 0.186548  [  456/  569]\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.244116  [    0/  569]\n",
      "loss: 0.258248  [  128/  569]\n",
      "loss: 0.214720  [  256/  569]\n",
      "loss: 0.222524  [  384/  569]\n",
      "loss: 0.215956  [  456/  569]\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.117289  [    0/  569]\n",
      "loss: 0.198254  [  128/  569]\n",
      "loss: 0.269496  [  256/  569]\n",
      "loss: 0.276184  [  384/  569]\n",
      "loss: 0.185526  [  456/  569]\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.139381  [    0/  569]\n",
      "loss: 0.239413  [  128/  569]\n",
      "loss: 0.449159  [  256/  569]\n",
      "loss: 0.193890  [  384/  569]\n",
      "loss: 0.156914  [  456/  569]\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.135879  [    0/  569]\n",
      "loss: 0.253253  [  128/  569]\n",
      "loss: 0.265123  [  256/  569]\n",
      "loss: 0.220121  [  384/  569]\n",
      "loss: 0.176319  [  456/  569]\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.225931  [    0/  569]\n",
      "loss: 0.202002  [  128/  569]\n",
      "loss: 0.156641  [  256/  569]\n",
      "loss: 0.287697  [  384/  569]\n",
      "loss: 0.193629  [  456/  569]\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.161080  [    0/  569]\n",
      "loss: 0.197319  [  128/  569]\n",
      "loss: 0.228717  [  256/  569]\n",
      "loss: 0.286357  [  384/  569]\n",
      "loss: 0.267694  [  456/  569]\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.197903  [    0/  569]\n",
      "loss: 0.218164  [  128/  569]\n",
      "loss: 0.280673  [  256/  569]\n",
      "loss: 0.300172  [  384/  569]\n",
      "loss: 0.182704  [  456/  569]\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.176269  [    0/  569]\n",
      "loss: 0.215767  [  128/  569]\n",
      "loss: 0.215488  [  256/  569]\n",
      "loss: 0.258608  [  384/  569]\n",
      "loss: 0.320933  [  456/  569]\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.303617  [    0/  569]\n",
      "loss: 0.276239  [  128/  569]\n",
      "loss: 0.217416  [  256/  569]\n",
      "loss: 0.197204  [  384/  569]\n",
      "loss: 0.204362  [  456/  569]\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.213932  [    0/  569]\n",
      "loss: 0.425323  [  128/  569]\n",
      "loss: 0.149865  [  256/  569]\n",
      "loss: 0.236647  [  384/  569]\n",
      "loss: 0.206360  [  456/  569]\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.151753  [    0/  569]\n",
      "loss: 0.187561  [  128/  569]\n",
      "loss: 0.226671  [  256/  569]\n",
      "loss: 0.327728  [  384/  569]\n",
      "loss: 0.132912  [  456/  569]\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.206216  [    0/  569]\n",
      "loss: 0.193636  [  128/  569]\n",
      "loss: 0.269602  [  256/  569]\n",
      "loss: 0.227922  [  384/  569]\n",
      "loss: 0.188010  [  456/  569]\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.295292  [    0/  569]\n",
      "loss: 0.167494  [  128/  569]\n",
      "loss: 0.133487  [  256/  569]\n",
      "loss: 0.228739  [  384/  569]\n",
      "loss: 0.259432  [  456/  569]\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.157784  [    0/  569]\n",
      "loss: 0.319316  [  128/  569]\n",
      "loss: 0.313111  [  256/  569]\n",
      "loss: 0.259985  [  384/  569]\n",
      "loss: 0.241770  [  456/  569]\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.199503  [    0/  569]\n",
      "loss: 0.204305  [  128/  569]\n",
      "loss: 0.240167  [  256/  569]\n",
      "loss: 0.221696  [  384/  569]\n",
      "loss: 0.438886  [  456/  569]\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.215467  [    0/  569]\n",
      "loss: 0.172845  [  128/  569]\n",
      "loss: 0.178891  [  256/  569]\n",
      "loss: 0.290765  [  384/  569]\n",
      "loss: 0.073859  [  456/  569]\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.165617  [    0/  569]\n",
      "loss: 0.126264  [  128/  569]\n",
      "loss: 0.270967  [  256/  569]\n",
      "loss: 0.222088  [  384/  569]\n",
      "loss: 0.300937  [  456/  569]\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.289149  [    0/  569]\n",
      "loss: 0.258479  [  128/  569]\n",
      "loss: 0.125935  [  256/  569]\n",
      "loss: 0.194214  [  384/  569]\n",
      "loss: 0.281343  [  456/  569]\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.201579  [    0/  569]\n",
      "loss: 0.203672  [  128/  569]\n",
      "loss: 0.326730  [  256/  569]\n",
      "loss: 0.189442  [  384/  569]\n",
      "loss: 0.134724  [  456/  569]\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.163001  [    0/  569]\n",
      "loss: 0.209796  [  128/  569]\n",
      "loss: 0.211013  [  256/  569]\n",
      "loss: 0.252122  [  384/  569]\n",
      "loss: 0.195844  [  456/  569]\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.198448  [    0/  569]\n",
      "loss: 0.206352  [  128/  569]\n",
      "loss: 0.211058  [  256/  569]\n",
      "loss: 0.316123  [  384/  569]\n",
      "loss: 0.300693  [  456/  569]\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.235097  [    0/  569]\n",
      "loss: 0.202321  [  128/  569]\n",
      "loss: 0.100126  [  256/  569]\n",
      "loss: 0.223863  [  384/  569]\n",
      "loss: 0.335100  [  456/  569]\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.279271  [    0/  569]\n",
      "loss: 0.191829  [  128/  569]\n",
      "loss: 0.244605  [  256/  569]\n",
      "loss: 0.225627  [  384/  569]\n",
      "loss: 0.242014  [  456/  569]\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.203074  [    0/  569]\n",
      "loss: 0.193288  [  128/  569]\n",
      "loss: 0.281211  [  256/  569]\n",
      "loss: 0.485984  [  384/  569]\n",
      "loss: 0.212355  [  456/  569]\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.229388  [    0/  569]\n",
      "loss: 0.194013  [  128/  569]\n",
      "loss: 0.296550  [  256/  569]\n",
      "loss: 0.119037  [  384/  569]\n",
      "loss: 0.151113  [  456/  569]\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.266826  [    0/  569]\n",
      "loss: 0.256625  [  128/  569]\n",
      "loss: 0.272713  [  256/  569]\n",
      "loss: 0.151884  [  384/  569]\n",
      "loss: 0.266939  [  456/  569]\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.247181  [    0/  569]\n",
      "loss: 0.431146  [  128/  569]\n",
      "loss: 0.219924  [  256/  569]\n",
      "loss: 0.188517  [  384/  569]\n",
      "loss: 0.190961  [  456/  569]\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.228146  [    0/  569]\n",
      "loss: 0.306744  [  128/  569]\n",
      "loss: 0.403349  [  256/  569]\n",
      "loss: 0.330452  [  384/  569]\n",
      "loss: 0.081563  [  456/  569]\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.397042  [    0/  569]\n",
      "loss: 0.221315  [  128/  569]\n",
      "loss: 0.202041  [  256/  569]\n",
      "loss: 0.135455  [  384/  569]\n",
      "loss: 0.168662  [  456/  569]\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.231374  [    0/  569]\n",
      "loss: 0.244839  [  128/  569]\n",
      "loss: 0.069114  [  256/  569]\n",
      "loss: 0.214393  [  384/  569]\n",
      "loss: 0.308133  [  456/  569]\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.286578  [    0/  569]\n",
      "loss: 0.217996  [  128/  569]\n",
      "loss: 0.092555  [  256/  569]\n",
      "loss: 0.282971  [  384/  569]\n",
      "loss: 0.272582  [  456/  569]\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.184189  [    0/  569]\n",
      "loss: 0.389803  [  128/  569]\n",
      "loss: 0.207396  [  256/  569]\n",
      "loss: 0.122868  [  384/  569]\n",
      "loss: 0.145239  [  456/  569]\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.123455  [    0/  569]\n",
      "loss: 0.264446  [  128/  569]\n",
      "loss: 0.295239  [  256/  569]\n",
      "loss: 0.306556  [  384/  569]\n",
      "loss: 0.238261  [  456/  569]\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.241401  [    0/  569]\n",
      "loss: 0.279672  [  128/  569]\n",
      "loss: 0.468256  [  256/  569]\n",
      "loss: 0.125082  [  384/  569]\n",
      "loss: 0.071810  [  456/  569]\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.253069  [    0/  569]\n",
      "loss: 0.252665  [  128/  569]\n",
      "loss: 0.177688  [  256/  569]\n",
      "loss: 0.232034  [  384/  569]\n",
      "loss: 0.167792  [  456/  569]\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.234656  [    0/  569]\n",
      "loss: 0.214729  [  128/  569]\n",
      "loss: 0.223325  [  256/  569]\n",
      "loss: 0.299641  [  384/  569]\n",
      "loss: 0.205491  [  456/  569]\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.145214  [    0/  569]\n",
      "loss: 0.307748  [  128/  569]\n",
      "loss: 0.154607  [  256/  569]\n",
      "loss: 0.137175  [  384/  569]\n",
      "loss: 0.171639  [  456/  569]\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.175984  [    0/  569]\n",
      "loss: 0.155276  [  128/  569]\n",
      "loss: 0.153653  [  256/  569]\n",
      "loss: 0.294582  [  384/  569]\n",
      "loss: 0.280572  [  456/  569]\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.245151  [    0/  569]\n",
      "loss: 0.261169  [  128/  569]\n",
      "loss: 0.225049  [  256/  569]\n",
      "loss: 0.174549  [  384/  569]\n",
      "loss: 0.253626  [  456/  569]\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.153404  [    0/  569]\n",
      "loss: 0.211017  [  128/  569]\n",
      "loss: 0.181139  [  256/  569]\n",
      "loss: 0.198002  [  384/  569]\n",
      "loss: 0.246984  [  456/  569]\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.260575  [    0/  569]\n",
      "loss: 0.153454  [  128/  569]\n",
      "loss: 0.150375  [  256/  569]\n",
      "loss: 0.208176  [  384/  569]\n",
      "loss: 0.241792  [  456/  569]\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.267152  [    0/  569]\n",
      "loss: 0.124800  [  128/  569]\n",
      "loss: 0.246442  [  256/  569]\n",
      "loss: 0.145346  [  384/  569]\n",
      "loss: 0.207724  [  456/  569]\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.288529  [    0/  569]\n",
      "loss: 0.133800  [  128/  569]\n",
      "loss: 0.217842  [  256/  569]\n",
      "loss: 0.126498  [  384/  569]\n",
      "loss: 0.284843  [  456/  569]\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.188191  [    0/  569]\n",
      "loss: 0.356039  [  128/  569]\n",
      "loss: 0.170427  [  256/  569]\n",
      "loss: 0.160109  [  384/  569]\n",
      "loss: 0.216696  [  456/  569]\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.232980  [    0/  569]\n",
      "loss: 0.257264  [  128/  569]\n",
      "loss: 0.128788  [  256/  569]\n",
      "loss: 0.296620  [  384/  569]\n",
      "loss: 0.265871  [  456/  569]\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.203418  [    0/  569]\n",
      "loss: 0.251458  [  128/  569]\n",
      "loss: 0.265768  [  256/  569]\n",
      "loss: 0.263081  [  384/  569]\n",
      "loss: 0.112622  [  456/  569]\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.255795  [    0/  569]\n",
      "loss: 0.218332  [  128/  569]\n",
      "loss: 0.214991  [  256/  569]\n",
      "loss: 0.237273  [  384/  569]\n",
      "loss: 0.193932  [  456/  569]\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.170885  [    0/  569]\n",
      "loss: 0.293989  [  128/  569]\n",
      "loss: 0.174764  [  256/  569]\n",
      "loss: 0.176102  [  384/  569]\n",
      "loss: 0.115663  [  456/  569]\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.204808  [    0/  569]\n",
      "loss: 0.177147  [  128/  569]\n",
      "loss: 0.196122  [  256/  569]\n",
      "loss: 0.211078  [  384/  569]\n",
      "loss: 0.255749  [  456/  569]\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.367441  [    0/  569]\n",
      "loss: 0.143565  [  128/  569]\n",
      "loss: 0.260039  [  256/  569]\n",
      "loss: 0.205503  [  384/  569]\n",
      "loss: 0.165474  [  456/  569]\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.214575  [    0/  569]\n",
      "loss: 0.245740  [  128/  569]\n",
      "loss: 0.178004  [  256/  569]\n",
      "loss: 0.304954  [  384/  569]\n",
      "loss: 0.236139  [  456/  569]\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.188745  [    0/  569]\n",
      "loss: 0.424270  [  128/  569]\n",
      "loss: 0.150023  [  256/  569]\n",
      "loss: 0.141240  [  384/  569]\n",
      "loss: 0.118170  [  456/  569]\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.176410  [    0/  569]\n",
      "loss: 0.205909  [  128/  569]\n",
      "loss: 0.199449  [  256/  569]\n",
      "loss: 0.238998  [  384/  569]\n",
      "loss: 0.184741  [  456/  569]\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.165622  [    0/  569]\n",
      "loss: 0.235228  [  128/  569]\n",
      "loss: 0.173608  [  256/  569]\n",
      "loss: 0.253166  [  384/  569]\n",
      "loss: 0.212268  [  456/  569]\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.279696  [    0/  569]\n",
      "loss: 0.179676  [  128/  569]\n",
      "loss: 0.259491  [  256/  569]\n",
      "loss: 0.156701  [  384/  569]\n",
      "loss: 0.152819  [  456/  569]\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.339537  [    0/  569]\n",
      "loss: 0.215708  [  128/  569]\n",
      "loss: 0.152574  [  256/  569]\n",
      "loss: 0.258558  [  384/  569]\n",
      "loss: 0.215062  [  456/  569]\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.126865  [    0/  569]\n",
      "loss: 0.305361  [  128/  569]\n",
      "loss: 0.308627  [  256/  569]\n",
      "loss: 0.298655  [  384/  569]\n",
      "loss: 0.339993  [  456/  569]\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.222482  [    0/  569]\n",
      "loss: 0.165199  [  128/  569]\n",
      "loss: 0.159885  [  256/  569]\n",
      "loss: 0.208570  [  384/  569]\n",
      "loss: 0.341712  [  456/  569]\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.419666  [    0/  569]\n",
      "loss: 0.186268  [  128/  569]\n",
      "loss: 0.207620  [  256/  569]\n",
      "loss: 0.149358  [  384/  569]\n",
      "loss: 0.222844  [  456/  569]\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.211680  [    0/  569]\n",
      "loss: 0.224218  [  128/  569]\n",
      "loss: 0.221190  [  256/  569]\n",
      "loss: 0.249650  [  384/  569]\n",
      "loss: 0.238623  [  456/  569]\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.230573  [    0/  569]\n",
      "loss: 0.269288  [  128/  569]\n",
      "loss: 0.139843  [  256/  569]\n",
      "loss: 0.257859  [  384/  569]\n",
      "loss: 0.197342  [  456/  569]\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.167822  [    0/  569]\n",
      "loss: 0.302427  [  128/  569]\n",
      "loss: 0.186183  [  256/  569]\n",
      "loss: 0.192641  [  384/  569]\n",
      "loss: 0.224871  [  456/  569]\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.256825  [    0/  569]\n",
      "loss: 0.235442  [  128/  569]\n",
      "loss: 0.239328  [  256/  569]\n",
      "loss: 0.260553  [  384/  569]\n",
      "loss: 0.159125  [  456/  569]\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.125300  [    0/  569]\n",
      "loss: 0.266778  [  128/  569]\n",
      "loss: 0.198568  [  256/  569]\n",
      "loss: 0.363274  [  384/  569]\n",
      "loss: 0.312221  [  456/  569]\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.201032  [    0/  569]\n",
      "loss: 0.163623  [  128/  569]\n",
      "loss: 0.221251  [  256/  569]\n",
      "loss: 0.246251  [  384/  569]\n",
      "loss: 0.161791  [  456/  569]\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.247407  [    0/  569]\n",
      "loss: 0.186631  [  128/  569]\n",
      "loss: 0.205023  [  256/  569]\n",
      "loss: 0.230518  [  384/  569]\n",
      "loss: 0.196110  [  456/  569]\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.155635  [    0/  569]\n",
      "loss: 0.186795  [  128/  569]\n",
      "loss: 0.182854  [  256/  569]\n",
      "loss: 0.185794  [  384/  569]\n",
      "loss: 0.241433  [  456/  569]\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.110201  [    0/  569]\n",
      "loss: 0.170070  [  128/  569]\n",
      "loss: 0.110666  [  256/  569]\n",
      "loss: 0.228498  [  384/  569]\n",
      "loss: 0.293028  [  456/  569]\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.188668  [    0/  569]\n",
      "loss: 0.219855  [  128/  569]\n",
      "loss: 0.235235  [  256/  569]\n",
      "loss: 0.161512  [  384/  569]\n",
      "loss: 0.289120  [  456/  569]\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.239206  [    0/  569]\n",
      "loss: 0.183189  [  128/  569]\n",
      "loss: 0.187693  [  256/  569]\n",
      "loss: 0.196006  [  384/  569]\n",
      "loss: 0.302047  [  456/  569]\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.059359  [    0/  569]\n",
      "loss: 0.236061  [  128/  569]\n",
      "loss: 0.142479  [  256/  569]\n",
      "loss: 0.408254  [  384/  569]\n",
      "loss: 0.229639  [  456/  569]\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.222426  [    0/  569]\n",
      "loss: 0.135598  [  128/  569]\n",
      "loss: 0.195289  [  256/  569]\n",
      "loss: 0.197624  [  384/  569]\n",
      "loss: 0.245521  [  456/  569]\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.219950  [    0/  569]\n",
      "loss: 0.262810  [  128/  569]\n",
      "loss: 0.169558  [  256/  569]\n",
      "loss: 0.159641  [  384/  569]\n",
      "loss: 0.146685  [  456/  569]\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.198275  [    0/  569]\n",
      "loss: 0.109773  [  128/  569]\n",
      "loss: 0.138528  [  256/  569]\n",
      "loss: 0.256374  [  384/  569]\n",
      "loss: 0.300857  [  456/  569]\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.224945  [    0/  569]\n",
      "loss: 0.208286  [  128/  569]\n",
      "loss: 0.108313  [  256/  569]\n",
      "loss: 0.224626  [  384/  569]\n",
      "loss: 0.195566  [  456/  569]\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.142640  [    0/  569]\n",
      "loss: 0.185365  [  128/  569]\n",
      "loss: 0.238486  [  256/  569]\n",
      "loss: 0.191138  [  384/  569]\n",
      "loss: 0.112044  [  456/  569]\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.266390  [    0/  569]\n",
      "loss: 0.198688  [  128/  569]\n",
      "loss: 0.119928  [  256/  569]\n",
      "loss: 0.211942  [  384/  569]\n",
      "loss: 0.146902  [  456/  569]\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.208966  [    0/  569]\n",
      "loss: 0.240659  [  128/  569]\n",
      "loss: 0.221607  [  256/  569]\n",
      "loss: 0.231214  [  384/  569]\n",
      "loss: 0.194993  [  456/  569]\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.213802  [    0/  569]\n",
      "loss: 0.214051  [  128/  569]\n",
      "loss: 0.176194  [  256/  569]\n",
      "loss: 0.195864  [  384/  569]\n",
      "loss: 0.226563  [  456/  569]\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.162209  [    0/  569]\n",
      "loss: 0.159069  [  128/  569]\n",
      "loss: 0.154765  [  256/  569]\n",
      "loss: 0.269279  [  384/  569]\n",
      "loss: 0.129889  [  456/  569]\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.134106  [    0/  569]\n",
      "loss: 0.208894  [  128/  569]\n",
      "loss: 0.199615  [  256/  569]\n",
      "loss: 0.326421  [  384/  569]\n",
      "loss: 0.213338  [  456/  569]\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.205556  [    0/  569]\n",
      "loss: 0.220107  [  128/  569]\n",
      "loss: 0.135337  [  256/  569]\n",
      "loss: 0.182783  [  384/  569]\n",
      "loss: 0.197264  [  456/  569]\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.238967  [    0/  569]\n",
      "loss: 0.145626  [  128/  569]\n",
      "loss: 0.138864  [  256/  569]\n",
      "loss: 0.576291  [  384/  569]\n",
      "loss: 0.345504  [  456/  569]\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.178918  [    0/  569]\n",
      "loss: 0.173523  [  128/  569]\n",
      "loss: 0.208980  [  256/  569]\n",
      "loss: 0.239643  [  384/  569]\n",
      "loss: 0.237410  [  456/  569]\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.313871  [    0/  569]\n",
      "loss: 0.225640  [  128/  569]\n",
      "loss: 0.103193  [  256/  569]\n",
      "loss: 0.159825  [  384/  569]\n",
      "loss: 0.133979  [  456/  569]\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.389811  [    0/  569]\n",
      "loss: 0.161422  [  128/  569]\n",
      "loss: 0.185002  [  256/  569]\n",
      "loss: 0.279100  [  384/  569]\n",
      "loss: 0.160172  [  456/  569]\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.246319  [    0/  569]\n",
      "loss: 0.298016  [  128/  569]\n",
      "loss: 0.233757  [  256/  569]\n",
      "loss: 0.090104  [  384/  569]\n",
      "loss: 0.165347  [  456/  569]\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.227819  [    0/  569]\n",
      "loss: 0.164056  [  128/  569]\n",
      "loss: 0.224409  [  256/  569]\n",
      "loss: 0.246751  [  384/  569]\n",
      "loss: 0.322111  [  456/  569]\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.159714  [    0/  569]\n",
      "loss: 0.267802  [  128/  569]\n",
      "loss: 0.155535  [  256/  569]\n",
      "loss: 0.181821  [  384/  569]\n",
      "loss: 0.274714  [  456/  569]\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.213495  [    0/  569]\n",
      "loss: 0.336757  [  128/  569]\n",
      "loss: 0.184284  [  256/  569]\n",
      "loss: 0.205772  [  384/  569]\n",
      "loss: 0.280236  [  456/  569]\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.161795  [    0/  569]\n",
      "loss: 0.254751  [  128/  569]\n",
      "loss: 0.281064  [  256/  569]\n",
      "loss: 0.278685  [  384/  569]\n",
      "loss: 0.170435  [  456/  569]\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.252587  [    0/  569]\n",
      "loss: 0.100255  [  128/  569]\n",
      "loss: 0.140670  [  256/  569]\n",
      "loss: 0.278658  [  384/  569]\n",
      "loss: 0.151238  [  456/  569]\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.239846  [    0/  569]\n",
      "loss: 0.162755  [  128/  569]\n",
      "loss: 0.155775  [  256/  569]\n",
      "loss: 0.260269  [  384/  569]\n",
      "loss: 0.213469  [  456/  569]\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.188325  [    0/  569]\n",
      "loss: 0.284271  [  128/  569]\n",
      "loss: 0.234741  [  256/  569]\n",
      "loss: 0.232149  [  384/  569]\n",
      "loss: 0.179724  [  456/  569]\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.167533  [    0/  569]\n",
      "loss: 0.185098  [  128/  569]\n",
      "loss: 0.328667  [  256/  569]\n",
      "loss: 0.466021  [  384/  569]\n",
      "loss: 0.232919  [  456/  569]\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.268801  [    0/  569]\n",
      "loss: 0.118324  [  128/  569]\n",
      "loss: 0.235645  [  256/  569]\n",
      "loss: 0.299198  [  384/  569]\n",
      "loss: 0.243852  [  456/  569]\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.193793  [    0/  569]\n",
      "loss: 0.221525  [  128/  569]\n",
      "loss: 0.219206  [  256/  569]\n",
      "loss: 0.205535  [  384/  569]\n",
      "loss: 0.218927  [  456/  569]\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.212754  [    0/  569]\n",
      "loss: 0.225091  [  128/  569]\n",
      "loss: 0.152516  [  256/  569]\n",
      "loss: 0.165032  [  384/  569]\n",
      "loss: 0.264628  [  456/  569]\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.188009  [    0/  569]\n",
      "loss: 0.243776  [  128/  569]\n",
      "loss: 0.236516  [  256/  569]\n",
      "loss: 0.161008  [  384/  569]\n",
      "loss: 0.276247  [  456/  569]\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.121697  [    0/  569]\n",
      "loss: 0.175371  [  128/  569]\n",
      "loss: 0.186783  [  256/  569]\n",
      "loss: 0.268438  [  384/  569]\n",
      "loss: 0.234796  [  456/  569]\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.177434  [    0/  569]\n",
      "loss: 0.246265  [  128/  569]\n",
      "loss: 0.330755  [  256/  569]\n",
      "loss: 0.276708  [  384/  569]\n",
      "loss: 0.227842  [  456/  569]\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.290370  [    0/  569]\n",
      "loss: 0.175387  [  128/  569]\n",
      "loss: 0.159974  [  256/  569]\n",
      "loss: 0.209532  [  384/  569]\n",
      "loss: 0.190542  [  456/  569]\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.165413  [    0/  569]\n",
      "loss: 0.239799  [  128/  569]\n",
      "loss: 0.143269  [  256/  569]\n",
      "loss: 0.164067  [  384/  569]\n",
      "loss: 0.319618  [  456/  569]\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.228545  [    0/  569]\n",
      "loss: 0.256561  [  128/  569]\n",
      "loss: 0.163549  [  256/  569]\n",
      "loss: 0.193691  [  384/  569]\n",
      "loss: 0.381860  [  456/  569]\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.420026  [    0/  569]\n",
      "loss: 0.233255  [  128/  569]\n",
      "loss: 0.183043  [  256/  569]\n",
      "loss: 0.247358  [  384/  569]\n",
      "loss: 0.306907  [  456/  569]\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.189589  [    0/  569]\n",
      "loss: 0.095225  [  128/  569]\n",
      "loss: 0.404555  [  256/  569]\n",
      "loss: 0.160155  [  384/  569]\n",
      "loss: 0.213430  [  456/  569]\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.045787  [    0/  569]\n",
      "loss: 0.211400  [  128/  569]\n",
      "loss: 0.314162  [  256/  569]\n",
      "loss: 0.310805  [  384/  569]\n",
      "loss: 0.220750  [  456/  569]\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.195340  [    0/  569]\n",
      "loss: 0.217647  [  128/  569]\n",
      "loss: 0.125880  [  256/  569]\n",
      "loss: 0.131822  [  384/  569]\n",
      "loss: 0.269293  [  456/  569]\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.140116  [    0/  569]\n",
      "loss: 0.138249  [  128/  569]\n",
      "loss: 0.210453  [  256/  569]\n",
      "loss: 0.330487  [  384/  569]\n",
      "loss: 0.229096  [  456/  569]\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.158530  [    0/  569]\n",
      "loss: 0.260801  [  128/  569]\n",
      "loss: 0.131780  [  256/  569]\n",
      "loss: 0.264455  [  384/  569]\n",
      "loss: 0.089327  [  456/  569]\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.177551  [    0/  569]\n",
      "loss: 0.224466  [  128/  569]\n",
      "loss: 0.216702  [  256/  569]\n",
      "loss: 0.231115  [  384/  569]\n",
      "loss: 0.172955  [  456/  569]\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.149372  [    0/  569]\n",
      "loss: 0.258411  [  128/  569]\n",
      "loss: 0.409384  [  256/  569]\n",
      "loss: 0.291237  [  384/  569]\n",
      "loss: 0.098906  [  456/  569]\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.255235  [    0/  569]\n",
      "loss: 0.161715  [  128/  569]\n",
      "loss: 0.274830  [  256/  569]\n",
      "loss: 0.386806  [  384/  569]\n",
      "loss: 0.316140  [  456/  569]\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.281953  [    0/  569]\n",
      "loss: 0.186033  [  128/  569]\n",
      "loss: 0.194245  [  256/  569]\n",
      "loss: 0.353783  [  384/  569]\n",
      "loss: 0.117340  [  456/  569]\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.235512  [    0/  569]\n",
      "loss: 0.245392  [  128/  569]\n",
      "loss: 0.249963  [  256/  569]\n",
      "loss: 0.119981  [  384/  569]\n",
      "loss: 0.189901  [  456/  569]\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.144325  [    0/  569]\n",
      "loss: 0.172786  [  128/  569]\n",
      "loss: 0.234796  [  256/  569]\n",
      "loss: 0.229695  [  384/  569]\n",
      "loss: 0.293305  [  456/  569]\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.157705  [    0/  569]\n",
      "loss: 0.143929  [  128/  569]\n",
      "loss: 0.258793  [  256/  569]\n",
      "loss: 0.186103  [  384/  569]\n",
      "loss: 0.140125  [  456/  569]\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.182125  [    0/  569]\n",
      "loss: 0.227376  [  128/  569]\n",
      "loss: 0.211048  [  256/  569]\n",
      "loss: 0.182631  [  384/  569]\n",
      "loss: 0.133116  [  456/  569]\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.144611  [    0/  569]\n",
      "loss: 0.357141  [  128/  569]\n",
      "loss: 0.294685  [  256/  569]\n",
      "loss: 0.180899  [  384/  569]\n",
      "loss: 0.146338  [  456/  569]\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.158648  [    0/  569]\n",
      "loss: 0.317564  [  128/  569]\n",
      "loss: 0.294955  [  256/  569]\n",
      "loss: 0.206028  [  384/  569]\n",
      "loss: 0.214423  [  456/  569]\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.261505  [    0/  569]\n",
      "loss: 0.185107  [  128/  569]\n",
      "loss: 0.207764  [  256/  569]\n",
      "loss: 0.251532  [  384/  569]\n",
      "loss: 0.258278  [  456/  569]\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.231648  [    0/  569]\n",
      "loss: 0.261097  [  128/  569]\n",
      "loss: 0.225221  [  256/  569]\n",
      "loss: 0.117083  [  384/  569]\n",
      "loss: 0.193876  [  456/  569]\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.241483  [    0/  569]\n",
      "loss: 0.121469  [  128/  569]\n",
      "loss: 0.169370  [  256/  569]\n",
      "loss: 0.176705  [  384/  569]\n",
      "loss: 0.248369  [  456/  569]\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.208911  [    0/  569]\n",
      "loss: 0.239390  [  128/  569]\n",
      "loss: 0.257455  [  256/  569]\n",
      "loss: 0.215613  [  384/  569]\n",
      "loss: 0.218727  [  456/  569]\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.234923  [    0/  569]\n",
      "loss: 0.192752  [  128/  569]\n",
      "loss: 0.152681  [  256/  569]\n",
      "loss: 0.253546  [  384/  569]\n",
      "loss: 0.230201  [  456/  569]\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.142721  [    0/  569]\n",
      "loss: 0.223667  [  128/  569]\n",
      "loss: 0.214033  [  256/  569]\n",
      "loss: 0.290432  [  384/  569]\n",
      "loss: 0.325042  [  456/  569]\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.171700  [    0/  569]\n",
      "loss: 0.214604  [  128/  569]\n",
      "loss: 0.230236  [  256/  569]\n",
      "loss: 0.194566  [  384/  569]\n",
      "loss: 0.170086  [  456/  569]\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.199383  [    0/  569]\n",
      "loss: 0.212439  [  128/  569]\n",
      "loss: 0.383121  [  256/  569]\n",
      "loss: 0.239758  [  384/  569]\n",
      "loss: 0.131200  [  456/  569]\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.153863  [    0/  569]\n",
      "loss: 0.105947  [  128/  569]\n",
      "loss: 0.121235  [  256/  569]\n",
      "loss: 0.199710  [  384/  569]\n",
      "loss: 0.235441  [  456/  569]\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.224361  [    0/  569]\n",
      "loss: 0.257896  [  128/  569]\n",
      "loss: 0.239617  [  256/  569]\n",
      "loss: 0.146382  [  384/  569]\n",
      "loss: 0.192616  [  456/  569]\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.107340  [    0/  569]\n",
      "loss: 0.322450  [  128/  569]\n",
      "loss: 0.208651  [  256/  569]\n",
      "loss: 0.076947  [  384/  569]\n",
      "loss: 0.148305  [  456/  569]\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.095007  [    0/  569]\n",
      "loss: 0.156100  [  128/  569]\n",
      "loss: 0.216065  [  256/  569]\n",
      "loss: 0.188133  [  384/  569]\n",
      "loss: 0.247848  [  456/  569]\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.193440  [    0/  569]\n",
      "loss: 0.317746  [  128/  569]\n",
      "loss: 0.146435  [  256/  569]\n",
      "loss: 0.227694  [  384/  569]\n",
      "loss: 0.255163  [  456/  569]\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.203346  [    0/  569]\n",
      "loss: 0.204830  [  128/  569]\n",
      "loss: 0.269623  [  256/  569]\n",
      "loss: 0.140369  [  384/  569]\n",
      "loss: 0.133898  [  456/  569]\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.160915  [    0/  569]\n",
      "loss: 0.200267  [  128/  569]\n",
      "loss: 0.282522  [  256/  569]\n",
      "loss: 0.155063  [  384/  569]\n",
      "loss: 0.253034  [  456/  569]\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 0.236145  [    0/  569]\n",
      "loss: 0.376818  [  128/  569]\n",
      "loss: 0.356279  [  256/  569]\n",
      "loss: 0.188590  [  384/  569]\n",
      "loss: 0.229322  [  456/  569]\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.242828  [    0/  569]\n",
      "loss: 0.168264  [  128/  569]\n",
      "loss: 0.225560  [  256/  569]\n",
      "loss: 0.236084  [  384/  569]\n",
      "loss: 0.170586  [  456/  569]\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.224515  [    0/  569]\n",
      "loss: 0.224808  [  128/  569]\n",
      "loss: 0.286422  [  256/  569]\n",
      "loss: 0.217158  [  384/  569]\n",
      "loss: 0.177756  [  456/  569]\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.256388  [    0/  569]\n",
      "loss: 0.161049  [  128/  569]\n",
      "loss: 0.213162  [  256/  569]\n",
      "loss: 0.378341  [  384/  569]\n",
      "loss: 0.110152  [  456/  569]\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.250950  [    0/  569]\n",
      "loss: 0.183373  [  128/  569]\n",
      "loss: 0.097188  [  256/  569]\n",
      "loss: 0.191242  [  384/  569]\n",
      "loss: 0.121260  [  456/  569]\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.236991  [    0/  569]\n",
      "loss: 0.246750  [  128/  569]\n",
      "loss: 0.136640  [  256/  569]\n",
      "loss: 0.190962  [  384/  569]\n",
      "loss: 0.222068  [  456/  569]\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.213201  [    0/  569]\n",
      "loss: 0.116210  [  128/  569]\n",
      "loss: 0.330701  [  256/  569]\n",
      "loss: 0.279271  [  384/  569]\n",
      "loss: 0.201048  [  456/  569]\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.293160  [    0/  569]\n",
      "loss: 0.178336  [  128/  569]\n",
      "loss: 0.164611  [  256/  569]\n",
      "loss: 0.216099  [  384/  569]\n",
      "loss: 0.164085  [  456/  569]\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.234749  [    0/  569]\n",
      "loss: 0.431191  [  128/  569]\n",
      "loss: 0.195671  [  256/  569]\n",
      "loss: 0.235157  [  384/  569]\n",
      "loss: 0.140484  [  456/  569]\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.226549  [    0/  569]\n",
      "loss: 0.140953  [  128/  569]\n",
      "loss: 0.298785  [  256/  569]\n",
      "loss: 0.160298  [  384/  569]\n",
      "loss: 0.268545  [  456/  569]\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.235922  [    0/  569]\n",
      "loss: 0.165331  [  128/  569]\n",
      "loss: 0.297297  [  256/  569]\n",
      "loss: 0.085368  [  384/  569]\n",
      "loss: 0.272448  [  456/  569]\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.307133  [    0/  569]\n",
      "loss: 0.107209  [  128/  569]\n",
      "loss: 0.196183  [  256/  569]\n",
      "loss: 0.117659  [  384/  569]\n",
      "loss: 0.201085  [  456/  569]\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.205471  [    0/  569]\n",
      "loss: 0.195911  [  128/  569]\n",
      "loss: 0.174230  [  256/  569]\n",
      "loss: 0.276912  [  384/  569]\n",
      "loss: 0.226056  [  456/  569]\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.248657  [    0/  569]\n",
      "loss: 0.150514  [  128/  569]\n",
      "loss: 0.201119  [  256/  569]\n",
      "loss: 0.128413  [  384/  569]\n",
      "loss: 0.225721  [  456/  569]\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.073360  [    0/  569]\n",
      "loss: 0.337484  [  128/  569]\n",
      "loss: 0.319280  [  256/  569]\n",
      "loss: 0.210794  [  384/  569]\n",
      "loss: 0.198232  [  456/  569]\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.295098  [    0/  569]\n",
      "loss: 0.193163  [  128/  569]\n",
      "loss: 0.224790  [  256/  569]\n",
      "loss: 0.186105  [  384/  569]\n",
      "loss: 0.151430  [  456/  569]\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.262166  [    0/  569]\n",
      "loss: 0.174378  [  128/  569]\n",
      "loss: 0.142733  [  256/  569]\n",
      "loss: 0.148602  [  384/  569]\n",
      "loss: 0.199767  [  456/  569]\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.171378  [    0/  569]\n",
      "loss: 0.130396  [  128/  569]\n",
      "loss: 0.179092  [  256/  569]\n",
      "loss: 0.265516  [  384/  569]\n",
      "loss: 0.229850  [  456/  569]\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.354581  [    0/  569]\n",
      "loss: 0.169273  [  128/  569]\n",
      "loss: 0.257253  [  256/  569]\n",
      "loss: 0.199298  [  384/  569]\n",
      "loss: 0.139988  [  456/  569]\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.236776  [    0/  569]\n",
      "loss: 0.082219  [  128/  569]\n",
      "loss: 0.169910  [  256/  569]\n",
      "loss: 0.163522  [  384/  569]\n",
      "loss: 0.274254  [  456/  569]\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.171910  [    0/  569]\n",
      "loss: 0.153597  [  128/  569]\n",
      "loss: 0.231976  [  256/  569]\n",
      "loss: 0.274218  [  384/  569]\n",
      "loss: 0.208896  [  456/  569]\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.229202  [    0/  569]\n",
      "loss: 0.113994  [  128/  569]\n",
      "loss: 0.238172  [  256/  569]\n",
      "loss: 0.212397  [  384/  569]\n",
      "loss: 0.252695  [  456/  569]\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.215890  [    0/  569]\n",
      "loss: 0.169239  [  128/  569]\n",
      "loss: 0.181251  [  256/  569]\n",
      "loss: 0.162670  [  384/  569]\n",
      "loss: 0.116244  [  456/  569]\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.434778  [    0/  569]\n",
      "loss: 0.198095  [  128/  569]\n",
      "loss: 0.152817  [  256/  569]\n",
      "loss: 0.216106  [  384/  569]\n",
      "loss: 0.099218  [  456/  569]\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.133894  [    0/  569]\n",
      "loss: 0.183095  [  128/  569]\n",
      "loss: 0.243028  [  256/  569]\n",
      "loss: 0.224696  [  384/  569]\n",
      "loss: 0.161214  [  456/  569]\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.301627  [    0/  569]\n",
      "loss: 0.127657  [  128/  569]\n",
      "loss: 0.118867  [  256/  569]\n",
      "loss: 0.242644  [  384/  569]\n",
      "loss: 0.192067  [  456/  569]\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.162913  [    0/  569]\n",
      "loss: 0.118868  [  128/  569]\n",
      "loss: 0.333277  [  256/  569]\n",
      "loss: 0.068688  [  384/  569]\n",
      "loss: 0.280081  [  456/  569]\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.174038  [    0/  569]\n",
      "loss: 0.296144  [  128/  569]\n",
      "loss: 0.138528  [  256/  569]\n",
      "loss: 0.133261  [  384/  569]\n",
      "loss: 0.277725  [  456/  569]\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.195348  [    0/  569]\n",
      "loss: 0.386734  [  128/  569]\n",
      "loss: 0.144795  [  256/  569]\n",
      "loss: 0.245409  [  384/  569]\n",
      "loss: 0.195746  [  456/  569]\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.182380  [    0/  569]\n",
      "loss: 0.196205  [  128/  569]\n",
      "loss: 0.175210  [  256/  569]\n",
      "loss: 0.151708  [  384/  569]\n",
      "loss: 0.290515  [  456/  569]\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.239993  [    0/  569]\n",
      "loss: 0.241684  [  128/  569]\n",
      "loss: 0.172518  [  256/  569]\n",
      "loss: 0.194928  [  384/  569]\n",
      "loss: 0.298278  [  456/  569]\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.088491  [    0/  569]\n",
      "loss: 0.320752  [  128/  569]\n",
      "loss: 0.232099  [  256/  569]\n",
      "loss: 0.162864  [  384/  569]\n",
      "loss: 0.178731  [  456/  569]\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.143122  [    0/  569]\n",
      "loss: 0.171754  [  128/  569]\n",
      "loss: 0.095408  [  256/  569]\n",
      "loss: 0.309731  [  384/  569]\n",
      "loss: 0.167562  [  456/  569]\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.142480  [    0/  569]\n",
      "loss: 0.209002  [  128/  569]\n",
      "loss: 0.099193  [  256/  569]\n",
      "loss: 0.303570  [  384/  569]\n",
      "loss: 0.216831  [  456/  569]\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.310234  [    0/  569]\n",
      "loss: 0.309201  [  128/  569]\n",
      "loss: 0.266092  [  256/  569]\n",
      "loss: 0.097965  [  384/  569]\n",
      "loss: 0.243570  [  456/  569]\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.190382  [    0/  569]\n",
      "loss: 0.225084  [  128/  569]\n",
      "loss: 0.155322  [  256/  569]\n",
      "loss: 0.220967  [  384/  569]\n",
      "loss: 0.158260  [  456/  569]\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 0.173944  [    0/  569]\n",
      "loss: 0.169606  [  128/  569]\n",
      "loss: 0.225839  [  256/  569]\n",
      "loss: 0.105479  [  384/  569]\n",
      "loss: 0.193301  [  456/  569]\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.187750  [    0/  569]\n",
      "loss: 0.221457  [  128/  569]\n",
      "loss: 0.207566  [  256/  569]\n",
      "loss: 0.167823  [  384/  569]\n",
      "loss: 0.210975  [  456/  569]\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.261301  [    0/  569]\n",
      "loss: 0.154946  [  128/  569]\n",
      "loss: 0.315318  [  256/  569]\n",
      "loss: 0.193070  [  384/  569]\n",
      "loss: 0.254349  [  456/  569]\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.207993  [    0/  569]\n",
      "loss: 0.155046  [  128/  569]\n",
      "loss: 0.229655  [  256/  569]\n",
      "loss: 0.200088  [  384/  569]\n",
      "loss: 0.128367  [  456/  569]\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.249087  [    0/  569]\n",
      "loss: 0.222891  [  128/  569]\n",
      "loss: 0.240038  [  256/  569]\n",
      "loss: 0.249999  [  384/  569]\n",
      "loss: 0.270345  [  456/  569]\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.239082  [    0/  569]\n",
      "loss: 0.257581  [  128/  569]\n",
      "loss: 0.128732  [  256/  569]\n",
      "loss: 0.263676  [  384/  569]\n",
      "loss: 0.201734  [  456/  569]\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.252975  [    0/  569]\n",
      "loss: 0.410046  [  128/  569]\n",
      "loss: 0.175925  [  256/  569]\n",
      "loss: 0.264515  [  384/  569]\n",
      "loss: 0.215304  [  456/  569]\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.220553  [    0/  569]\n",
      "loss: 0.183112  [  128/  569]\n",
      "loss: 0.132519  [  256/  569]\n",
      "loss: 0.147690  [  384/  569]\n",
      "loss: 0.189845  [  456/  569]\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.177660  [    0/  569]\n",
      "loss: 0.217909  [  128/  569]\n",
      "loss: 0.259075  [  256/  569]\n",
      "loss: 0.143573  [  384/  569]\n",
      "loss: 0.158559  [  456/  569]\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.124466  [    0/  569]\n",
      "loss: 0.126951  [  128/  569]\n",
      "loss: 0.165203  [  256/  569]\n",
      "loss: 0.153046  [  384/  569]\n",
      "loss: 0.372914  [  456/  569]\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.272087  [    0/  569]\n",
      "loss: 0.162519  [  128/  569]\n",
      "loss: 0.160570  [  256/  569]\n",
      "loss: 0.179706  [  384/  569]\n",
      "loss: 0.132432  [  456/  569]\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.196208  [    0/  569]\n",
      "loss: 0.217648  [  128/  569]\n",
      "loss: 0.198709  [  256/  569]\n",
      "loss: 0.354384  [  384/  569]\n",
      "loss: 0.419452  [  456/  569]\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.432075  [    0/  569]\n",
      "loss: 0.235389  [  128/  569]\n",
      "loss: 0.207873  [  256/  569]\n",
      "loss: 0.146882  [  384/  569]\n",
      "loss: 0.180416  [  456/  569]\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "loss: 0.226399  [    0/  569]\n",
      "loss: 0.138521  [  128/  569]\n",
      "loss: 0.116914  [  256/  569]\n",
      "loss: 0.139620  [  384/  569]\n",
      "loss: 0.354264  [  456/  569]\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "loss: 0.145267  [    0/  569]\n",
      "loss: 0.164622  [  128/  569]\n",
      "loss: 0.321772  [  256/  569]\n",
      "loss: 0.195713  [  384/  569]\n",
      "loss: 0.140415  [  456/  569]\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "loss: 0.301993  [    0/  569]\n",
      "loss: 0.249172  [  128/  569]\n",
      "loss: 0.145456  [  256/  569]\n",
      "loss: 0.130275  [  384/  569]\n",
      "loss: 0.168833  [  456/  569]\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "loss: 0.178066  [    0/  569]\n",
      "loss: 0.157336  [  128/  569]\n",
      "loss: 0.168258  [  256/  569]\n",
      "loss: 0.238572  [  384/  569]\n",
      "loss: 0.219622  [  456/  569]\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "loss: 0.307285  [    0/  569]\n",
      "loss: 0.085879  [  128/  569]\n",
      "loss: 0.256962  [  256/  569]\n",
      "loss: 0.287252  [  384/  569]\n",
      "loss: 0.174075  [  456/  569]\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "loss: 0.171705  [    0/  569]\n",
      "loss: 0.255808  [  128/  569]\n",
      "loss: 0.181966  [  256/  569]\n",
      "loss: 0.193327  [  384/  569]\n",
      "loss: 0.140570  [  456/  569]\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "loss: 0.111162  [    0/  569]\n",
      "loss: 0.267839  [  128/  569]\n",
      "loss: 0.303234  [  256/  569]\n",
      "loss: 0.166145  [  384/  569]\n",
      "loss: 0.293740  [  456/  569]\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "loss: 0.164040  [    0/  569]\n",
      "loss: 0.289021  [  128/  569]\n",
      "loss: 0.266621  [  256/  569]\n",
      "loss: 0.200379  [  384/  569]\n",
      "loss: 0.166268  [  456/  569]\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "loss: 0.328871  [    0/  569]\n",
      "loss: 0.227475  [  128/  569]\n",
      "loss: 0.121724  [  256/  569]\n",
      "loss: 0.209286  [  384/  569]\n",
      "loss: 0.244438  [  456/  569]\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "loss: 0.221149  [    0/  569]\n",
      "loss: 0.229608  [  128/  569]\n",
      "loss: 0.253280  [  256/  569]\n",
      "loss: 0.238153  [  384/  569]\n",
      "loss: 0.072369  [  456/  569]\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "loss: 0.226099  [    0/  569]\n",
      "loss: 0.127124  [  128/  569]\n",
      "loss: 0.147303  [  256/  569]\n",
      "loss: 0.283095  [  384/  569]\n",
      "loss: 0.209104  [  456/  569]\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 0.204594  [    0/  569]\n",
      "loss: 0.207800  [  128/  569]\n",
      "loss: 0.230106  [  256/  569]\n",
      "loss: 0.300025  [  384/  569]\n",
      "loss: 0.223497  [  456/  569]\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "loss: 0.179036  [    0/  569]\n",
      "loss: 0.195561  [  128/  569]\n",
      "loss: 0.233441  [  256/  569]\n",
      "loss: 0.158774  [  384/  569]\n",
      "loss: 0.162582  [  456/  569]\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "loss: 0.249265  [    0/  569]\n",
      "loss: 0.204178  [  128/  569]\n",
      "loss: 0.237983  [  256/  569]\n",
      "loss: 0.145826  [  384/  569]\n",
      "loss: 0.240500  [  456/  569]\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "loss: 0.115650  [    0/  569]\n",
      "loss: 0.122307  [  128/  569]\n",
      "loss: 0.294607  [  256/  569]\n",
      "loss: 0.143371  [  384/  569]\n",
      "loss: 0.108946  [  456/  569]\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "loss: 0.171723  [    0/  569]\n",
      "loss: 0.169936  [  128/  569]\n",
      "loss: 0.185199  [  256/  569]\n",
      "loss: 0.293630  [  384/  569]\n",
      "loss: 0.141608  [  456/  569]\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "loss: 0.113210  [    0/  569]\n",
      "loss: 0.239883  [  128/  569]\n",
      "loss: 0.230498  [  256/  569]\n",
      "loss: 0.286266  [  384/  569]\n",
      "loss: 0.135894  [  456/  569]\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "loss: 0.181148  [    0/  569]\n",
      "loss: 0.248937  [  128/  569]\n",
      "loss: 0.207597  [  256/  569]\n",
      "loss: 0.250509  [  384/  569]\n",
      "loss: 0.203607  [  456/  569]\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "loss: 0.178240  [    0/  569]\n",
      "loss: 0.398932  [  128/  569]\n",
      "loss: 0.214610  [  256/  569]\n",
      "loss: 0.197845  [  384/  569]\n",
      "loss: 0.165354  [  456/  569]\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "loss: 0.256275  [    0/  569]\n",
      "loss: 0.194881  [  128/  569]\n",
      "loss: 0.102723  [  256/  569]\n",
      "loss: 0.128273  [  384/  569]\n",
      "loss: 0.248431  [  456/  569]\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "loss: 0.218688  [    0/  569]\n",
      "loss: 0.216329  [  128/  569]\n",
      "loss: 0.220237  [  256/  569]\n",
      "loss: 0.167093  [  384/  569]\n",
      "loss: 0.100247  [  456/  569]\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "loss: 0.262707  [    0/  569]\n",
      "loss: 0.197364  [  128/  569]\n",
      "loss: 0.094007  [  256/  569]\n",
      "loss: 0.201071  [  384/  569]\n",
      "loss: 0.186535  [  456/  569]\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "loss: 0.264393  [    0/  569]\n",
      "loss: 0.366693  [  128/  569]\n",
      "loss: 0.290210  [  256/  569]\n",
      "loss: 0.128866  [  384/  569]\n",
      "loss: 0.259989  [  456/  569]\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "loss: 0.191298  [    0/  569]\n",
      "loss: 0.228847  [  128/  569]\n",
      "loss: 0.172574  [  256/  569]\n",
      "loss: 0.169632  [  384/  569]\n",
      "loss: 0.241150  [  456/  569]\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "loss: 0.294354  [    0/  569]\n",
      "loss: 0.228927  [  128/  569]\n",
      "loss: 0.120878  [  256/  569]\n",
      "loss: 0.172891  [  384/  569]\n",
      "loss: 0.173045  [  456/  569]\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "loss: 0.147790  [    0/  569]\n",
      "loss: 0.175947  [  128/  569]\n",
      "loss: 0.195460  [  256/  569]\n",
      "loss: 0.213438  [  384/  569]\n",
      "loss: 0.135124  [  456/  569]\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "loss: 0.096013  [    0/  569]\n",
      "loss: 0.132601  [  128/  569]\n",
      "loss: 0.249602  [  256/  569]\n",
      "loss: 0.308932  [  384/  569]\n",
      "loss: 0.124817  [  456/  569]\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "loss: 0.294903  [    0/  569]\n",
      "loss: 0.247138  [  128/  569]\n",
      "loss: 0.162489  [  256/  569]\n",
      "loss: 0.175881  [  384/  569]\n",
      "loss: 0.232879  [  456/  569]\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "loss: 0.219896  [    0/  569]\n",
      "loss: 0.169901  [  128/  569]\n",
      "loss: 0.219716  [  256/  569]\n",
      "loss: 0.209294  [  384/  569]\n",
      "loss: 0.250985  [  456/  569]\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "loss: 0.229975  [    0/  569]\n",
      "loss: 0.193637  [  128/  569]\n",
      "loss: 0.256769  [  256/  569]\n",
      "loss: 0.139529  [  384/  569]\n",
      "loss: 0.213630  [  456/  569]\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "loss: 0.192543  [    0/  569]\n",
      "loss: 0.149391  [  128/  569]\n",
      "loss: 0.179141  [  256/  569]\n",
      "loss: 0.236909  [  384/  569]\n",
      "loss: 0.218676  [  456/  569]\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "loss: 0.275136  [    0/  569]\n",
      "loss: 0.147618  [  128/  569]\n",
      "loss: 0.204575  [  256/  569]\n",
      "loss: 0.142226  [  384/  569]\n",
      "loss: 0.188894  [  456/  569]\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "loss: 0.234241  [    0/  569]\n",
      "loss: 0.242754  [  128/  569]\n",
      "loss: 0.142866  [  256/  569]\n",
      "loss: 0.259857  [  384/  569]\n",
      "loss: 0.162841  [  456/  569]\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "loss: 0.207552  [    0/  569]\n",
      "loss: 0.183136  [  128/  569]\n",
      "loss: 0.229869  [  256/  569]\n",
      "loss: 0.129122  [  384/  569]\n",
      "loss: 0.262170  [  456/  569]\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "loss: 0.124329  [    0/  569]\n",
      "loss: 0.112617  [  128/  569]\n",
      "loss: 0.195822  [  256/  569]\n",
      "loss: 0.286739  [  384/  569]\n",
      "loss: 0.268958  [  456/  569]\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "loss: 0.230122  [    0/  569]\n",
      "loss: 0.302430  [  128/  569]\n",
      "loss: 0.182196  [  256/  569]\n",
      "loss: 0.155844  [  384/  569]\n",
      "loss: 0.210360  [  456/  569]\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "loss: 0.248813  [    0/  569]\n",
      "loss: 0.122294  [  128/  569]\n",
      "loss: 0.265370  [  256/  569]\n",
      "loss: 0.212462  [  384/  569]\n",
      "loss: 0.248177  [  456/  569]\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "loss: 0.175158  [    0/  569]\n",
      "loss: 0.152023  [  128/  569]\n",
      "loss: 0.190796  [  256/  569]\n",
      "loss: 0.182199  [  384/  569]\n",
      "loss: 0.211019  [  456/  569]\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "loss: 0.134024  [    0/  569]\n",
      "loss: 0.184682  [  128/  569]\n",
      "loss: 0.105362  [  256/  569]\n",
      "loss: 0.314220  [  384/  569]\n",
      "loss: 0.210951  [  456/  569]\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "loss: 0.255533  [    0/  569]\n",
      "loss: 0.208771  [  128/  569]\n",
      "loss: 0.203884  [  256/  569]\n",
      "loss: 0.147707  [  384/  569]\n",
      "loss: 0.150600  [  456/  569]\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "loss: 0.303849  [    0/  569]\n",
      "loss: 0.289531  [  128/  569]\n",
      "loss: 0.186016  [  256/  569]\n",
      "loss: 0.188141  [  384/  569]\n",
      "loss: 0.168631  [  456/  569]\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "loss: 0.154392  [    0/  569]\n",
      "loss: 0.306626  [  128/  569]\n",
      "loss: 0.143888  [  256/  569]\n",
      "loss: 0.133841  [  384/  569]\n",
      "loss: 0.186624  [  456/  569]\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "loss: 0.126115  [    0/  569]\n",
      "loss: 0.141290  [  128/  569]\n",
      "loss: 0.115367  [  256/  569]\n",
      "loss: 0.196755  [  384/  569]\n",
      "loss: 0.151191  [  456/  569]\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "loss: 0.102892  [    0/  569]\n",
      "loss: 0.147413  [  128/  569]\n",
      "loss: 0.151947  [  256/  569]\n",
      "loss: 0.259520  [  384/  569]\n",
      "loss: 0.261690  [  456/  569]\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "loss: 0.215519  [    0/  569]\n",
      "loss: 0.209276  [  128/  569]\n",
      "loss: 0.176156  [  256/  569]\n",
      "loss: 0.231398  [  384/  569]\n",
      "loss: 0.301327  [  456/  569]\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "loss: 0.370140  [    0/  569]\n",
      "loss: 0.165822  [  128/  569]\n",
      "loss: 0.289163  [  256/  569]\n",
      "loss: 0.173671  [  384/  569]\n",
      "loss: 0.235438  [  456/  569]\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "loss: 0.197029  [    0/  569]\n",
      "loss: 0.144627  [  128/  569]\n",
      "loss: 0.142037  [  256/  569]\n",
      "loss: 0.230749  [  384/  569]\n",
      "loss: 0.256123  [  456/  569]\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "loss: 0.223528  [    0/  569]\n",
      "loss: 0.233899  [  128/  569]\n",
      "loss: 0.122420  [  256/  569]\n",
      "loss: 0.171006  [  384/  569]\n",
      "loss: 0.236137  [  456/  569]\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "loss: 0.277472  [    0/  569]\n",
      "loss: 0.154753  [  128/  569]\n",
      "loss: 0.263941  [  256/  569]\n",
      "loss: 0.213942  [  384/  569]\n",
      "loss: 0.178916  [  456/  569]\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "loss: 0.042265  [    0/  569]\n",
      "loss: 0.124543  [  128/  569]\n",
      "loss: 0.147614  [  256/  569]\n",
      "loss: 0.218382  [  384/  569]\n",
      "loss: 0.168757  [  456/  569]\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "loss: 0.277843  [    0/  569]\n",
      "loss: 0.148019  [  128/  569]\n",
      "loss: 0.068549  [  256/  569]\n",
      "loss: 0.137479  [  384/  569]\n",
      "loss: 0.255586  [  456/  569]\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "loss: 0.223624  [    0/  569]\n",
      "loss: 0.163784  [  128/  569]\n",
      "loss: 0.272049  [  256/  569]\n",
      "loss: 0.240478  [  384/  569]\n",
      "loss: 0.148563  [  456/  569]\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "loss: 0.258590  [    0/  569]\n",
      "loss: 0.191844  [  128/  569]\n",
      "loss: 0.136499  [  256/  569]\n",
      "loss: 0.227889  [  384/  569]\n",
      "loss: 0.193947  [  456/  569]\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "loss: 0.161692  [    0/  569]\n",
      "loss: 0.204759  [  128/  569]\n",
      "loss: 0.225248  [  256/  569]\n",
      "loss: 0.135130  [  384/  569]\n",
      "loss: 0.123277  [  456/  569]\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "loss: 0.133503  [    0/  569]\n",
      "loss: 0.176194  [  128/  569]\n",
      "loss: 0.185777  [  256/  569]\n",
      "loss: 0.290772  [  384/  569]\n",
      "loss: 0.122860  [  456/  569]\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "loss: 0.180403  [    0/  569]\n",
      "loss: 0.133545  [  128/  569]\n",
      "loss: 0.213793  [  256/  569]\n",
      "loss: 0.179326  [  384/  569]\n",
      "loss: 0.197655  [  456/  569]\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "loss: 0.100490  [    0/  569]\n",
      "loss: 0.240618  [  128/  569]\n",
      "loss: 0.130217  [  256/  569]\n",
      "loss: 0.142428  [  384/  569]\n",
      "loss: 0.336093  [  456/  569]\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "loss: 0.147495  [    0/  569]\n",
      "loss: 0.229375  [  128/  569]\n",
      "loss: 0.230535  [  256/  569]\n",
      "loss: 0.223826  [  384/  569]\n",
      "loss: 0.117985  [  456/  569]\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "loss: 0.231898  [    0/  569]\n",
      "loss: 0.202228  [  128/  569]\n",
      "loss: 0.174727  [  256/  569]\n",
      "loss: 0.347112  [  384/  569]\n",
      "loss: 0.190730  [  456/  569]\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "loss: 0.209285  [    0/  569]\n",
      "loss: 0.243659  [  128/  569]\n",
      "loss: 0.228618  [  256/  569]\n",
      "loss: 0.238202  [  384/  569]\n",
      "loss: 0.142612  [  456/  569]\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "loss: 0.246211  [    0/  569]\n",
      "loss: 0.215641  [  128/  569]\n",
      "loss: 0.116617  [  256/  569]\n",
      "loss: 0.351115  [  384/  569]\n",
      "loss: 0.181160  [  456/  569]\n",
      "Done! lr = 0.001\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dl, model, loss_fn, optimizer, scheduler=None)\n",
    "print(f\"Done! lr = {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducción del learning rate exponencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver el mismo ejemplo reduciendo el learning rate exponencialmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "# Se define una semilla para que la inicialización de los pesos aleatoria sea siempre la misma\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "model = CancerNeuralNetwork(31, 1)\n",
    "model.to(device)\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "LR = 1e-3\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "\n",
    "schedulerExponential = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.010835  [    0/  569]\n",
      "loss: 4.976839  [  128/  569]\n",
      "loss: 1.088202  [  256/  569]\n",
      "loss: 0.989754  [  384/  569]\n",
      "loss: 2.056240  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.452749  [    0/  569]\n",
      "loss: 0.817804  [  128/  569]\n",
      "loss: 1.101999  [  256/  569]\n",
      "loss: 0.719499  [  384/  569]\n",
      "loss: 0.860600  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.076765  [    0/  569]\n",
      "loss: 0.695022  [  128/  569]\n",
      "loss: 0.957789  [  256/  569]\n",
      "loss: 0.903349  [  384/  569]\n",
      "loss: 1.135585  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.2900e-04.\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.701673  [    0/  569]\n",
      "loss: 0.648564  [  128/  569]\n",
      "loss: 0.670574  [  256/  569]\n",
      "loss: 0.607591  [  384/  569]\n",
      "loss: 0.579254  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.5610e-04.\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.584377  [    0/  569]\n",
      "loss: 0.629451  [  128/  569]\n",
      "loss: 0.612009  [  256/  569]\n",
      "loss: 0.597891  [  384/  569]\n",
      "loss: 0.555261  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.9049e-04.\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.551084  [    0/  569]\n",
      "loss: 0.593376  [  128/  569]\n",
      "loss: 0.605462  [  256/  569]\n",
      "loss: 0.653608  [  384/  569]\n",
      "loss: 0.568694  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.3144e-04.\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.559495  [    0/  569]\n",
      "loss: 0.591203  [  128/  569]\n",
      "loss: 0.563477  [  256/  569]\n",
      "loss: 0.571932  [  384/  569]\n",
      "loss: 0.587132  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.7830e-04.\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.581155  [    0/  569]\n",
      "loss: 0.557200  [  128/  569]\n",
      "loss: 0.615105  [  256/  569]\n",
      "loss: 0.528208  [  384/  569]\n",
      "loss: 0.549739  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.3047e-04.\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.585598  [    0/  569]\n",
      "loss: 0.561335  [  128/  569]\n",
      "loss: 0.540029  [  256/  569]\n",
      "loss: 0.558765  [  384/  569]\n",
      "loss: 0.551368  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.8742e-04.\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.559206  [    0/  569]\n",
      "loss: 0.553010  [  128/  569]\n",
      "loss: 0.525550  [  256/  569]\n",
      "loss: 0.551794  [  384/  569]\n",
      "loss: 0.568441  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4868e-04.\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.556216  [    0/  569]\n",
      "loss: 0.521318  [  128/  569]\n",
      "loss: 0.510400  [  256/  569]\n",
      "loss: 0.530822  [  384/  569]\n",
      "loss: 0.559207  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.1381e-04.\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.502698  [    0/  569]\n",
      "loss: 0.552587  [  128/  569]\n",
      "loss: 0.550381  [  256/  569]\n",
      "loss: 0.525957  [  384/  569]\n",
      "loss: 0.499059  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8243e-04.\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.507806  [    0/  569]\n",
      "loss: 0.562871  [  128/  569]\n",
      "loss: 0.551181  [  256/  569]\n",
      "loss: 0.507538  [  384/  569]\n",
      "loss: 0.534216  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5419e-04.\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.553730  [    0/  569]\n",
      "loss: 0.543440  [  128/  569]\n",
      "loss: 0.561131  [  256/  569]\n",
      "loss: 0.552931  [  384/  569]\n",
      "loss: 0.527676  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2877e-04.\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.529662  [    0/  569]\n",
      "loss: 0.510787  [  128/  569]\n",
      "loss: 0.481543  [  256/  569]\n",
      "loss: 0.552605  [  384/  569]\n",
      "loss: 0.553198  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0589e-04.\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.545856  [    0/  569]\n",
      "loss: 0.495802  [  128/  569]\n",
      "loss: 0.532656  [  256/  569]\n",
      "loss: 0.512769  [  384/  569]\n",
      "loss: 0.549671  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8530e-04.\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.521138  [    0/  569]\n",
      "loss: 0.501720  [  128/  569]\n",
      "loss: 0.537137  [  256/  569]\n",
      "loss: 0.513372  [  384/  569]\n",
      "loss: 0.401879  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6677e-04.\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.510076  [    0/  569]\n",
      "loss: 0.532782  [  128/  569]\n",
      "loss: 0.486431  [  256/  569]\n",
      "loss: 0.504868  [  384/  569]\n",
      "loss: 0.496710  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5009e-04.\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.513390  [    0/  569]\n",
      "loss: 0.500137  [  128/  569]\n",
      "loss: 0.480049  [  256/  569]\n",
      "loss: 0.467917  [  384/  569]\n",
      "loss: 0.528345  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3509e-04.\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.482476  [    0/  569]\n",
      "loss: 0.526606  [  128/  569]\n",
      "loss: 0.542273  [  256/  569]\n",
      "loss: 0.514688  [  384/  569]\n",
      "loss: 0.487813  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2158e-04.\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.458018  [    0/  569]\n",
      "loss: 0.503167  [  128/  569]\n",
      "loss: 0.568097  [  256/  569]\n",
      "loss: 0.517542  [  384/  569]\n",
      "loss: 0.509591  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0942e-04.\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.523150  [    0/  569]\n",
      "loss: 0.516567  [  128/  569]\n",
      "loss: 0.539929  [  256/  569]\n",
      "loss: 0.490629  [  384/  569]\n",
      "loss: 0.536520  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.8477e-05.\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.516190  [    0/  569]\n",
      "loss: 0.534259  [  128/  569]\n",
      "loss: 0.483823  [  256/  569]\n",
      "loss: 0.540584  [  384/  569]\n",
      "loss: 0.515317  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.8629e-05.\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.488848  [    0/  569]\n",
      "loss: 0.488586  [  128/  569]\n",
      "loss: 0.527949  [  256/  569]\n",
      "loss: 0.517626  [  384/  569]\n",
      "loss: 0.506905  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.9766e-05.\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.534676  [    0/  569]\n",
      "loss: 0.496036  [  128/  569]\n",
      "loss: 0.518143  [  256/  569]\n",
      "loss: 0.491111  [  384/  569]\n",
      "loss: 0.480157  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.1790e-05.\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.457590  [    0/  569]\n",
      "loss: 0.468447  [  128/  569]\n",
      "loss: 0.485586  [  256/  569]\n",
      "loss: 0.480727  [  384/  569]\n",
      "loss: 0.496449  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.4611e-05.\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.448108  [    0/  569]\n",
      "loss: 0.506405  [  128/  569]\n",
      "loss: 0.491574  [  256/  569]\n",
      "loss: 0.542300  [  384/  569]\n",
      "loss: 0.498557  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.8150e-05.\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.571007  [    0/  569]\n",
      "loss: 0.500370  [  128/  569]\n",
      "loss: 0.526144  [  256/  569]\n",
      "loss: 0.517554  [  384/  569]\n",
      "loss: 0.418860  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.2335e-05.\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.489645  [    0/  569]\n",
      "loss: 0.492261  [  128/  569]\n",
      "loss: 0.497501  [  256/  569]\n",
      "loss: 0.526421  [  384/  569]\n",
      "loss: 0.507547  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.7101e-05.\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.515241  [    0/  569]\n",
      "loss: 0.505122  [  128/  569]\n",
      "loss: 0.482242  [  256/  569]\n",
      "loss: 0.469245  [  384/  569]\n",
      "loss: 0.513568  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.2391e-05.\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.471109  [    0/  569]\n",
      "loss: 0.464097  [  128/  569]\n",
      "loss: 0.465745  [  256/  569]\n",
      "loss: 0.549933  [  384/  569]\n",
      "loss: 0.551712  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.8152e-05.\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.560833  [    0/  569]\n",
      "loss: 0.476519  [  128/  569]\n",
      "loss: 0.462926  [  256/  569]\n",
      "loss: 0.466127  [  384/  569]\n",
      "loss: 0.523287  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4337e-05.\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.501092  [    0/  569]\n",
      "loss: 0.471013  [  128/  569]\n",
      "loss: 0.481729  [  256/  569]\n",
      "loss: 0.513086  [  384/  569]\n",
      "loss: 0.499433  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.0903e-05.\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.513799  [    0/  569]\n",
      "loss: 0.533031  [  128/  569]\n",
      "loss: 0.492603  [  256/  569]\n",
      "loss: 0.532121  [  384/  569]\n",
      "loss: 0.480680  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.7813e-05.\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.542245  [    0/  569]\n",
      "loss: 0.460193  [  128/  569]\n",
      "loss: 0.501657  [  256/  569]\n",
      "loss: 0.473735  [  384/  569]\n",
      "loss: 0.524984  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5032e-05.\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.531146  [    0/  569]\n",
      "loss: 0.451763  [  128/  569]\n",
      "loss: 0.486044  [  256/  569]\n",
      "loss: 0.523665  [  384/  569]\n",
      "loss: 0.466094  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2528e-05.\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.509640  [    0/  569]\n",
      "loss: 0.523161  [  128/  569]\n",
      "loss: 0.489548  [  256/  569]\n",
      "loss: 0.480592  [  384/  569]\n",
      "loss: 0.478782  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0276e-05.\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.520181  [    0/  569]\n",
      "loss: 0.455620  [  128/  569]\n",
      "loss: 0.494460  [  256/  569]\n",
      "loss: 0.456742  [  384/  569]\n",
      "loss: 0.553443  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8248e-05.\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.501131  [    0/  569]\n",
      "loss: 0.478075  [  128/  569]\n",
      "loss: 0.482241  [  256/  569]\n",
      "loss: 0.477432  [  384/  569]\n",
      "loss: 0.520742  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6423e-05.\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.506489  [    0/  569]\n",
      "loss: 0.486815  [  128/  569]\n",
      "loss: 0.483892  [  256/  569]\n",
      "loss: 0.522162  [  384/  569]\n",
      "loss: 0.468393  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4781e-05.\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.478692  [    0/  569]\n",
      "loss: 0.500820  [  128/  569]\n",
      "loss: 0.442842  [  256/  569]\n",
      "loss: 0.477344  [  384/  569]\n",
      "loss: 0.495354  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3303e-05.\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.520748  [    0/  569]\n",
      "loss: 0.497158  [  128/  569]\n",
      "loss: 0.513849  [  256/  569]\n",
      "loss: 0.486552  [  384/  569]\n",
      "loss: 0.498001  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1973e-05.\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.528901  [    0/  569]\n",
      "loss: 0.489231  [  128/  569]\n",
      "loss: 0.478240  [  256/  569]\n",
      "loss: 0.504754  [  384/  569]\n",
      "loss: 0.471272  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0775e-05.\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.504814  [    0/  569]\n",
      "loss: 0.484547  [  128/  569]\n",
      "loss: 0.511491  [  256/  569]\n",
      "loss: 0.541848  [  384/  569]\n",
      "loss: 0.494569  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.6977e-06.\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.543934  [    0/  569]\n",
      "loss: 0.516278  [  128/  569]\n",
      "loss: 0.504200  [  256/  569]\n",
      "loss: 0.482803  [  384/  569]\n",
      "loss: 0.494467  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.7280e-06.\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.509743  [    0/  569]\n",
      "loss: 0.469576  [  128/  569]\n",
      "loss: 0.484183  [  256/  569]\n",
      "loss: 0.520189  [  384/  569]\n",
      "loss: 0.463320  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.8552e-06.\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.501340  [    0/  569]\n",
      "loss: 0.502248  [  128/  569]\n",
      "loss: 0.490474  [  256/  569]\n",
      "loss: 0.500013  [  384/  569]\n",
      "loss: 0.449719  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.0697e-06.\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.505837  [    0/  569]\n",
      "loss: 0.483937  [  128/  569]\n",
      "loss: 0.458455  [  256/  569]\n",
      "loss: 0.487382  [  384/  569]\n",
      "loss: 0.504022  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.3627e-06.\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.533149  [    0/  569]\n",
      "loss: 0.462000  [  128/  569]\n",
      "loss: 0.487747  [  256/  569]\n",
      "loss: 0.552044  [  384/  569]\n",
      "loss: 0.476615  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.7264e-06.\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.486019  [    0/  569]\n",
      "loss: 0.489138  [  128/  569]\n",
      "loss: 0.473455  [  256/  569]\n",
      "loss: 0.482539  [  384/  569]\n",
      "loss: 0.476538  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.1538e-06.\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.473178  [    0/  569]\n",
      "loss: 0.550564  [  128/  569]\n",
      "loss: 0.481441  [  256/  569]\n",
      "loss: 0.545266  [  384/  569]\n",
      "loss: 0.471438  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.6384e-06.\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.470910  [    0/  569]\n",
      "loss: 0.469885  [  128/  569]\n",
      "loss: 0.519271  [  256/  569]\n",
      "loss: 0.459884  [  384/  569]\n",
      "loss: 0.526085  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.1746e-06.\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.454163  [    0/  569]\n",
      "loss: 0.473303  [  128/  569]\n",
      "loss: 0.529579  [  256/  569]\n",
      "loss: 0.483655  [  384/  569]\n",
      "loss: 0.577227  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.7571e-06.\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.456411  [    0/  569]\n",
      "loss: 0.506780  [  128/  569]\n",
      "loss: 0.508373  [  256/  569]\n",
      "loss: 0.467486  [  384/  569]\n",
      "loss: 0.545889  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.3814e-06.\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.467301  [    0/  569]\n",
      "loss: 0.515915  [  128/  569]\n",
      "loss: 0.527114  [  256/  569]\n",
      "loss: 0.508293  [  384/  569]\n",
      "loss: 0.541268  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.0433e-06.\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.520867  [    0/  569]\n",
      "loss: 0.469643  [  128/  569]\n",
      "loss: 0.526800  [  256/  569]\n",
      "loss: 0.455738  [  384/  569]\n",
      "loss: 0.476750  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.7389e-06.\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.496595  [    0/  569]\n",
      "loss: 0.541986  [  128/  569]\n",
      "loss: 0.479767  [  256/  569]\n",
      "loss: 0.430425  [  384/  569]\n",
      "loss: 0.448480  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4650e-06.\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.501036  [    0/  569]\n",
      "loss: 0.508021  [  128/  569]\n",
      "loss: 0.479641  [  256/  569]\n",
      "loss: 0.505938  [  384/  569]\n",
      "loss: 0.465939  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2185e-06.\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.499537  [    0/  569]\n",
      "loss: 0.475005  [  128/  569]\n",
      "loss: 0.469257  [  256/  569]\n",
      "loss: 0.509334  [  384/  569]\n",
      "loss: 0.495083  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9967e-06.\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.499669  [    0/  569]\n",
      "loss: 0.499362  [  128/  569]\n",
      "loss: 0.543428  [  256/  569]\n",
      "loss: 0.488942  [  384/  569]\n",
      "loss: 0.459550  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7970e-06.\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.535257  [    0/  569]\n",
      "loss: 0.534208  [  128/  569]\n",
      "loss: 0.472607  [  256/  569]\n",
      "loss: 0.470335  [  384/  569]\n",
      "loss: 0.483833  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6173e-06.\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.522485  [    0/  569]\n",
      "loss: 0.460799  [  128/  569]\n",
      "loss: 0.495818  [  256/  569]\n",
      "loss: 0.505285  [  384/  569]\n",
      "loss: 0.469134  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4556e-06.\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.460099  [    0/  569]\n",
      "loss: 0.457894  [  128/  569]\n",
      "loss: 0.533524  [  256/  569]\n",
      "loss: 0.501508  [  384/  569]\n",
      "loss: 0.487374  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3100e-06.\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.464289  [    0/  569]\n",
      "loss: 0.511935  [  128/  569]\n",
      "loss: 0.495576  [  256/  569]\n",
      "loss: 0.540824  [  384/  569]\n",
      "loss: 0.484119  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1790e-06.\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.485482  [    0/  569]\n",
      "loss: 0.449939  [  128/  569]\n",
      "loss: 0.496226  [  256/  569]\n",
      "loss: 0.501589  [  384/  569]\n",
      "loss: 0.542650  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0611e-06.\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.539530  [    0/  569]\n",
      "loss: 0.530663  [  128/  569]\n",
      "loss: 0.489341  [  256/  569]\n",
      "loss: 0.439916  [  384/  569]\n",
      "loss: 0.501925  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.5500e-07.\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.512576  [    0/  569]\n",
      "loss: 0.500437  [  128/  569]\n",
      "loss: 0.488869  [  256/  569]\n",
      "loss: 0.487714  [  384/  569]\n",
      "loss: 0.457207  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.5950e-07.\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.489931  [    0/  569]\n",
      "loss: 0.483036  [  128/  569]\n",
      "loss: 0.504213  [  256/  569]\n",
      "loss: 0.501579  [  384/  569]\n",
      "loss: 0.500445  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.7355e-07.\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.505654  [    0/  569]\n",
      "loss: 0.505556  [  128/  569]\n",
      "loss: 0.522134  [  256/  569]\n",
      "loss: 0.481598  [  384/  569]\n",
      "loss: 0.449562  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.9620e-07.\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.525090  [    0/  569]\n",
      "loss: 0.474651  [  128/  569]\n",
      "loss: 0.453523  [  256/  569]\n",
      "loss: 0.457047  [  384/  569]\n",
      "loss: 0.482985  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.2658e-07.\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.538428  [    0/  569]\n",
      "loss: 0.473426  [  128/  569]\n",
      "loss: 0.471190  [  256/  569]\n",
      "loss: 0.493197  [  384/  569]\n",
      "loss: 0.524375  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.6392e-07.\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.543344  [    0/  569]\n",
      "loss: 0.508941  [  128/  569]\n",
      "loss: 0.501859  [  256/  569]\n",
      "loss: 0.470102  [  384/  569]\n",
      "loss: 0.508133  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.0753e-07.\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.437423  [    0/  569]\n",
      "loss: 0.508780  [  128/  569]\n",
      "loss: 0.481995  [  256/  569]\n",
      "loss: 0.491402  [  384/  569]\n",
      "loss: 0.513043  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.5678e-07.\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.523248  [    0/  569]\n",
      "loss: 0.486009  [  128/  569]\n",
      "loss: 0.494281  [  256/  569]\n",
      "loss: 0.427223  [  384/  569]\n",
      "loss: 0.467718  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.1110e-07.\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.506742  [    0/  569]\n",
      "loss: 0.471784  [  128/  569]\n",
      "loss: 0.515377  [  256/  569]\n",
      "loss: 0.504676  [  384/  569]\n",
      "loss: 0.501501  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.6999e-07.\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.462819  [    0/  569]\n",
      "loss: 0.474534  [  128/  569]\n",
      "loss: 0.531855  [  256/  569]\n",
      "loss: 0.514266  [  384/  569]\n",
      "loss: 0.513947  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.3299e-07.\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.533899  [    0/  569]\n",
      "loss: 0.461988  [  128/  569]\n",
      "loss: 0.470033  [  256/  569]\n",
      "loss: 0.520833  [  384/  569]\n",
      "loss: 0.510568  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9969e-07.\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.506760  [    0/  569]\n",
      "loss: 0.507232  [  128/  569]\n",
      "loss: 0.493627  [  256/  569]\n",
      "loss: 0.517464  [  384/  569]\n",
      "loss: 0.512193  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6972e-07.\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.490871  [    0/  569]\n",
      "loss: 0.477991  [  128/  569]\n",
      "loss: 0.455718  [  256/  569]\n",
      "loss: 0.476025  [  384/  569]\n",
      "loss: 0.448221  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4275e-07.\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.482651  [    0/  569]\n",
      "loss: 0.467870  [  128/  569]\n",
      "loss: 0.502581  [  256/  569]\n",
      "loss: 0.485588  [  384/  569]\n",
      "loss: 0.528466  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1847e-07.\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.463228  [    0/  569]\n",
      "loss: 0.488798  [  128/  569]\n",
      "loss: 0.493509  [  256/  569]\n",
      "loss: 0.537906  [  384/  569]\n",
      "loss: 0.531104  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9663e-07.\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.459209  [    0/  569]\n",
      "loss: 0.504902  [  128/  569]\n",
      "loss: 0.531699  [  256/  569]\n",
      "loss: 0.555529  [  384/  569]\n",
      "loss: 0.455499  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7696e-07.\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.526885  [    0/  569]\n",
      "loss: 0.539515  [  128/  569]\n",
      "loss: 0.477300  [  256/  569]\n",
      "loss: 0.485729  [  384/  569]\n",
      "loss: 0.517462  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5927e-07.\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.515930  [    0/  569]\n",
      "loss: 0.502507  [  128/  569]\n",
      "loss: 0.442946  [  256/  569]\n",
      "loss: 0.481099  [  384/  569]\n",
      "loss: 0.505224  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4334e-07.\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.463236  [    0/  569]\n",
      "loss: 0.498708  [  128/  569]\n",
      "loss: 0.482169  [  256/  569]\n",
      "loss: 0.511234  [  384/  569]\n",
      "loss: 0.486922  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2901e-07.\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.502482  [    0/  569]\n",
      "loss: 0.501609  [  128/  569]\n",
      "loss: 0.527503  [  256/  569]\n",
      "loss: 0.478510  [  384/  569]\n",
      "loss: 0.480314  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1611e-07.\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.463657  [    0/  569]\n",
      "loss: 0.507594  [  128/  569]\n",
      "loss: 0.487734  [  256/  569]\n",
      "loss: 0.471626  [  384/  569]\n",
      "loss: 0.492362  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0450e-07.\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.453987  [    0/  569]\n",
      "loss: 0.530651  [  128/  569]\n",
      "loss: 0.544687  [  256/  569]\n",
      "loss: 0.520220  [  384/  569]\n",
      "loss: 0.478116  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.4046e-08.\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.459284  [    0/  569]\n",
      "loss: 0.513450  [  128/  569]\n",
      "loss: 0.459539  [  256/  569]\n",
      "loss: 0.537646  [  384/  569]\n",
      "loss: 0.542297  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.4641e-08.\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.482630  [    0/  569]\n",
      "loss: 0.513550  [  128/  569]\n",
      "loss: 0.491888  [  256/  569]\n",
      "loss: 0.534987  [  384/  569]\n",
      "loss: 0.477108  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.6177e-08.\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.505653  [    0/  569]\n",
      "loss: 0.465650  [  128/  569]\n",
      "loss: 0.481900  [  256/  569]\n",
      "loss: 0.454563  [  384/  569]\n",
      "loss: 0.493038  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.8560e-08.\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.478102  [    0/  569]\n",
      "loss: 0.496753  [  128/  569]\n",
      "loss: 0.486742  [  256/  569]\n",
      "loss: 0.508917  [  384/  569]\n",
      "loss: 0.504914  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.1704e-08.\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.507278  [    0/  569]\n",
      "loss: 0.509284  [  128/  569]\n",
      "loss: 0.481731  [  256/  569]\n",
      "loss: 0.509018  [  384/  569]\n",
      "loss: 0.431367  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.5533e-08.\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.536533  [    0/  569]\n",
      "loss: 0.481867  [  128/  569]\n",
      "loss: 0.521631  [  256/  569]\n",
      "loss: 0.478199  [  384/  569]\n",
      "loss: 0.478897  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.9980e-08.\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.475573  [    0/  569]\n",
      "loss: 0.502060  [  128/  569]\n",
      "loss: 0.497746  [  256/  569]\n",
      "loss: 0.506001  [  384/  569]\n",
      "loss: 0.453228  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.4982e-08.\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.451049  [    0/  569]\n",
      "loss: 0.453686  [  128/  569]\n",
      "loss: 0.480324  [  256/  569]\n",
      "loss: 0.551471  [  384/  569]\n",
      "loss: 0.475313  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.0484e-08.\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.491703  [    0/  569]\n",
      "loss: 0.471949  [  128/  569]\n",
      "loss: 0.530756  [  256/  569]\n",
      "loss: 0.452334  [  384/  569]\n",
      "loss: 0.450778  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.6435e-08.\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.513654  [    0/  569]\n",
      "loss: 0.481025  [  128/  569]\n",
      "loss: 0.504222  [  256/  569]\n",
      "loss: 0.484368  [  384/  569]\n",
      "loss: 0.521615  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.2792e-08.\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.494438  [    0/  569]\n",
      "loss: 0.460757  [  128/  569]\n",
      "loss: 0.456111  [  256/  569]\n",
      "loss: 0.512194  [  384/  569]\n",
      "loss: 0.476931  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9513e-08.\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.538684  [    0/  569]\n",
      "loss: 0.499525  [  128/  569]\n",
      "loss: 0.421158  [  256/  569]\n",
      "loss: 0.518019  [  384/  569]\n",
      "loss: 0.490450  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6561e-08.\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.521803  [    0/  569]\n",
      "loss: 0.509414  [  128/  569]\n",
      "loss: 0.485654  [  256/  569]\n",
      "loss: 0.454295  [  384/  569]\n",
      "loss: 0.489260  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3905e-08.\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.483755  [    0/  569]\n",
      "loss: 0.480181  [  128/  569]\n",
      "loss: 0.486118  [  256/  569]\n",
      "loss: 0.495887  [  384/  569]\n",
      "loss: 0.518047  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1515e-08.\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.545969  [    0/  569]\n",
      "loss: 0.469716  [  128/  569]\n",
      "loss: 0.536787  [  256/  569]\n",
      "loss: 0.469985  [  384/  569]\n",
      "loss: 0.476385  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9363e-08.\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.493091  [    0/  569]\n",
      "loss: 0.503821  [  128/  569]\n",
      "loss: 0.555063  [  256/  569]\n",
      "loss: 0.490606  [  384/  569]\n",
      "loss: 0.476046  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7427e-08.\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.489500  [    0/  569]\n",
      "loss: 0.475456  [  128/  569]\n",
      "loss: 0.462621  [  256/  569]\n",
      "loss: 0.456987  [  384/  569]\n",
      "loss: 0.522521  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5684e-08.\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.442357  [    0/  569]\n",
      "loss: 0.534285  [  128/  569]\n",
      "loss: 0.467913  [  256/  569]\n",
      "loss: 0.443356  [  384/  569]\n",
      "loss: 0.500303  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4116e-08.\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.499834  [    0/  569]\n",
      "loss: 0.501529  [  128/  569]\n",
      "loss: 0.537457  [  256/  569]\n",
      "loss: 0.464067  [  384/  569]\n",
      "loss: 0.480921  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2704e-08.\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.527521  [    0/  569]\n",
      "loss: 0.460619  [  128/  569]\n",
      "loss: 0.516907  [  256/  569]\n",
      "loss: 0.472791  [  384/  569]\n",
      "loss: 0.458585  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1434e-08.\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.559287  [    0/  569]\n",
      "loss: 0.478094  [  128/  569]\n",
      "loss: 0.483017  [  256/  569]\n",
      "loss: 0.497930  [  384/  569]\n",
      "loss: 0.502215  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0290e-08.\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.505413  [    0/  569]\n",
      "loss: 0.488887  [  128/  569]\n",
      "loss: 0.506789  [  256/  569]\n",
      "loss: 0.476906  [  384/  569]\n",
      "loss: 0.569637  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.2614e-09.\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.547981  [    0/  569]\n",
      "loss: 0.446624  [  128/  569]\n",
      "loss: 0.439664  [  256/  569]\n",
      "loss: 0.477089  [  384/  569]\n",
      "loss: 0.505636  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.3352e-09.\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.472431  [    0/  569]\n",
      "loss: 0.504216  [  128/  569]\n",
      "loss: 0.482161  [  256/  569]\n",
      "loss: 0.507979  [  384/  569]\n",
      "loss: 0.504447  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.5017e-09.\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.584012  [    0/  569]\n",
      "loss: 0.467265  [  128/  569]\n",
      "loss: 0.469637  [  256/  569]\n",
      "loss: 0.442851  [  384/  569]\n",
      "loss: 0.535667  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.7516e-09.\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.489090  [    0/  569]\n",
      "loss: 0.474434  [  128/  569]\n",
      "loss: 0.503100  [  256/  569]\n",
      "loss: 0.433067  [  384/  569]\n",
      "loss: 0.519183  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.0764e-09.\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.509311  [    0/  569]\n",
      "loss: 0.485983  [  128/  569]\n",
      "loss: 0.469075  [  256/  569]\n",
      "loss: 0.487174  [  384/  569]\n",
      "loss: 0.509058  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.4688e-09.\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.511176  [    0/  569]\n",
      "loss: 0.531767  [  128/  569]\n",
      "loss: 0.454100  [  256/  569]\n",
      "loss: 0.439795  [  384/  569]\n",
      "loss: 0.505902  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.9219e-09.\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.477884  [    0/  569]\n",
      "loss: 0.490577  [  128/  569]\n",
      "loss: 0.502470  [  256/  569]\n",
      "loss: 0.499436  [  384/  569]\n",
      "loss: 0.522199  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.4297e-09.\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.468777  [    0/  569]\n",
      "loss: 0.452983  [  128/  569]\n",
      "loss: 0.511146  [  256/  569]\n",
      "loss: 0.558780  [  384/  569]\n",
      "loss: 0.499148  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.9867e-09.\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.443324  [    0/  569]\n",
      "loss: 0.497047  [  128/  569]\n",
      "loss: 0.509191  [  256/  569]\n",
      "loss: 0.505307  [  384/  569]\n",
      "loss: 0.495948  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.5881e-09.\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.524478  [    0/  569]\n",
      "loss: 0.486897  [  128/  569]\n",
      "loss: 0.472116  [  256/  569]\n",
      "loss: 0.478045  [  384/  569]\n",
      "loss: 0.452522  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.2292e-09.\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.472182  [    0/  569]\n",
      "loss: 0.461560  [  128/  569]\n",
      "loss: 0.517071  [  256/  569]\n",
      "loss: 0.489174  [  384/  569]\n",
      "loss: 0.493264  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9063e-09.\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.494617  [    0/  569]\n",
      "loss: 0.447025  [  128/  569]\n",
      "loss: 0.490157  [  256/  569]\n",
      "loss: 0.502166  [  384/  569]\n",
      "loss: 0.567924  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6157e-09.\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.468392  [    0/  569]\n",
      "loss: 0.547022  [  128/  569]\n",
      "loss: 0.452728  [  256/  569]\n",
      "loss: 0.470773  [  384/  569]\n",
      "loss: 0.527777  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3541e-09.\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.511470  [    0/  569]\n",
      "loss: 0.494974  [  128/  569]\n",
      "loss: 0.462150  [  256/  569]\n",
      "loss: 0.452598  [  384/  569]\n",
      "loss: 0.449685  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1187e-09.\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.486590  [    0/  569]\n",
      "loss: 0.510606  [  128/  569]\n",
      "loss: 0.523609  [  256/  569]\n",
      "loss: 0.406924  [  384/  569]\n",
      "loss: 0.524330  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9068e-09.\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.501931  [    0/  569]\n",
      "loss: 0.491987  [  128/  569]\n",
      "loss: 0.457426  [  256/  569]\n",
      "loss: 0.485675  [  384/  569]\n",
      "loss: 0.549870  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7162e-09.\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.555562  [    0/  569]\n",
      "loss: 0.527718  [  128/  569]\n",
      "loss: 0.466219  [  256/  569]\n",
      "loss: 0.491320  [  384/  569]\n",
      "loss: 0.491963  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5445e-09.\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.504971  [    0/  569]\n",
      "loss: 0.485898  [  128/  569]\n",
      "loss: 0.475412  [  256/  569]\n",
      "loss: 0.485876  [  384/  569]\n",
      "loss: 0.501155  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3901e-09.\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.505665  [    0/  569]\n",
      "loss: 0.482601  [  128/  569]\n",
      "loss: 0.494212  [  256/  569]\n",
      "loss: 0.505898  [  384/  569]\n",
      "loss: 0.546547  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2511e-09.\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.557358  [    0/  569]\n",
      "loss: 0.495675  [  128/  569]\n",
      "loss: 0.476447  [  256/  569]\n",
      "loss: 0.519282  [  384/  569]\n",
      "loss: 0.462733  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1260e-09.\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.448223  [    0/  569]\n",
      "loss: 0.502141  [  128/  569]\n",
      "loss: 0.526286  [  256/  569]\n",
      "loss: 0.494186  [  384/  569]\n",
      "loss: 0.471313  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0134e-09.\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.461016  [    0/  569]\n",
      "loss: 0.494552  [  128/  569]\n",
      "loss: 0.523540  [  256/  569]\n",
      "loss: 0.510849  [  384/  569]\n",
      "loss: 0.534889  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.1203e-10.\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.497302  [    0/  569]\n",
      "loss: 0.567914  [  128/  569]\n",
      "loss: 0.442680  [  256/  569]\n",
      "loss: 0.426759  [  384/  569]\n",
      "loss: 0.551942  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.2083e-10.\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.522000  [    0/  569]\n",
      "loss: 0.473334  [  128/  569]\n",
      "loss: 0.470951  [  256/  569]\n",
      "loss: 0.499864  [  384/  569]\n",
      "loss: 0.464434  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.3875e-10.\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.462233  [    0/  569]\n",
      "loss: 0.469747  [  128/  569]\n",
      "loss: 0.506034  [  256/  569]\n",
      "loss: 0.498028  [  384/  569]\n",
      "loss: 0.526922  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.6487e-10.\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.456086  [    0/  569]\n",
      "loss: 0.510089  [  128/  569]\n",
      "loss: 0.485843  [  256/  569]\n",
      "loss: 0.492017  [  384/  569]\n",
      "loss: 0.435711  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.9839e-10.\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.501575  [    0/  569]\n",
      "loss: 0.504129  [  128/  569]\n",
      "loss: 0.513854  [  256/  569]\n",
      "loss: 0.485471  [  384/  569]\n",
      "loss: 0.444732  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.3855e-10.\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.508381  [    0/  569]\n",
      "loss: 0.543921  [  128/  569]\n",
      "loss: 0.458125  [  256/  569]\n",
      "loss: 0.478802  [  384/  569]\n",
      "loss: 0.491441  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.8469e-10.\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.510336  [    0/  569]\n",
      "loss: 0.511550  [  128/  569]\n",
      "loss: 0.469688  [  256/  569]\n",
      "loss: 0.512456  [  384/  569]\n",
      "loss: 0.479037  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.3622e-10.\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.490528  [    0/  569]\n",
      "loss: 0.503689  [  128/  569]\n",
      "loss: 0.452908  [  256/  569]\n",
      "loss: 0.493071  [  384/  569]\n",
      "loss: 0.514761  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.9260e-10.\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.484299  [    0/  569]\n",
      "loss: 0.483953  [  128/  569]\n",
      "loss: 0.557753  [  256/  569]\n",
      "loss: 0.461889  [  384/  569]\n",
      "loss: 0.493598  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.5334e-10.\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.513849  [    0/  569]\n",
      "loss: 0.496861  [  128/  569]\n",
      "loss: 0.515297  [  256/  569]\n",
      "loss: 0.486728  [  384/  569]\n",
      "loss: 0.525789  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.1801e-10.\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.482012  [    0/  569]\n",
      "loss: 0.541138  [  128/  569]\n",
      "loss: 0.430936  [  256/  569]\n",
      "loss: 0.541039  [  384/  569]\n",
      "loss: 0.487958  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8621e-10.\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.519877  [    0/  569]\n",
      "loss: 0.546502  [  128/  569]\n",
      "loss: 0.495212  [  256/  569]\n",
      "loss: 0.513908  [  384/  569]\n",
      "loss: 0.456910  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5759e-10.\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.456903  [    0/  569]\n",
      "loss: 0.477341  [  128/  569]\n",
      "loss: 0.489304  [  256/  569]\n",
      "loss: 0.454188  [  384/  569]\n",
      "loss: 0.496683  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3183e-10.\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.498605  [    0/  569]\n",
      "loss: 0.513352  [  128/  569]\n",
      "loss: 0.492472  [  256/  569]\n",
      "loss: 0.487247  [  384/  569]\n",
      "loss: 0.457247  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0864e-10.\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.533420  [    0/  569]\n",
      "loss: 0.500709  [  128/  569]\n",
      "loss: 0.477686  [  256/  569]\n",
      "loss: 0.519186  [  384/  569]\n",
      "loss: 0.474704  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8778e-10.\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.442699  [    0/  569]\n",
      "loss: 0.472724  [  128/  569]\n",
      "loss: 0.489110  [  256/  569]\n",
      "loss: 0.510749  [  384/  569]\n",
      "loss: 0.522019  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6900e-10.\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.486682  [    0/  569]\n",
      "loss: 0.493648  [  128/  569]\n",
      "loss: 0.488100  [  256/  569]\n",
      "loss: 0.477224  [  384/  569]\n",
      "loss: 0.525003  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5210e-10.\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.504751  [    0/  569]\n",
      "loss: 0.506258  [  128/  569]\n",
      "loss: 0.481332  [  256/  569]\n",
      "loss: 0.462399  [  384/  569]\n",
      "loss: 0.510114  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3689e-10.\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.444148  [    0/  569]\n",
      "loss: 0.475955  [  128/  569]\n",
      "loss: 0.578040  [  256/  569]\n",
      "loss: 0.460214  [  384/  569]\n",
      "loss: 0.544502  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2320e-10.\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.513162  [    0/  569]\n",
      "loss: 0.515134  [  128/  569]\n",
      "loss: 0.480730  [  256/  569]\n",
      "loss: 0.482025  [  384/  569]\n",
      "loss: 0.522379  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1088e-10.\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.511354  [    0/  569]\n",
      "loss: 0.488346  [  128/  569]\n",
      "loss: 0.486265  [  256/  569]\n",
      "loss: 0.502645  [  384/  569]\n",
      "loss: 0.533291  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.9794e-11.\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.485469  [    0/  569]\n",
      "loss: 0.532985  [  128/  569]\n",
      "loss: 0.471415  [  256/  569]\n",
      "loss: 0.486235  [  384/  569]\n",
      "loss: 0.470351  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.9814e-11.\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.447073  [    0/  569]\n",
      "loss: 0.495767  [  128/  569]\n",
      "loss: 0.490030  [  256/  569]\n",
      "loss: 0.473708  [  384/  569]\n",
      "loss: 0.561449  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.0833e-11.\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.543151  [    0/  569]\n",
      "loss: 0.484242  [  128/  569]\n",
      "loss: 0.487063  [  256/  569]\n",
      "loss: 0.484825  [  384/  569]\n",
      "loss: 0.505491  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.2750e-11.\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.538383  [    0/  569]\n",
      "loss: 0.476761  [  128/  569]\n",
      "loss: 0.496322  [  256/  569]\n",
      "loss: 0.514059  [  384/  569]\n",
      "loss: 0.449303  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.5475e-11.\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.490045  [    0/  569]\n",
      "loss: 0.575163  [  128/  569]\n",
      "loss: 0.418393  [  256/  569]\n",
      "loss: 0.540772  [  384/  569]\n",
      "loss: 0.477597  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.8927e-11.\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.499606  [    0/  569]\n",
      "loss: 0.524744  [  128/  569]\n",
      "loss: 0.480191  [  256/  569]\n",
      "loss: 0.514829  [  384/  569]\n",
      "loss: 0.500872  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.3035e-11.\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.505556  [    0/  569]\n",
      "loss: 0.449363  [  128/  569]\n",
      "loss: 0.469280  [  256/  569]\n",
      "loss: 0.487477  [  384/  569]\n",
      "loss: 0.463244  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.7731e-11.\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.506696  [    0/  569]\n",
      "loss: 0.479694  [  128/  569]\n",
      "loss: 0.509021  [  256/  569]\n",
      "loss: 0.480848  [  384/  569]\n",
      "loss: 0.531690  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.2958e-11.\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.458063  [    0/  569]\n",
      "loss: 0.460448  [  128/  569]\n",
      "loss: 0.526255  [  256/  569]\n",
      "loss: 0.479843  [  384/  569]\n",
      "loss: 0.509213  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.8662e-11.\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.456535  [    0/  569]\n",
      "loss: 0.529787  [  128/  569]\n",
      "loss: 0.456452  [  256/  569]\n",
      "loss: 0.490955  [  384/  569]\n",
      "loss: 0.485569  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4796e-11.\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.510356  [    0/  569]\n",
      "loss: 0.481615  [  128/  569]\n",
      "loss: 0.449290  [  256/  569]\n",
      "loss: 0.474686  [  384/  569]\n",
      "loss: 0.474079  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.1316e-11.\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.511512  [    0/  569]\n",
      "loss: 0.472081  [  128/  569]\n",
      "loss: 0.529362  [  256/  569]\n",
      "loss: 0.506065  [  384/  569]\n",
      "loss: 0.505586  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8185e-11.\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.493878  [    0/  569]\n",
      "loss: 0.495963  [  128/  569]\n",
      "loss: 0.484602  [  256/  569]\n",
      "loss: 0.504929  [  384/  569]\n",
      "loss: 0.509379  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5366e-11.\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.499052  [    0/  569]\n",
      "loss: 0.460582  [  128/  569]\n",
      "loss: 0.514420  [  256/  569]\n",
      "loss: 0.509387  [  384/  569]\n",
      "loss: 0.470782  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2830e-11.\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.411848  [    0/  569]\n",
      "loss: 0.458933  [  128/  569]\n",
      "loss: 0.497176  [  256/  569]\n",
      "loss: 0.512172  [  384/  569]\n",
      "loss: 0.489520  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0547e-11.\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.496564  [    0/  569]\n",
      "loss: 0.505822  [  128/  569]\n",
      "loss: 0.526916  [  256/  569]\n",
      "loss: 0.464210  [  384/  569]\n",
      "loss: 0.436086  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8492e-11.\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.514658  [    0/  569]\n",
      "loss: 0.533627  [  128/  569]\n",
      "loss: 0.538884  [  256/  569]\n",
      "loss: 0.487203  [  384/  569]\n",
      "loss: 0.502700  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6643e-11.\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.538111  [    0/  569]\n",
      "loss: 0.519688  [  128/  569]\n",
      "loss: 0.509310  [  256/  569]\n",
      "loss: 0.472107  [  384/  569]\n",
      "loss: 0.463739  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4979e-11.\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.479473  [    0/  569]\n",
      "loss: 0.474131  [  128/  569]\n",
      "loss: 0.508327  [  256/  569]\n",
      "loss: 0.552054  [  384/  569]\n",
      "loss: 0.442546  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3481e-11.\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.481991  [    0/  569]\n",
      "loss: 0.444396  [  128/  569]\n",
      "loss: 0.494225  [  256/  569]\n",
      "loss: 0.531577  [  384/  569]\n",
      "loss: 0.466997  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2133e-11.\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.429662  [    0/  569]\n",
      "loss: 0.513560  [  128/  569]\n",
      "loss: 0.528424  [  256/  569]\n",
      "loss: 0.468508  [  384/  569]\n",
      "loss: 0.511055  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0919e-11.\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.479742  [    0/  569]\n",
      "loss: 0.478597  [  128/  569]\n",
      "loss: 0.485429  [  256/  569]\n",
      "loss: 0.528943  [  384/  569]\n",
      "loss: 0.468656  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.8274e-12.\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.526694  [    0/  569]\n",
      "loss: 0.481983  [  128/  569]\n",
      "loss: 0.483722  [  256/  569]\n",
      "loss: 0.507318  [  384/  569]\n",
      "loss: 0.543276  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.8447e-12.\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.496541  [    0/  569]\n",
      "loss: 0.513076  [  128/  569]\n",
      "loss: 0.485056  [  256/  569]\n",
      "loss: 0.514208  [  384/  569]\n",
      "loss: 0.474402  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.9602e-12.\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.493451  [    0/  569]\n",
      "loss: 0.504356  [  128/  569]\n",
      "loss: 0.536507  [  256/  569]\n",
      "loss: 0.486704  [  384/  569]\n",
      "loss: 0.506088  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.1642e-12.\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.487993  [    0/  569]\n",
      "loss: 0.527177  [  128/  569]\n",
      "loss: 0.520916  [  256/  569]\n",
      "loss: 0.514257  [  384/  569]\n",
      "loss: 0.545856  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.4478e-12.\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.527566  [    0/  569]\n",
      "loss: 0.472811  [  128/  569]\n",
      "loss: 0.509153  [  256/  569]\n",
      "loss: 0.482579  [  384/  569]\n",
      "loss: 0.531072  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.8030e-12.\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.549161  [    0/  569]\n",
      "loss: 0.507189  [  128/  569]\n",
      "loss: 0.477055  [  256/  569]\n",
      "loss: 0.486804  [  384/  569]\n",
      "loss: 0.471477  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.2227e-12.\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.516737  [    0/  569]\n",
      "loss: 0.485278  [  128/  569]\n",
      "loss: 0.509176  [  256/  569]\n",
      "loss: 0.498346  [  384/  569]\n",
      "loss: 0.507107  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.7004e-12.\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.506486  [    0/  569]\n",
      "loss: 0.483994  [  128/  569]\n",
      "loss: 0.450271  [  256/  569]\n",
      "loss: 0.559596  [  384/  569]\n",
      "loss: 0.480726  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.2304e-12.\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.495209  [    0/  569]\n",
      "loss: 0.516915  [  128/  569]\n",
      "loss: 0.504104  [  256/  569]\n",
      "loss: 0.442492  [  384/  569]\n",
      "loss: 0.482870  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.8073e-12.\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.519545  [    0/  569]\n",
      "loss: 0.432013  [  128/  569]\n",
      "loss: 0.507591  [  256/  569]\n",
      "loss: 0.537773  [  384/  569]\n",
      "loss: 0.518665  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4266e-12.\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.486749  [    0/  569]\n",
      "loss: 0.475431  [  128/  569]\n",
      "loss: 0.485225  [  256/  569]\n",
      "loss: 0.466017  [  384/  569]\n",
      "loss: 0.469496  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.0839e-12.\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.516593  [    0/  569]\n",
      "loss: 0.424206  [  128/  569]\n",
      "loss: 0.542037  [  256/  569]\n",
      "loss: 0.486681  [  384/  569]\n",
      "loss: 0.482605  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.7756e-12.\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.477814  [    0/  569]\n",
      "loss: 0.481138  [  128/  569]\n",
      "loss: 0.548267  [  256/  569]\n",
      "loss: 0.494481  [  384/  569]\n",
      "loss: 0.476634  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4980e-12.\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.515807  [    0/  569]\n",
      "loss: 0.492659  [  128/  569]\n",
      "loss: 0.493146  [  256/  569]\n",
      "loss: 0.514242  [  384/  569]\n",
      "loss: 0.452786  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2482e-12.\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.510184  [    0/  569]\n",
      "loss: 0.460342  [  128/  569]\n",
      "loss: 0.536569  [  256/  569]\n",
      "loss: 0.468361  [  384/  569]\n",
      "loss: 0.511432  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0234e-12.\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.476246  [    0/  569]\n",
      "loss: 0.512193  [  128/  569]\n",
      "loss: 0.486200  [  256/  569]\n",
      "loss: 0.502341  [  384/  569]\n",
      "loss: 0.485014  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8210e-12.\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.475384  [    0/  569]\n",
      "loss: 0.533226  [  128/  569]\n",
      "loss: 0.473717  [  256/  569]\n",
      "loss: 0.489748  [  384/  569]\n",
      "loss: 0.496957  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6389e-12.\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.483950  [    0/  569]\n",
      "loss: 0.481027  [  128/  569]\n",
      "loss: 0.493345  [  256/  569]\n",
      "loss: 0.507283  [  384/  569]\n",
      "loss: 0.477875  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4750e-12.\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.489358  [    0/  569]\n",
      "loss: 0.504273  [  128/  569]\n",
      "loss: 0.506836  [  256/  569]\n",
      "loss: 0.531276  [  384/  569]\n",
      "loss: 0.476520  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3275e-12.\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.492108  [    0/  569]\n",
      "loss: 0.493832  [  128/  569]\n",
      "loss: 0.486899  [  256/  569]\n",
      "loss: 0.495923  [  384/  569]\n",
      "loss: 0.525697  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1948e-12.\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.510234  [    0/  569]\n",
      "loss: 0.416592  [  128/  569]\n",
      "loss: 0.505655  [  256/  569]\n",
      "loss: 0.479516  [  384/  569]\n",
      "loss: 0.522711  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0753e-12.\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.481098  [    0/  569]\n",
      "loss: 0.505302  [  128/  569]\n",
      "loss: 0.550514  [  256/  569]\n",
      "loss: 0.475558  [  384/  569]\n",
      "loss: 0.507342  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.6777e-13.\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.496495  [    0/  569]\n",
      "loss: 0.517477  [  128/  569]\n",
      "loss: 0.475413  [  256/  569]\n",
      "loss: 0.556287  [  384/  569]\n",
      "loss: 0.432454  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.7100e-13.\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.513364  [    0/  569]\n",
      "loss: 0.502660  [  128/  569]\n",
      "loss: 0.504208  [  256/  569]\n",
      "loss: 0.454668  [  384/  569]\n",
      "loss: 0.485948  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.8390e-13.\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.490983  [    0/  569]\n",
      "loss: 0.515849  [  128/  569]\n",
      "loss: 0.512969  [  256/  569]\n",
      "loss: 0.451226  [  384/  569]\n",
      "loss: 0.512467  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.0551e-13.\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.423121  [    0/  569]\n",
      "loss: 0.515844  [  128/  569]\n",
      "loss: 0.433808  [  256/  569]\n",
      "loss: 0.508450  [  384/  569]\n",
      "loss: 0.515984  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.3496e-13.\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.520871  [    0/  569]\n",
      "loss: 0.464397  [  128/  569]\n",
      "loss: 0.522551  [  256/  569]\n",
      "loss: 0.484348  [  384/  569]\n",
      "loss: 0.475428  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.7146e-13.\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.497463  [    0/  569]\n",
      "loss: 0.469171  [  128/  569]\n",
      "loss: 0.482945  [  256/  569]\n",
      "loss: 0.414720  [  384/  569]\n",
      "loss: 0.499240  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.1432e-13.\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.475822  [    0/  569]\n",
      "loss: 0.477279  [  128/  569]\n",
      "loss: 0.483943  [  256/  569]\n",
      "loss: 0.523148  [  384/  569]\n",
      "loss: 0.484268  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.6288e-13.\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.484924  [    0/  569]\n",
      "loss: 0.448915  [  128/  569]\n",
      "loss: 0.470297  [  256/  569]\n",
      "loss: 0.538823  [  384/  569]\n",
      "loss: 0.568283  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.1660e-13.\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.474859  [    0/  569]\n",
      "loss: 0.471508  [  128/  569]\n",
      "loss: 0.529903  [  256/  569]\n",
      "loss: 0.460218  [  384/  569]\n",
      "loss: 0.489636  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.7494e-13.\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.519640  [    0/  569]\n",
      "loss: 0.437860  [  128/  569]\n",
      "loss: 0.464281  [  256/  569]\n",
      "loss: 0.486345  [  384/  569]\n",
      "loss: 0.484797  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.3744e-13.\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.496705  [    0/  569]\n",
      "loss: 0.488451  [  128/  569]\n",
      "loss: 0.541316  [  256/  569]\n",
      "loss: 0.471100  [  384/  569]\n",
      "loss: 0.460729  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.0370e-13.\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.546963  [    0/  569]\n",
      "loss: 0.478786  [  128/  569]\n",
      "loss: 0.487307  [  256/  569]\n",
      "loss: 0.494778  [  384/  569]\n",
      "loss: 0.498905  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.7333e-13.\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.489078  [    0/  569]\n",
      "loss: 0.495379  [  128/  569]\n",
      "loss: 0.560097  [  256/  569]\n",
      "loss: 0.474875  [  384/  569]\n",
      "loss: 0.474399  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4600e-13.\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.478854  [    0/  569]\n",
      "loss: 0.467685  [  128/  569]\n",
      "loss: 0.529203  [  256/  569]\n",
      "loss: 0.507039  [  384/  569]\n",
      "loss: 0.447886  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2140e-13.\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.515842  [    0/  569]\n",
      "loss: 0.488991  [  128/  569]\n",
      "loss: 0.513367  [  256/  569]\n",
      "loss: 0.464933  [  384/  569]\n",
      "loss: 0.478906  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9926e-13.\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.545683  [    0/  569]\n",
      "loss: 0.477174  [  128/  569]\n",
      "loss: 0.542842  [  256/  569]\n",
      "loss: 0.489685  [  384/  569]\n",
      "loss: 0.479401  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7933e-13.\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.480586  [    0/  569]\n",
      "loss: 0.455127  [  128/  569]\n",
      "loss: 0.471468  [  256/  569]\n",
      "loss: 0.505401  [  384/  569]\n",
      "loss: 0.469464  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6140e-13.\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.472430  [    0/  569]\n",
      "loss: 0.468506  [  128/  569]\n",
      "loss: 0.553712  [  256/  569]\n",
      "loss: 0.469940  [  384/  569]\n",
      "loss: 0.501290  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4526e-13.\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.519986  [    0/  569]\n",
      "loss: 0.488130  [  128/  569]\n",
      "loss: 0.501717  [  256/  569]\n",
      "loss: 0.462330  [  384/  569]\n",
      "loss: 0.518334  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3073e-13.\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.502742  [    0/  569]\n",
      "loss: 0.462359  [  128/  569]\n",
      "loss: 0.511208  [  256/  569]\n",
      "loss: 0.443384  [  384/  569]\n",
      "loss: 0.530151  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1766e-13.\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.483713  [    0/  569]\n",
      "loss: 0.530399  [  128/  569]\n",
      "loss: 0.509256  [  256/  569]\n",
      "loss: 0.437210  [  384/  569]\n",
      "loss: 0.464163  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0589e-13.\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.507130  [    0/  569]\n",
      "loss: 0.489072  [  128/  569]\n",
      "loss: 0.550457  [  256/  569]\n",
      "loss: 0.463798  [  384/  569]\n",
      "loss: 0.501523  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.5304e-14.\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.483702  [    0/  569]\n",
      "loss: 0.491764  [  128/  569]\n",
      "loss: 0.502225  [  256/  569]\n",
      "loss: 0.478085  [  384/  569]\n",
      "loss: 0.546066  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.5773e-14.\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.497328  [    0/  569]\n",
      "loss: 0.520694  [  128/  569]\n",
      "loss: 0.533815  [  256/  569]\n",
      "loss: 0.438454  [  384/  569]\n",
      "loss: 0.540584  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.7196e-14.\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.528884  [    0/  569]\n",
      "loss: 0.501856  [  128/  569]\n",
      "loss: 0.498282  [  256/  569]\n",
      "loss: 0.488994  [  384/  569]\n",
      "loss: 0.460997  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.9476e-14.\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.486861  [    0/  569]\n",
      "loss: 0.510341  [  128/  569]\n",
      "loss: 0.537358  [  256/  569]\n",
      "loss: 0.509403  [  384/  569]\n",
      "loss: 0.491781  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.2529e-14.\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.513625  [    0/  569]\n",
      "loss: 0.489327  [  128/  569]\n",
      "loss: 0.518130  [  256/  569]\n",
      "loss: 0.523842  [  384/  569]\n",
      "loss: 0.504548  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.6276e-14.\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.469272  [    0/  569]\n",
      "loss: 0.457780  [  128/  569]\n",
      "loss: 0.484968  [  256/  569]\n",
      "loss: 0.529244  [  384/  569]\n",
      "loss: 0.510607  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.0648e-14.\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.470200  [    0/  569]\n",
      "loss: 0.495355  [  128/  569]\n",
      "loss: 0.442939  [  256/  569]\n",
      "loss: 0.499145  [  384/  569]\n",
      "loss: 0.523384  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.5583e-14.\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.493482  [    0/  569]\n",
      "loss: 0.522618  [  128/  569]\n",
      "loss: 0.534253  [  256/  569]\n",
      "loss: 0.482150  [  384/  569]\n",
      "loss: 0.545032  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.1025e-14.\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.445089  [    0/  569]\n",
      "loss: 0.536742  [  128/  569]\n",
      "loss: 0.507620  [  256/  569]\n",
      "loss: 0.491488  [  384/  569]\n",
      "loss: 0.486977  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.6923e-14.\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.519325  [    0/  569]\n",
      "loss: 0.475702  [  128/  569]\n",
      "loss: 0.520491  [  256/  569]\n",
      "loss: 0.476558  [  384/  569]\n",
      "loss: 0.488414  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.3230e-14.\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.540500  [    0/  569]\n",
      "loss: 0.492759  [  128/  569]\n",
      "loss: 0.510435  [  256/  569]\n",
      "loss: 0.478258  [  384/  569]\n",
      "loss: 0.474521  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9907e-14.\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.509880  [    0/  569]\n",
      "loss: 0.544335  [  128/  569]\n",
      "loss: 0.563358  [  256/  569]\n",
      "loss: 0.488326  [  384/  569]\n",
      "loss: 0.436253  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6917e-14.\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.491835  [    0/  569]\n",
      "loss: 0.489443  [  128/  569]\n",
      "loss: 0.489872  [  256/  569]\n",
      "loss: 0.495393  [  384/  569]\n",
      "loss: 0.467836  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4225e-14.\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.513586  [    0/  569]\n",
      "loss: 0.539419  [  128/  569]\n",
      "loss: 0.469350  [  256/  569]\n",
      "loss: 0.489436  [  384/  569]\n",
      "loss: 0.504385  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1802e-14.\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.440259  [    0/  569]\n",
      "loss: 0.479413  [  128/  569]\n",
      "loss: 0.539830  [  256/  569]\n",
      "loss: 0.524785  [  384/  569]\n",
      "loss: 0.536576  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9622e-14.\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.482681  [    0/  569]\n",
      "loss: 0.484938  [  128/  569]\n",
      "loss: 0.494676  [  256/  569]\n",
      "loss: 0.537836  [  384/  569]\n",
      "loss: 0.489865  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7660e-14.\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.509825  [    0/  569]\n",
      "loss: 0.506449  [  128/  569]\n",
      "loss: 0.455716  [  256/  569]\n",
      "loss: 0.519396  [  384/  569]\n",
      "loss: 0.515445  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5894e-14.\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.490520  [    0/  569]\n",
      "loss: 0.519709  [  128/  569]\n",
      "loss: 0.497482  [  256/  569]\n",
      "loss: 0.520399  [  384/  569]\n",
      "loss: 0.501822  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4305e-14.\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.481567  [    0/  569]\n",
      "loss: 0.520836  [  128/  569]\n",
      "loss: 0.529786  [  256/  569]\n",
      "loss: 0.457924  [  384/  569]\n",
      "loss: 0.488286  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2874e-14.\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.495586  [    0/  569]\n",
      "loss: 0.502431  [  128/  569]\n",
      "loss: 0.474744  [  256/  569]\n",
      "loss: 0.536066  [  384/  569]\n",
      "loss: 0.474035  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1587e-14.\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.531325  [    0/  569]\n",
      "loss: 0.524396  [  128/  569]\n",
      "loss: 0.505210  [  256/  569]\n",
      "loss: 0.482284  [  384/  569]\n",
      "loss: 0.485879  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0428e-14.\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.489633  [    0/  569]\n",
      "loss: 0.484189  [  128/  569]\n",
      "loss: 0.553188  [  256/  569]\n",
      "loss: 0.530529  [  384/  569]\n",
      "loss: 0.475183  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.3852e-15.\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.510439  [    0/  569]\n",
      "loss: 0.518625  [  128/  569]\n",
      "loss: 0.461153  [  256/  569]\n",
      "loss: 0.479941  [  384/  569]\n",
      "loss: 0.493309  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.4467e-15.\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.483150  [    0/  569]\n",
      "loss: 0.509330  [  128/  569]\n",
      "loss: 0.506012  [  256/  569]\n",
      "loss: 0.481453  [  384/  569]\n",
      "loss: 0.466328  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.6020e-15.\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.490898  [    0/  569]\n",
      "loss: 0.530187  [  128/  569]\n",
      "loss: 0.500542  [  256/  569]\n",
      "loss: 0.473805  [  384/  569]\n",
      "loss: 0.474217  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.8418e-15.\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.513927  [    0/  569]\n",
      "loss: 0.479149  [  128/  569]\n",
      "loss: 0.499482  [  256/  569]\n",
      "loss: 0.522258  [  384/  569]\n",
      "loss: 0.499658  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.1576e-15.\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.455636  [    0/  569]\n",
      "loss: 0.522759  [  128/  569]\n",
      "loss: 0.456728  [  256/  569]\n",
      "loss: 0.475813  [  384/  569]\n",
      "loss: 0.488459  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.5419e-15.\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.467735  [    0/  569]\n",
      "loss: 0.541018  [  128/  569]\n",
      "loss: 0.523966  [  256/  569]\n",
      "loss: 0.471136  [  384/  569]\n",
      "loss: 0.471691  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.9877e-15.\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.502360  [    0/  569]\n",
      "loss: 0.512219  [  128/  569]\n",
      "loss: 0.510711  [  256/  569]\n",
      "loss: 0.501487  [  384/  569]\n",
      "loss: 0.484822  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.4889e-15.\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.520097  [    0/  569]\n",
      "loss: 0.519144  [  128/  569]\n",
      "loss: 0.553354  [  256/  569]\n",
      "loss: 0.507914  [  384/  569]\n",
      "loss: 0.475875  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.0400e-15.\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.489004  [    0/  569]\n",
      "loss: 0.490694  [  128/  569]\n",
      "loss: 0.474371  [  256/  569]\n",
      "loss: 0.477717  [  384/  569]\n",
      "loss: 0.483979  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.6360e-15.\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.486673  [    0/  569]\n",
      "loss: 0.502183  [  128/  569]\n",
      "loss: 0.496218  [  256/  569]\n",
      "loss: 0.545977  [  384/  569]\n",
      "loss: 0.549498  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.2724e-15.\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.521033  [    0/  569]\n",
      "loss: 0.482810  [  128/  569]\n",
      "loss: 0.472921  [  256/  569]\n",
      "loss: 0.477280  [  384/  569]\n",
      "loss: 0.532671  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9452e-15.\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.503740  [    0/  569]\n",
      "loss: 0.483076  [  128/  569]\n",
      "loss: 0.480405  [  256/  569]\n",
      "loss: 0.496985  [  384/  569]\n",
      "loss: 0.471800  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6507e-15.\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.600126  [    0/  569]\n",
      "loss: 0.522360  [  128/  569]\n",
      "loss: 0.481031  [  256/  569]\n",
      "loss: 0.525088  [  384/  569]\n",
      "loss: 0.470633  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3856e-15.\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.506018  [    0/  569]\n",
      "loss: 0.504483  [  128/  569]\n",
      "loss: 0.486920  [  256/  569]\n",
      "loss: 0.463487  [  384/  569]\n",
      "loss: 0.491399  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1470e-15.\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.457019  [    0/  569]\n",
      "loss: 0.532388  [  128/  569]\n",
      "loss: 0.513069  [  256/  569]\n",
      "loss: 0.463852  [  384/  569]\n",
      "loss: 0.557767  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9323e-15.\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.483757  [    0/  569]\n",
      "loss: 0.485691  [  128/  569]\n",
      "loss: 0.507015  [  256/  569]\n",
      "loss: 0.520890  [  384/  569]\n",
      "loss: 0.480118  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7391e-15.\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.458974  [    0/  569]\n",
      "loss: 0.515661  [  128/  569]\n",
      "loss: 0.569580  [  256/  569]\n",
      "loss: 0.443824  [  384/  569]\n",
      "loss: 0.456869  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5652e-15.\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.486182  [    0/  569]\n",
      "loss: 0.501057  [  128/  569]\n",
      "loss: 0.503273  [  256/  569]\n",
      "loss: 0.505103  [  384/  569]\n",
      "loss: 0.498317  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4087e-15.\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.504506  [    0/  569]\n",
      "loss: 0.462869  [  128/  569]\n",
      "loss: 0.468718  [  256/  569]\n",
      "loss: 0.497548  [  384/  569]\n",
      "loss: 0.480456  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2678e-15.\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.453807  [    0/  569]\n",
      "loss: 0.501078  [  128/  569]\n",
      "loss: 0.509907  [  256/  569]\n",
      "loss: 0.480989  [  384/  569]\n",
      "loss: 0.499514  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1410e-15.\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.479818  [    0/  569]\n",
      "loss: 0.476037  [  128/  569]\n",
      "loss: 0.556826  [  256/  569]\n",
      "loss: 0.510133  [  384/  569]\n",
      "loss: 0.464666  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0269e-15.\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.514552  [    0/  569]\n",
      "loss: 0.488678  [  128/  569]\n",
      "loss: 0.469121  [  256/  569]\n",
      "loss: 0.502785  [  384/  569]\n",
      "loss: 0.516406  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.2423e-16.\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.510916  [    0/  569]\n",
      "loss: 0.463737  [  128/  569]\n",
      "loss: 0.506757  [  256/  569]\n",
      "loss: 0.516167  [  384/  569]\n",
      "loss: 0.521547  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.3181e-16.\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.516021  [    0/  569]\n",
      "loss: 0.530444  [  128/  569]\n",
      "loss: 0.474632  [  256/  569]\n",
      "loss: 0.468058  [  384/  569]\n",
      "loss: 0.450801  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.4863e-16.\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.482371  [    0/  569]\n",
      "loss: 0.507390  [  128/  569]\n",
      "loss: 0.462696  [  256/  569]\n",
      "loss: 0.498706  [  384/  569]\n",
      "loss: 0.454909  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.7376e-16.\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.499996  [    0/  569]\n",
      "loss: 0.504848  [  128/  569]\n",
      "loss: 0.450094  [  256/  569]\n",
      "loss: 0.515264  [  384/  569]\n",
      "loss: 0.537196  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.0639e-16.\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.516026  [    0/  569]\n",
      "loss: 0.476970  [  128/  569]\n",
      "loss: 0.474429  [  256/  569]\n",
      "loss: 0.470118  [  384/  569]\n",
      "loss: 0.518126  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.4575e-16.\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.482754  [    0/  569]\n",
      "loss: 0.448396  [  128/  569]\n",
      "loss: 0.465943  [  256/  569]\n",
      "loss: 0.530750  [  384/  569]\n",
      "loss: 0.487519  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.9117e-16.\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.465136  [    0/  569]\n",
      "loss: 0.495543  [  128/  569]\n",
      "loss: 0.489530  [  256/  569]\n",
      "loss: 0.506375  [  384/  569]\n",
      "loss: 0.568051  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.4206e-16.\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.548230  [    0/  569]\n",
      "loss: 0.491807  [  128/  569]\n",
      "loss: 0.524291  [  256/  569]\n",
      "loss: 0.462492  [  384/  569]\n",
      "loss: 0.460728  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.9785e-16.\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.491792  [    0/  569]\n",
      "loss: 0.474871  [  128/  569]\n",
      "loss: 0.527193  [  256/  569]\n",
      "loss: 0.459678  [  384/  569]\n",
      "loss: 0.543148  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.5807e-16.\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.539679  [    0/  569]\n",
      "loss: 0.478389  [  128/  569]\n",
      "loss: 0.493574  [  256/  569]\n",
      "loss: 0.476227  [  384/  569]\n",
      "loss: 0.522929  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.2226e-16.\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.509601  [    0/  569]\n",
      "loss: 0.485639  [  128/  569]\n",
      "loss: 0.524336  [  256/  569]\n",
      "loss: 0.459293  [  384/  569]\n",
      "loss: 0.545455  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9003e-16.\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.434112  [    0/  569]\n",
      "loss: 0.484157  [  128/  569]\n",
      "loss: 0.453058  [  256/  569]\n",
      "loss: 0.550416  [  384/  569]\n",
      "loss: 0.544805  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6103e-16.\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.520588  [    0/  569]\n",
      "loss: 0.507301  [  128/  569]\n",
      "loss: 0.511381  [  256/  569]\n",
      "loss: 0.542096  [  384/  569]\n",
      "loss: 0.472862  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3493e-16.\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.497129  [    0/  569]\n",
      "loss: 0.469415  [  128/  569]\n",
      "loss: 0.458257  [  256/  569]\n",
      "loss: 0.484434  [  384/  569]\n",
      "loss: 0.495524  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1143e-16.\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.482186  [    0/  569]\n",
      "loss: 0.509043  [  128/  569]\n",
      "loss: 0.523045  [  256/  569]\n",
      "loss: 0.559550  [  384/  569]\n",
      "loss: 0.509259  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9029e-16.\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.469117  [    0/  569]\n",
      "loss: 0.486657  [  128/  569]\n",
      "loss: 0.479239  [  256/  569]\n",
      "loss: 0.516465  [  384/  569]\n",
      "loss: 0.496224  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7126e-16.\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.512808  [    0/  569]\n",
      "loss: 0.480034  [  128/  569]\n",
      "loss: 0.472088  [  256/  569]\n",
      "loss: 0.503326  [  384/  569]\n",
      "loss: 0.475797  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5414e-16.\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.508214  [    0/  569]\n",
      "loss: 0.466750  [  128/  569]\n",
      "loss: 0.482729  [  256/  569]\n",
      "loss: 0.488900  [  384/  569]\n",
      "loss: 0.526456  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3872e-16.\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.509774  [    0/  569]\n",
      "loss: 0.462819  [  128/  569]\n",
      "loss: 0.508449  [  256/  569]\n",
      "loss: 0.507928  [  384/  569]\n",
      "loss: 0.522297  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2485e-16.\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.494532  [    0/  569]\n",
      "loss: 0.551882  [  128/  569]\n",
      "loss: 0.523452  [  256/  569]\n",
      "loss: 0.523142  [  384/  569]\n",
      "loss: 0.451584  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1236e-16.\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.528213  [    0/  569]\n",
      "loss: 0.466942  [  128/  569]\n",
      "loss: 0.474780  [  256/  569]\n",
      "loss: 0.437387  [  384/  569]\n",
      "loss: 0.507821  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0113e-16.\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.511365  [    0/  569]\n",
      "loss: 0.549535  [  128/  569]\n",
      "loss: 0.426488  [  256/  569]\n",
      "loss: 0.464999  [  384/  569]\n",
      "loss: 0.515472  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.1015e-17.\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.531485  [    0/  569]\n",
      "loss: 0.508180  [  128/  569]\n",
      "loss: 0.454371  [  256/  569]\n",
      "loss: 0.530928  [  384/  569]\n",
      "loss: 0.543092  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.1914e-17.\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.471480  [    0/  569]\n",
      "loss: 0.520096  [  128/  569]\n",
      "loss: 0.557987  [  256/  569]\n",
      "loss: 0.439660  [  384/  569]\n",
      "loss: 0.526783  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.3723e-17.\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.461676  [    0/  569]\n",
      "loss: 0.533128  [  128/  569]\n",
      "loss: 0.498543  [  256/  569]\n",
      "loss: 0.517642  [  384/  569]\n",
      "loss: 0.509507  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.6350e-17.\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.525730  [    0/  569]\n",
      "loss: 0.519726  [  128/  569]\n",
      "loss: 0.522859  [  256/  569]\n",
      "loss: 0.517443  [  384/  569]\n",
      "loss: 0.470093  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.9715e-17.\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.537121  [    0/  569]\n",
      "loss: 0.447303  [  128/  569]\n",
      "loss: 0.503505  [  256/  569]\n",
      "loss: 0.500096  [  384/  569]\n",
      "loss: 0.473762  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.3744e-17.\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.478038  [    0/  569]\n",
      "loss: 0.496892  [  128/  569]\n",
      "loss: 0.494343  [  256/  569]\n",
      "loss: 0.526662  [  384/  569]\n",
      "loss: 0.557844  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.8369e-17.\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.483063  [    0/  569]\n",
      "loss: 0.510494  [  128/  569]\n",
      "loss: 0.496714  [  256/  569]\n",
      "loss: 0.464018  [  384/  569]\n",
      "loss: 0.460605  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.3532e-17.\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.471851  [    0/  569]\n",
      "loss: 0.509698  [  128/  569]\n",
      "loss: 0.425011  [  256/  569]\n",
      "loss: 0.546137  [  384/  569]\n",
      "loss: 0.491503  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.9179e-17.\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.496240  [    0/  569]\n",
      "loss: 0.482695  [  128/  569]\n",
      "loss: 0.493180  [  256/  569]\n",
      "loss: 0.484691  [  384/  569]\n",
      "loss: 0.480403  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.5261e-17.\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.516427  [    0/  569]\n",
      "loss: 0.506900  [  128/  569]\n",
      "loss: 0.480097  [  256/  569]\n",
      "loss: 0.500388  [  384/  569]\n",
      "loss: 0.499916  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.1735e-17.\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.486975  [    0/  569]\n",
      "loss: 0.467160  [  128/  569]\n",
      "loss: 0.456166  [  256/  569]\n",
      "loss: 0.510234  [  384/  569]\n",
      "loss: 0.464322  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8562e-17.\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.511870  [    0/  569]\n",
      "loss: 0.511758  [  128/  569]\n",
      "loss: 0.490486  [  256/  569]\n",
      "loss: 0.466677  [  384/  569]\n",
      "loss: 0.451525  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5705e-17.\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.499941  [    0/  569]\n",
      "loss: 0.458607  [  128/  569]\n",
      "loss: 0.523347  [  256/  569]\n",
      "loss: 0.472055  [  384/  569]\n",
      "loss: 0.536219  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3135e-17.\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.523030  [    0/  569]\n",
      "loss: 0.489056  [  128/  569]\n",
      "loss: 0.503772  [  256/  569]\n",
      "loss: 0.459396  [  384/  569]\n",
      "loss: 0.550318  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0821e-17.\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.501000  [    0/  569]\n",
      "loss: 0.480926  [  128/  569]\n",
      "loss: 0.472841  [  256/  569]\n",
      "loss: 0.515969  [  384/  569]\n",
      "loss: 0.472936  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8739e-17.\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.539400  [    0/  569]\n",
      "loss: 0.494848  [  128/  569]\n",
      "loss: 0.496390  [  256/  569]\n",
      "loss: 0.525963  [  384/  569]\n",
      "loss: 0.435757  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6865e-17.\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.508693  [    0/  569]\n",
      "loss: 0.512197  [  128/  569]\n",
      "loss: 0.492739  [  256/  569]\n",
      "loss: 0.494044  [  384/  569]\n",
      "loss: 0.506766  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5179e-17.\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.421827  [    0/  569]\n",
      "loss: 0.490100  [  128/  569]\n",
      "loss: 0.501156  [  256/  569]\n",
      "loss: 0.477981  [  384/  569]\n",
      "loss: 0.483374  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3661e-17.\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.473092  [    0/  569]\n",
      "loss: 0.500241  [  128/  569]\n",
      "loss: 0.499113  [  256/  569]\n",
      "loss: 0.474317  [  384/  569]\n",
      "loss: 0.555027  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2295e-17.\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.517249  [    0/  569]\n",
      "loss: 0.442709  [  128/  569]\n",
      "loss: 0.509874  [  256/  569]\n",
      "loss: 0.489647  [  384/  569]\n",
      "loss: 0.463553  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1065e-17.\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.498554  [    0/  569]\n",
      "loss: 0.533386  [  128/  569]\n",
      "loss: 0.464527  [  256/  569]\n",
      "loss: 0.531436  [  384/  569]\n",
      "loss: 0.495374  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.9588e-18.\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.478754  [    0/  569]\n",
      "loss: 0.538737  [  128/  569]\n",
      "loss: 0.478584  [  256/  569]\n",
      "loss: 0.531338  [  384/  569]\n",
      "loss: 0.455311  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.9629e-18.\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.464392  [    0/  569]\n",
      "loss: 0.500353  [  128/  569]\n",
      "loss: 0.509158  [  256/  569]\n",
      "loss: 0.550879  [  384/  569]\n",
      "loss: 0.472024  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.0666e-18.\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.509303  [    0/  569]\n",
      "loss: 0.459801  [  128/  569]\n",
      "loss: 0.450346  [  256/  569]\n",
      "loss: 0.499852  [  384/  569]\n",
      "loss: 0.530546  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.2600e-18.\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.513270  [    0/  569]\n",
      "loss: 0.479116  [  128/  569]\n",
      "loss: 0.520024  [  256/  569]\n",
      "loss: 0.483881  [  384/  569]\n",
      "loss: 0.496350  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.5340e-18.\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.489598  [    0/  569]\n",
      "loss: 0.447427  [  128/  569]\n",
      "loss: 0.460165  [  256/  569]\n",
      "loss: 0.523167  [  384/  569]\n",
      "loss: 0.489877  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.8806e-18.\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.467007  [    0/  569]\n",
      "loss: 0.484374  [  128/  569]\n",
      "loss: 0.492519  [  256/  569]\n",
      "loss: 0.518405  [  384/  569]\n",
      "loss: 0.485264  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.2925e-18.\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.515473  [    0/  569]\n",
      "loss: 0.475847  [  128/  569]\n",
      "loss: 0.465492  [  256/  569]\n",
      "loss: 0.507441  [  384/  569]\n",
      "loss: 0.490423  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.7633e-18.\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.526662  [    0/  569]\n",
      "loss: 0.486661  [  128/  569]\n",
      "loss: 0.473495  [  256/  569]\n",
      "loss: 0.506542  [  384/  569]\n",
      "loss: 0.504167  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.2869e-18.\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.473484  [    0/  569]\n",
      "loss: 0.494606  [  128/  569]\n",
      "loss: 0.488754  [  256/  569]\n",
      "loss: 0.485419  [  384/  569]\n",
      "loss: 0.532334  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.8583e-18.\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.541229  [    0/  569]\n",
      "loss: 0.514167  [  128/  569]\n",
      "loss: 0.481788  [  256/  569]\n",
      "loss: 0.499334  [  384/  569]\n",
      "loss: 0.499513  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4724e-18.\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.503394  [    0/  569]\n",
      "loss: 0.495144  [  128/  569]\n",
      "loss: 0.538385  [  256/  569]\n",
      "loss: 0.458948  [  384/  569]\n",
      "loss: 0.498289  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.1252e-18.\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.490959  [    0/  569]\n",
      "loss: 0.489480  [  128/  569]\n",
      "loss: 0.505987  [  256/  569]\n",
      "loss: 0.497772  [  384/  569]\n",
      "loss: 0.504125  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8127e-18.\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.473943  [    0/  569]\n",
      "loss: 0.461113  [  128/  569]\n",
      "loss: 0.513894  [  256/  569]\n",
      "loss: 0.516710  [  384/  569]\n",
      "loss: 0.484824  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5314e-18.\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.513057  [    0/  569]\n",
      "loss: 0.492735  [  128/  569]\n",
      "loss: 0.479013  [  256/  569]\n",
      "loss: 0.490281  [  384/  569]\n",
      "loss: 0.483370  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2783e-18.\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.511024  [    0/  569]\n",
      "loss: 0.493901  [  128/  569]\n",
      "loss: 0.478081  [  256/  569]\n",
      "loss: 0.501389  [  384/  569]\n",
      "loss: 0.504104  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0504e-18.\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.502988  [    0/  569]\n",
      "loss: 0.473911  [  128/  569]\n",
      "loss: 0.516872  [  256/  569]\n",
      "loss: 0.449598  [  384/  569]\n",
      "loss: 0.504256  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8454e-18.\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.497212  [    0/  569]\n",
      "loss: 0.473849  [  128/  569]\n",
      "loss: 0.450648  [  256/  569]\n",
      "loss: 0.535441  [  384/  569]\n",
      "loss: 0.498677  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6609e-18.\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.464889  [    0/  569]\n",
      "loss: 0.492401  [  128/  569]\n",
      "loss: 0.484644  [  256/  569]\n",
      "loss: 0.482830  [  384/  569]\n",
      "loss: 0.550065  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4948e-18.\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.490044  [    0/  569]\n",
      "loss: 0.494618  [  128/  569]\n",
      "loss: 0.492592  [  256/  569]\n",
      "loss: 0.497563  [  384/  569]\n",
      "loss: 0.481908  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3453e-18.\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.426773  [    0/  569]\n",
      "loss: 0.514802  [  128/  569]\n",
      "loss: 0.500362  [  256/  569]\n",
      "loss: 0.506298  [  384/  569]\n",
      "loss: 0.475266  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2108e-18.\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.488584  [    0/  569]\n",
      "loss: 0.484556  [  128/  569]\n",
      "loss: 0.445886  [  256/  569]\n",
      "loss: 0.538603  [  384/  569]\n",
      "loss: 0.564809  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0897e-18.\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.498446  [    0/  569]\n",
      "loss: 0.472547  [  128/  569]\n",
      "loss: 0.521186  [  256/  569]\n",
      "loss: 0.496600  [  384/  569]\n",
      "loss: 0.445362  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.8072e-19.\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.506643  [    0/  569]\n",
      "loss: 0.506857  [  128/  569]\n",
      "loss: 0.482829  [  256/  569]\n",
      "loss: 0.513833  [  384/  569]\n",
      "loss: 0.485283  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.8264e-19.\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.530560  [    0/  569]\n",
      "loss: 0.491710  [  128/  569]\n",
      "loss: 0.469600  [  256/  569]\n",
      "loss: 0.456066  [  384/  569]\n",
      "loss: 0.483822  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.9438e-19.\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.479042  [    0/  569]\n",
      "loss: 0.480890  [  128/  569]\n",
      "loss: 0.515433  [  256/  569]\n",
      "loss: 0.479087  [  384/  569]\n",
      "loss: 0.470818  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.1494e-19.\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.482706  [    0/  569]\n",
      "loss: 0.481409  [  128/  569]\n",
      "loss: 0.554451  [  256/  569]\n",
      "loss: 0.510319  [  384/  569]\n",
      "loss: 0.421464  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.4345e-19.\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.511078  [    0/  569]\n",
      "loss: 0.428019  [  128/  569]\n",
      "loss: 0.484374  [  256/  569]\n",
      "loss: 0.472093  [  384/  569]\n",
      "loss: 0.521188  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.7910e-19.\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.522198  [    0/  569]\n",
      "loss: 0.513839  [  128/  569]\n",
      "loss: 0.468032  [  256/  569]\n",
      "loss: 0.495951  [  384/  569]\n",
      "loss: 0.498648  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.2119e-19.\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.472578  [    0/  569]\n",
      "loss: 0.465888  [  128/  569]\n",
      "loss: 0.508453  [  256/  569]\n",
      "loss: 0.529837  [  384/  569]\n",
      "loss: 0.471929  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.6907e-19.\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.468477  [    0/  569]\n",
      "loss: 0.521567  [  128/  569]\n",
      "loss: 0.506274  [  256/  569]\n",
      "loss: 0.497287  [  384/  569]\n",
      "loss: 0.511704  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.2217e-19.\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.493509  [    0/  569]\n",
      "loss: 0.510460  [  128/  569]\n",
      "loss: 0.454584  [  256/  569]\n",
      "loss: 0.513144  [  384/  569]\n",
      "loss: 0.518127  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.7995e-19.\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.495911  [    0/  569]\n",
      "loss: 0.472884  [  128/  569]\n",
      "loss: 0.448976  [  256/  569]\n",
      "loss: 0.498972  [  384/  569]\n",
      "loss: 0.444083  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4195e-19.\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.471222  [    0/  569]\n",
      "loss: 0.489825  [  128/  569]\n",
      "loss: 0.473676  [  256/  569]\n",
      "loss: 0.527896  [  384/  569]\n",
      "loss: 0.503060  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.0776e-19.\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.521760  [    0/  569]\n",
      "loss: 0.487255  [  128/  569]\n",
      "loss: 0.470639  [  256/  569]\n",
      "loss: 0.490203  [  384/  569]\n",
      "loss: 0.467973  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.7698e-19.\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.555359  [    0/  569]\n",
      "loss: 0.459345  [  128/  569]\n",
      "loss: 0.489022  [  256/  569]\n",
      "loss: 0.460144  [  384/  569]\n",
      "loss: 0.512417  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4928e-19.\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.487711  [    0/  569]\n",
      "loss: 0.473758  [  128/  569]\n",
      "loss: 0.496155  [  256/  569]\n",
      "loss: 0.507926  [  384/  569]\n",
      "loss: 0.491753  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2436e-19.\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.559388  [    0/  569]\n",
      "loss: 0.483797  [  128/  569]\n",
      "loss: 0.490339  [  256/  569]\n",
      "loss: 0.500687  [  384/  569]\n",
      "loss: 0.500432  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0192e-19.\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.443291  [    0/  569]\n",
      "loss: 0.543621  [  128/  569]\n",
      "loss: 0.506343  [  256/  569]\n",
      "loss: 0.486687  [  384/  569]\n",
      "loss: 0.535100  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8173e-19.\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.480491  [    0/  569]\n",
      "loss: 0.501568  [  128/  569]\n",
      "loss: 0.458842  [  256/  569]\n",
      "loss: 0.489047  [  384/  569]\n",
      "loss: 0.532839  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6356e-19.\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.520988  [    0/  569]\n",
      "loss: 0.528126  [  128/  569]\n",
      "loss: 0.538932  [  256/  569]\n",
      "loss: 0.446621  [  384/  569]\n",
      "loss: 0.486917  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4720e-19.\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.506770  [    0/  569]\n",
      "loss: 0.471820  [  128/  569]\n",
      "loss: 0.460270  [  256/  569]\n",
      "loss: 0.503040  [  384/  569]\n",
      "loss: 0.512323  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3248e-19.\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.480612  [    0/  569]\n",
      "loss: 0.477496  [  128/  569]\n",
      "loss: 0.480797  [  256/  569]\n",
      "loss: 0.519517  [  384/  569]\n",
      "loss: 0.522602  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1923e-19.\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.489718  [    0/  569]\n",
      "loss: 0.526004  [  128/  569]\n",
      "loss: 0.490904  [  256/  569]\n",
      "loss: 0.484075  [  384/  569]\n",
      "loss: 0.440483  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0731e-19.\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.465095  [    0/  569]\n",
      "loss: 0.512391  [  128/  569]\n",
      "loss: 0.531948  [  256/  569]\n",
      "loss: 0.492178  [  384/  569]\n",
      "loss: 0.520479  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.6578e-20.\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.551684  [    0/  569]\n",
      "loss: 0.422824  [  128/  569]\n",
      "loss: 0.560306  [  256/  569]\n",
      "loss: 0.496734  [  384/  569]\n",
      "loss: 0.518027  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.6920e-20.\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.516047  [    0/  569]\n",
      "loss: 0.504621  [  128/  569]\n",
      "loss: 0.467584  [  256/  569]\n",
      "loss: 0.516393  [  384/  569]\n",
      "loss: 0.464580  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.8228e-20.\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.466079  [    0/  569]\n",
      "loss: 0.542157  [  128/  569]\n",
      "loss: 0.481200  [  256/  569]\n",
      "loss: 0.470167  [  384/  569]\n",
      "loss: 0.469789  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.0405e-20.\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.462920  [    0/  569]\n",
      "loss: 0.529044  [  128/  569]\n",
      "loss: 0.480288  [  256/  569]\n",
      "loss: 0.446225  [  384/  569]\n",
      "loss: 0.483809  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.3365e-20.\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.445186  [    0/  569]\n",
      "loss: 0.493526  [  128/  569]\n",
      "loss: 0.495755  [  256/  569]\n",
      "loss: 0.556153  [  384/  569]\n",
      "loss: 0.479463  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.7028e-20.\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.508966  [    0/  569]\n",
      "loss: 0.479980  [  128/  569]\n",
      "loss: 0.532849  [  256/  569]\n",
      "loss: 0.453334  [  384/  569]\n",
      "loss: 0.532235  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.1326e-20.\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.509706  [    0/  569]\n",
      "loss: 0.477285  [  128/  569]\n",
      "loss: 0.485893  [  256/  569]\n",
      "loss: 0.506820  [  384/  569]\n",
      "loss: 0.467471  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.6193e-20.\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.423157  [    0/  569]\n",
      "loss: 0.519626  [  128/  569]\n",
      "loss: 0.446105  [  256/  569]\n",
      "loss: 0.483513  [  384/  569]\n",
      "loss: 0.525938  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.1574e-20.\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.417307  [    0/  569]\n",
      "loss: 0.570687  [  128/  569]\n",
      "loss: 0.450920  [  256/  569]\n",
      "loss: 0.492379  [  384/  569]\n",
      "loss: 0.588167  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.7416e-20.\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.468975  [    0/  569]\n",
      "loss: 0.478653  [  128/  569]\n",
      "loss: 0.451443  [  256/  569]\n",
      "loss: 0.511914  [  384/  569]\n",
      "loss: 0.541434  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.3675e-20.\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.486065  [    0/  569]\n",
      "loss: 0.471475  [  128/  569]\n",
      "loss: 0.556207  [  256/  569]\n",
      "loss: 0.441468  [  384/  569]\n",
      "loss: 0.493052  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.0307e-20.\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.408275  [    0/  569]\n",
      "loss: 0.482067  [  128/  569]\n",
      "loss: 0.508829  [  256/  569]\n",
      "loss: 0.536696  [  384/  569]\n",
      "loss: 0.507373  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.7276e-20.\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.463231  [    0/  569]\n",
      "loss: 0.471191  [  128/  569]\n",
      "loss: 0.458219  [  256/  569]\n",
      "loss: 0.473992  [  384/  569]\n",
      "loss: 0.503104  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4549e-20.\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.452049  [    0/  569]\n",
      "loss: 0.494581  [  128/  569]\n",
      "loss: 0.468817  [  256/  569]\n",
      "loss: 0.548382  [  384/  569]\n",
      "loss: 0.475930  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2094e-20.\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.499566  [    0/  569]\n",
      "loss: 0.484677  [  128/  569]\n",
      "loss: 0.496291  [  256/  569]\n",
      "loss: 0.553772  [  384/  569]\n",
      "loss: 0.448002  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9885e-20.\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.481536  [    0/  569]\n",
      "loss: 0.506286  [  128/  569]\n",
      "loss: 0.552383  [  256/  569]\n",
      "loss: 0.511883  [  384/  569]\n",
      "loss: 0.471020  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7896e-20.\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.467837  [    0/  569]\n",
      "loss: 0.524138  [  128/  569]\n",
      "loss: 0.469461  [  256/  569]\n",
      "loss: 0.503422  [  384/  569]\n",
      "loss: 0.501727  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6106e-20.\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.515179  [    0/  569]\n",
      "loss: 0.488108  [  128/  569]\n",
      "loss: 0.457560  [  256/  569]\n",
      "loss: 0.553625  [  384/  569]\n",
      "loss: 0.467641  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4496e-20.\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.507600  [    0/  569]\n",
      "loss: 0.503931  [  128/  569]\n",
      "loss: 0.526917  [  256/  569]\n",
      "loss: 0.476069  [  384/  569]\n",
      "loss: 0.466787  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3046e-20.\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.493046  [    0/  569]\n",
      "loss: 0.508848  [  128/  569]\n",
      "loss: 0.501541  [  256/  569]\n",
      "loss: 0.500535  [  384/  569]\n",
      "loss: 0.505603  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1742e-20.\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.435803  [    0/  569]\n",
      "loss: 0.514003  [  128/  569]\n",
      "loss: 0.514631  [  256/  569]\n",
      "loss: 0.527991  [  384/  569]\n",
      "loss: 0.486308  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0567e-20.\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.473717  [    0/  569]\n",
      "loss: 0.485687  [  128/  569]\n",
      "loss: 0.543178  [  256/  569]\n",
      "loss: 0.490680  [  384/  569]\n",
      "loss: 0.477845  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.5107e-21.\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.495939  [    0/  569]\n",
      "loss: 0.539168  [  128/  569]\n",
      "loss: 0.510081  [  256/  569]\n",
      "loss: 0.469110  [  384/  569]\n",
      "loss: 0.481836  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.5597e-21.\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.453366  [    0/  569]\n",
      "loss: 0.557466  [  128/  569]\n",
      "loss: 0.503066  [  256/  569]\n",
      "loss: 0.505448  [  384/  569]\n",
      "loss: 0.486781  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.7037e-21.\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.533177  [    0/  569]\n",
      "loss: 0.486219  [  128/  569]\n",
      "loss: 0.513115  [  256/  569]\n",
      "loss: 0.517498  [  384/  569]\n",
      "loss: 0.492988  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.9333e-21.\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.513541  [    0/  569]\n",
      "loss: 0.477189  [  128/  569]\n",
      "loss: 0.547679  [  256/  569]\n",
      "loss: 0.501349  [  384/  569]\n",
      "loss: 0.437992  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.2400e-21.\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.501529  [    0/  569]\n",
      "loss: 0.491993  [  128/  569]\n",
      "loss: 0.513047  [  256/  569]\n",
      "loss: 0.471655  [  384/  569]\n",
      "loss: 0.513492  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.6160e-21.\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.512947  [    0/  569]\n",
      "loss: 0.469411  [  128/  569]\n",
      "loss: 0.442770  [  256/  569]\n",
      "loss: 0.474110  [  384/  569]\n",
      "loss: 0.523849  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.0544e-21.\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.501860  [    0/  569]\n",
      "loss: 0.488423  [  128/  569]\n",
      "loss: 0.508951  [  256/  569]\n",
      "loss: 0.472421  [  384/  569]\n",
      "loss: 0.553233  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.5489e-21.\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.517542  [    0/  569]\n",
      "loss: 0.472767  [  128/  569]\n",
      "loss: 0.536617  [  256/  569]\n",
      "loss: 0.499456  [  384/  569]\n",
      "loss: 0.484031  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.0941e-21.\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.454442  [    0/  569]\n",
      "loss: 0.504821  [  128/  569]\n",
      "loss: 0.532199  [  256/  569]\n",
      "loss: 0.493726  [  384/  569]\n",
      "loss: 0.561702  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.6846e-21.\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.492992  [    0/  569]\n",
      "loss: 0.470047  [  128/  569]\n",
      "loss: 0.472610  [  256/  569]\n",
      "loss: 0.519225  [  384/  569]\n",
      "loss: 0.506687  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.3162e-21.\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.487066  [    0/  569]\n",
      "loss: 0.493491  [  128/  569]\n",
      "loss: 0.481153  [  256/  569]\n",
      "loss: 0.495243  [  384/  569]\n",
      "loss: 0.430317  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9846e-21.\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.485208  [    0/  569]\n",
      "loss: 0.498074  [  128/  569]\n",
      "loss: 0.433131  [  256/  569]\n",
      "loss: 0.475254  [  384/  569]\n",
      "loss: 0.515988  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6861e-21.\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.471208  [    0/  569]\n",
      "loss: 0.546029  [  128/  569]\n",
      "loss: 0.503823  [  256/  569]\n",
      "loss: 0.464300  [  384/  569]\n",
      "loss: 0.476494  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4175e-21.\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.438371  [    0/  569]\n",
      "loss: 0.529580  [  128/  569]\n",
      "loss: 0.543493  [  256/  569]\n",
      "loss: 0.406552  [  384/  569]\n",
      "loss: 0.464840  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1757e-21.\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.456627  [    0/  569]\n",
      "loss: 0.480865  [  128/  569]\n",
      "loss: 0.511755  [  256/  569]\n",
      "loss: 0.503455  [  384/  569]\n",
      "loss: 0.527471  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9582e-21.\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.510098  [    0/  569]\n",
      "loss: 0.498784  [  128/  569]\n",
      "loss: 0.533873  [  256/  569]\n",
      "loss: 0.481891  [  384/  569]\n",
      "loss: 0.467967  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7624e-21.\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.494900  [    0/  569]\n",
      "loss: 0.510229  [  128/  569]\n",
      "loss: 0.495593  [  256/  569]\n",
      "loss: 0.422451  [  384/  569]\n",
      "loss: 0.508635  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5861e-21.\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.454586  [    0/  569]\n",
      "loss: 0.471775  [  128/  569]\n",
      "loss: 0.478627  [  256/  569]\n",
      "loss: 0.440336  [  384/  569]\n",
      "loss: 0.548885  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4275e-21.\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 0.447411  [    0/  569]\n",
      "loss: 0.495067  [  128/  569]\n",
      "loss: 0.519426  [  256/  569]\n",
      "loss: 0.449407  [  384/  569]\n",
      "loss: 0.511296  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2848e-21.\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.529262  [    0/  569]\n",
      "loss: 0.466466  [  128/  569]\n",
      "loss: 0.480733  [  256/  569]\n",
      "loss: 0.500617  [  384/  569]\n",
      "loss: 0.482603  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1563e-21.\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.475224  [    0/  569]\n",
      "loss: 0.515571  [  128/  569]\n",
      "loss: 0.540933  [  256/  569]\n",
      "loss: 0.456687  [  384/  569]\n",
      "loss: 0.446459  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0407e-21.\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.489966  [    0/  569]\n",
      "loss: 0.531182  [  128/  569]\n",
      "loss: 0.458457  [  256/  569]\n",
      "loss: 0.516773  [  384/  569]\n",
      "loss: 0.478925  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.3659e-22.\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.533441  [    0/  569]\n",
      "loss: 0.509226  [  128/  569]\n",
      "loss: 0.455352  [  256/  569]\n",
      "loss: 0.526034  [  384/  569]\n",
      "loss: 0.488507  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.4293e-22.\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.479129  [    0/  569]\n",
      "loss: 0.525020  [  128/  569]\n",
      "loss: 0.496541  [  256/  569]\n",
      "loss: 0.490194  [  384/  569]\n",
      "loss: 0.460028  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.5864e-22.\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.508054  [    0/  569]\n",
      "loss: 0.459113  [  128/  569]\n",
      "loss: 0.546570  [  256/  569]\n",
      "loss: 0.549545  [  384/  569]\n",
      "loss: 0.508779  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.8277e-22.\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.485664  [    0/  569]\n",
      "loss: 0.457077  [  128/  569]\n",
      "loss: 0.480526  [  256/  569]\n",
      "loss: 0.539108  [  384/  569]\n",
      "loss: 0.490803  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.1450e-22.\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.525918  [    0/  569]\n",
      "loss: 0.524679  [  128/  569]\n",
      "loss: 0.493166  [  256/  569]\n",
      "loss: 0.481604  [  384/  569]\n",
      "loss: 0.443858  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.5305e-22.\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.470542  [    0/  569]\n",
      "loss: 0.547238  [  128/  569]\n",
      "loss: 0.485908  [  256/  569]\n",
      "loss: 0.483420  [  384/  569]\n",
      "loss: 0.507679  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.9774e-22.\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.492526  [    0/  569]\n",
      "loss: 0.439727  [  128/  569]\n",
      "loss: 0.486170  [  256/  569]\n",
      "loss: 0.502624  [  384/  569]\n",
      "loss: 0.555377  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.4797e-22.\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.471212  [    0/  569]\n",
      "loss: 0.469280  [  128/  569]\n",
      "loss: 0.504408  [  256/  569]\n",
      "loss: 0.490605  [  384/  569]\n",
      "loss: 0.483259  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.0317e-22.\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.507332  [    0/  569]\n",
      "loss: 0.494497  [  128/  569]\n",
      "loss: 0.451769  [  256/  569]\n",
      "loss: 0.515100  [  384/  569]\n",
      "loss: 0.470646  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.6285e-22.\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.488330  [    0/  569]\n",
      "loss: 0.456168  [  128/  569]\n",
      "loss: 0.482897  [  256/  569]\n",
      "loss: 0.452852  [  384/  569]\n",
      "loss: 0.444328  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.2657e-22.\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.467552  [    0/  569]\n",
      "loss: 0.492170  [  128/  569]\n",
      "loss: 0.541242  [  256/  569]\n",
      "loss: 0.438544  [  384/  569]\n",
      "loss: 0.531604  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9391e-22.\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.502806  [    0/  569]\n",
      "loss: 0.522907  [  128/  569]\n",
      "loss: 0.504668  [  256/  569]\n",
      "loss: 0.434423  [  384/  569]\n",
      "loss: 0.520757  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6452e-22.\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.456912  [    0/  569]\n",
      "loss: 0.518084  [  128/  569]\n",
      "loss: 0.491988  [  256/  569]\n",
      "loss: 0.451809  [  384/  569]\n",
      "loss: 0.500870  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3807e-22.\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.456398  [    0/  569]\n",
      "loss: 0.492500  [  128/  569]\n",
      "loss: 0.501795  [  256/  569]\n",
      "loss: 0.504863  [  384/  569]\n",
      "loss: 0.475554  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1426e-22.\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.499752  [    0/  569]\n",
      "loss: 0.509517  [  128/  569]\n",
      "loss: 0.478629  [  256/  569]\n",
      "loss: 0.466363  [  384/  569]\n",
      "loss: 0.530421  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9284e-22.\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.502807  [    0/  569]\n",
      "loss: 0.446461  [  128/  569]\n",
      "loss: 0.459165  [  256/  569]\n",
      "loss: 0.540217  [  384/  569]\n",
      "loss: 0.511405  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7355e-22.\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.500483  [    0/  569]\n",
      "loss: 0.518748  [  128/  569]\n",
      "loss: 0.450073  [  256/  569]\n",
      "loss: 0.468946  [  384/  569]\n",
      "loss: 0.515385  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5620e-22.\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.443592  [    0/  569]\n",
      "loss: 0.516847  [  128/  569]\n",
      "loss: 0.505562  [  256/  569]\n",
      "loss: 0.476850  [  384/  569]\n",
      "loss: 0.503158  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4058e-22.\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.496378  [    0/  569]\n",
      "loss: 0.477113  [  128/  569]\n",
      "loss: 0.498386  [  256/  569]\n",
      "loss: 0.508882  [  384/  569]\n",
      "loss: 0.472521  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2652e-22.\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.499008  [    0/  569]\n",
      "loss: 0.484723  [  128/  569]\n",
      "loss: 0.472282  [  256/  569]\n",
      "loss: 0.505648  [  384/  569]\n",
      "loss: 0.479240  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1387e-22.\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.469841  [    0/  569]\n",
      "loss: 0.512147  [  128/  569]\n",
      "loss: 0.531154  [  256/  569]\n",
      "loss: 0.487558  [  384/  569]\n",
      "loss: 0.456985  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0248e-22.\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.489941  [    0/  569]\n",
      "loss: 0.493139  [  128/  569]\n",
      "loss: 0.460775  [  256/  569]\n",
      "loss: 0.497283  [  384/  569]\n",
      "loss: 0.465008  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.2232e-23.\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.492566  [    0/  569]\n",
      "loss: 0.462271  [  128/  569]\n",
      "loss: 0.514241  [  256/  569]\n",
      "loss: 0.447519  [  384/  569]\n",
      "loss: 0.546031  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.3009e-23.\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.483391  [    0/  569]\n",
      "loss: 0.566338  [  128/  569]\n",
      "loss: 0.488759  [  256/  569]\n",
      "loss: 0.505515  [  384/  569]\n",
      "loss: 0.488458  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.4708e-23.\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.508820  [    0/  569]\n",
      "loss: 0.543485  [  128/  569]\n",
      "loss: 0.519703  [  256/  569]\n",
      "loss: 0.506966  [  384/  569]\n",
      "loss: 0.514520  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.7237e-23.\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.509176  [    0/  569]\n",
      "loss: 0.534473  [  128/  569]\n",
      "loss: 0.496053  [  256/  569]\n",
      "loss: 0.465497  [  384/  569]\n",
      "loss: 0.495367  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.0514e-23.\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.540734  [    0/  569]\n",
      "loss: 0.504543  [  128/  569]\n",
      "loss: 0.460557  [  256/  569]\n",
      "loss: 0.456241  [  384/  569]\n",
      "loss: 0.546203  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.4462e-23.\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.510736  [    0/  569]\n",
      "loss: 0.487393  [  128/  569]\n",
      "loss: 0.464984  [  256/  569]\n",
      "loss: 0.481288  [  384/  569]\n",
      "loss: 0.536223  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.9016e-23.\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.465471  [    0/  569]\n",
      "loss: 0.508849  [  128/  569]\n",
      "loss: 0.464631  [  256/  569]\n",
      "loss: 0.549601  [  384/  569]\n",
      "loss: 0.501557  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.4115e-23.\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.423553  [    0/  569]\n",
      "loss: 0.484333  [  128/  569]\n",
      "loss: 0.432116  [  256/  569]\n",
      "loss: 0.523934  [  384/  569]\n",
      "loss: 0.539698  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.9703e-23.\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.519331  [    0/  569]\n",
      "loss: 0.521788  [  128/  569]\n",
      "loss: 0.482507  [  256/  569]\n",
      "loss: 0.436513  [  384/  569]\n",
      "loss: 0.558814  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.5733e-23.\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.526067  [    0/  569]\n",
      "loss: 0.470565  [  128/  569]\n",
      "loss: 0.475962  [  256/  569]\n",
      "loss: 0.471847  [  384/  569]\n",
      "loss: 0.499693  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.2159e-23.\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 0.527667  [    0/  569]\n",
      "loss: 0.493151  [  128/  569]\n",
      "loss: 0.506817  [  256/  569]\n",
      "loss: 0.431562  [  384/  569]\n",
      "loss: 0.521084  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8944e-23.\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.485025  [    0/  569]\n",
      "loss: 0.514647  [  128/  569]\n",
      "loss: 0.492478  [  256/  569]\n",
      "loss: 0.478488  [  384/  569]\n",
      "loss: 0.492572  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6049e-23.\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.477246  [    0/  569]\n",
      "loss: 0.461237  [  128/  569]\n",
      "loss: 0.503745  [  256/  569]\n",
      "loss: 0.459910  [  384/  569]\n",
      "loss: 0.491223  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3444e-23.\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.499997  [    0/  569]\n",
      "loss: 0.533549  [  128/  569]\n",
      "loss: 0.520116  [  256/  569]\n",
      "loss: 0.482275  [  384/  569]\n",
      "loss: 0.437332  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1100e-23.\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.506765  [    0/  569]\n",
      "loss: 0.479662  [  128/  569]\n",
      "loss: 0.545036  [  256/  569]\n",
      "loss: 0.462392  [  384/  569]\n",
      "loss: 0.497380  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8990e-23.\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.506773  [    0/  569]\n",
      "loss: 0.506283  [  128/  569]\n",
      "loss: 0.483991  [  256/  569]\n",
      "loss: 0.528680  [  384/  569]\n",
      "loss: 0.521389  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7091e-23.\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.535015  [    0/  569]\n",
      "loss: 0.528123  [  128/  569]\n",
      "loss: 0.520943  [  256/  569]\n",
      "loss: 0.507399  [  384/  569]\n",
      "loss: 0.486462  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5382e-23.\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.551734  [    0/  569]\n",
      "loss: 0.513059  [  128/  569]\n",
      "loss: 0.534043  [  256/  569]\n",
      "loss: 0.438265  [  384/  569]\n",
      "loss: 0.462485  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3844e-23.\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.510293  [    0/  569]\n",
      "loss: 0.495914  [  128/  569]\n",
      "loss: 0.515352  [  256/  569]\n",
      "loss: 0.460250  [  384/  569]\n",
      "loss: 0.551745  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2459e-23.\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.450341  [    0/  569]\n",
      "loss: 0.471077  [  128/  569]\n",
      "loss: 0.481930  [  256/  569]\n",
      "loss: 0.539861  [  384/  569]\n",
      "loss: 0.505169  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1213e-23.\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.490465  [    0/  569]\n",
      "loss: 0.532778  [  128/  569]\n",
      "loss: 0.529146  [  256/  569]\n",
      "loss: 0.474416  [  384/  569]\n",
      "loss: 0.447166  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0092e-23.\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.462520  [    0/  569]\n",
      "loss: 0.507238  [  128/  569]\n",
      "loss: 0.486126  [  256/  569]\n",
      "loss: 0.564425  [  384/  569]\n",
      "loss: 0.527793  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.0828e-24.\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.466324  [    0/  569]\n",
      "loss: 0.507963  [  128/  569]\n",
      "loss: 0.504824  [  256/  569]\n",
      "loss: 0.463748  [  384/  569]\n",
      "loss: 0.475501  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.1745e-24.\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "loss: 0.494064  [    0/  569]\n",
      "loss: 0.469502  [  128/  569]\n",
      "loss: 0.467279  [  256/  569]\n",
      "loss: 0.460673  [  384/  569]\n",
      "loss: 0.529566  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.3571e-24.\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "loss: 0.465768  [    0/  569]\n",
      "loss: 0.481341  [  128/  569]\n",
      "loss: 0.495628  [  256/  569]\n",
      "loss: 0.487254  [  384/  569]\n",
      "loss: 0.488056  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.6214e-24.\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "loss: 0.489964  [    0/  569]\n",
      "loss: 0.505131  [  128/  569]\n",
      "loss: 0.439987  [  256/  569]\n",
      "loss: 0.512101  [  384/  569]\n",
      "loss: 0.482511  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.9592e-24.\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "loss: 0.479441  [    0/  569]\n",
      "loss: 0.456163  [  128/  569]\n",
      "loss: 0.518085  [  256/  569]\n",
      "loss: 0.508802  [  384/  569]\n",
      "loss: 0.501235  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.3633e-24.\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "loss: 0.519811  [    0/  569]\n",
      "loss: 0.471343  [  128/  569]\n",
      "loss: 0.457697  [  256/  569]\n",
      "loss: 0.512644  [  384/  569]\n",
      "loss: 0.448563  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.8270e-24.\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "loss: 0.530297  [    0/  569]\n",
      "loss: 0.489126  [  128/  569]\n",
      "loss: 0.476772  [  256/  569]\n",
      "loss: 0.497273  [  384/  569]\n",
      "loss: 0.495303  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.3443e-24.\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "loss: 0.454931  [    0/  569]\n",
      "loss: 0.510218  [  128/  569]\n",
      "loss: 0.510393  [  256/  569]\n",
      "loss: 0.445504  [  384/  569]\n",
      "loss: 0.569040  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.9098e-24.\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "loss: 0.525091  [    0/  569]\n",
      "loss: 0.463261  [  128/  569]\n",
      "loss: 0.480017  [  256/  569]\n",
      "loss: 0.476854  [  384/  569]\n",
      "loss: 0.488144  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.5189e-24.\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "loss: 0.493010  [    0/  569]\n",
      "loss: 0.462447  [  128/  569]\n",
      "loss: 0.494001  [  256/  569]\n",
      "loss: 0.496096  [  384/  569]\n",
      "loss: 0.519220  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.1670e-24.\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "loss: 0.487700  [    0/  569]\n",
      "loss: 0.487976  [  128/  569]\n",
      "loss: 0.527121  [  256/  569]\n",
      "loss: 0.512583  [  384/  569]\n",
      "loss: 0.462807  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8503e-24.\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "loss: 0.477552  [    0/  569]\n",
      "loss: 0.405955  [  128/  569]\n",
      "loss: 0.477103  [  256/  569]\n",
      "loss: 0.527692  [  384/  569]\n",
      "loss: 0.498266  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5652e-24.\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 0.491561  [    0/  569]\n",
      "loss: 0.522637  [  128/  569]\n",
      "loss: 0.525965  [  256/  569]\n",
      "loss: 0.504887  [  384/  569]\n",
      "loss: 0.452689  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3087e-24.\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "loss: 0.475126  [    0/  569]\n",
      "loss: 0.550817  [  128/  569]\n",
      "loss: 0.466879  [  256/  569]\n",
      "loss: 0.492447  [  384/  569]\n",
      "loss: 0.464279  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0779e-24.\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "loss: 0.494688  [    0/  569]\n",
      "loss: 0.477790  [  128/  569]\n",
      "loss: 0.532925  [  256/  569]\n",
      "loss: 0.498823  [  384/  569]\n",
      "loss: 0.543009  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8701e-24.\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "loss: 0.536471  [    0/  569]\n",
      "loss: 0.490600  [  128/  569]\n",
      "loss: 0.528385  [  256/  569]\n",
      "loss: 0.468795  [  384/  569]\n",
      "loss: 0.474579  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6831e-24.\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "loss: 0.478875  [    0/  569]\n",
      "loss: 0.482124  [  128/  569]\n",
      "loss: 0.483080  [  256/  569]\n",
      "loss: 0.532047  [  384/  569]\n",
      "loss: 0.470044  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5148e-24.\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "loss: 0.484382  [    0/  569]\n",
      "loss: 0.486585  [  128/  569]\n",
      "loss: 0.524924  [  256/  569]\n",
      "loss: 0.546526  [  384/  569]\n",
      "loss: 0.462332  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3633e-24.\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "loss: 0.453809  [    0/  569]\n",
      "loss: 0.490364  [  128/  569]\n",
      "loss: 0.533206  [  256/  569]\n",
      "loss: 0.541886  [  384/  569]\n",
      "loss: 0.469909  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2269e-24.\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "loss: 0.471273  [    0/  569]\n",
      "loss: 0.489286  [  128/  569]\n",
      "loss: 0.482585  [  256/  569]\n",
      "loss: 0.508934  [  384/  569]\n",
      "loss: 0.449359  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1043e-24.\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "loss: 0.497716  [    0/  569]\n",
      "loss: 0.523874  [  128/  569]\n",
      "loss: 0.475113  [  256/  569]\n",
      "loss: 0.450585  [  384/  569]\n",
      "loss: 0.522460  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.9383e-25.\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "loss: 0.472392  [    0/  569]\n",
      "loss: 0.501762  [  128/  569]\n",
      "loss: 0.477249  [  256/  569]\n",
      "loss: 0.496266  [  384/  569]\n",
      "loss: 0.485607  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.9445e-25.\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "loss: 0.512261  [    0/  569]\n",
      "loss: 0.490472  [  128/  569]\n",
      "loss: 0.499254  [  256/  569]\n",
      "loss: 0.539588  [  384/  569]\n",
      "loss: 0.468123  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.0500e-25.\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "loss: 0.529974  [    0/  569]\n",
      "loss: 0.506932  [  128/  569]\n",
      "loss: 0.496725  [  256/  569]\n",
      "loss: 0.503157  [  384/  569]\n",
      "loss: 0.532946  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.2450e-25.\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "loss: 0.472655  [    0/  569]\n",
      "loss: 0.477791  [  128/  569]\n",
      "loss: 0.479032  [  256/  569]\n",
      "loss: 0.459571  [  384/  569]\n",
      "loss: 0.545806  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.5205e-25.\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "loss: 0.509744  [    0/  569]\n",
      "loss: 0.552316  [  128/  569]\n",
      "loss: 0.492825  [  256/  569]\n",
      "loss: 0.491676  [  384/  569]\n",
      "loss: 0.502925  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.8685e-25.\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "loss: 0.511047  [    0/  569]\n",
      "loss: 0.495119  [  128/  569]\n",
      "loss: 0.524099  [  256/  569]\n",
      "loss: 0.531008  [  384/  569]\n",
      "loss: 0.421223  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.2816e-25.\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "loss: 0.424396  [    0/  569]\n",
      "loss: 0.489833  [  128/  569]\n",
      "loss: 0.543538  [  256/  569]\n",
      "loss: 0.502677  [  384/  569]\n",
      "loss: 0.508581  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.7535e-25.\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "loss: 0.502642  [    0/  569]\n",
      "loss: 0.533827  [  128/  569]\n",
      "loss: 0.471533  [  256/  569]\n",
      "loss: 0.511712  [  384/  569]\n",
      "loss: 0.485190  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.2781e-25.\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "loss: 0.483271  [    0/  569]\n",
      "loss: 0.487348  [  128/  569]\n",
      "loss: 0.552358  [  256/  569]\n",
      "loss: 0.509653  [  384/  569]\n",
      "loss: 0.474218  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.8503e-25.\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "loss: 0.486600  [    0/  569]\n",
      "loss: 0.514396  [  128/  569]\n",
      "loss: 0.555713  [  256/  569]\n",
      "loss: 0.459363  [  384/  569]\n",
      "loss: 0.500873  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4653e-25.\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "loss: 0.464380  [    0/  569]\n",
      "loss: 0.497474  [  128/  569]\n",
      "loss: 0.471682  [  256/  569]\n",
      "loss: 0.497433  [  384/  569]\n",
      "loss: 0.464610  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.1187e-25.\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "loss: 0.538261  [    0/  569]\n",
      "loss: 0.528540  [  128/  569]\n",
      "loss: 0.518357  [  256/  569]\n",
      "loss: 0.444939  [  384/  569]\n",
      "loss: 0.467661  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8069e-25.\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "loss: 0.605757  [    0/  569]\n",
      "loss: 0.493518  [  128/  569]\n",
      "loss: 0.475776  [  256/  569]\n",
      "loss: 0.490452  [  384/  569]\n",
      "loss: 0.529199  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5262e-25.\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "loss: 0.499375  [    0/  569]\n",
      "loss: 0.470312  [  128/  569]\n",
      "loss: 0.491435  [  256/  569]\n",
      "loss: 0.475909  [  384/  569]\n",
      "loss: 0.513203  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2736e-25.\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "loss: 0.505966  [    0/  569]\n",
      "loss: 0.468161  [  128/  569]\n",
      "loss: 0.509698  [  256/  569]\n",
      "loss: 0.517453  [  384/  569]\n",
      "loss: 0.533125  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0462e-25.\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "loss: 0.479998  [    0/  569]\n",
      "loss: 0.497094  [  128/  569]\n",
      "loss: 0.518059  [  256/  569]\n",
      "loss: 0.484580  [  384/  569]\n",
      "loss: 0.495815  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8416e-25.\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "loss: 0.510904  [    0/  569]\n",
      "loss: 0.466037  [  128/  569]\n",
      "loss: 0.539095  [  256/  569]\n",
      "loss: 0.516251  [  384/  569]\n",
      "loss: 0.417510  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6574e-25.\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "loss: 0.496150  [    0/  569]\n",
      "loss: 0.470336  [  128/  569]\n",
      "loss: 0.477571  [  256/  569]\n",
      "loss: 0.504678  [  384/  569]\n",
      "loss: 0.531798  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4917e-25.\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "loss: 0.465064  [    0/  569]\n",
      "loss: 0.501444  [  128/  569]\n",
      "loss: 0.510240  [  256/  569]\n",
      "loss: 0.549883  [  384/  569]\n",
      "loss: 0.483976  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3425e-25.\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "loss: 0.548010  [    0/  569]\n",
      "loss: 0.500150  [  128/  569]\n",
      "loss: 0.504596  [  256/  569]\n",
      "loss: 0.502238  [  384/  569]\n",
      "loss: 0.473154  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2083e-25.\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "loss: 0.522590  [    0/  569]\n",
      "loss: 0.509773  [  128/  569]\n",
      "loss: 0.513136  [  256/  569]\n",
      "loss: 0.491609  [  384/  569]\n",
      "loss: 0.469285  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0874e-25.\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "loss: 0.460768  [    0/  569]\n",
      "loss: 0.497412  [  128/  569]\n",
      "loss: 0.515633  [  256/  569]\n",
      "loss: 0.446719  [  384/  569]\n",
      "loss: 0.541494  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.7869e-26.\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "loss: 0.495753  [    0/  569]\n",
      "loss: 0.476070  [  128/  569]\n",
      "loss: 0.469795  [  256/  569]\n",
      "loss: 0.550573  [  384/  569]\n",
      "loss: 0.493260  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.8082e-26.\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "loss: 0.435903  [    0/  569]\n",
      "loss: 0.524116  [  128/  569]\n",
      "loss: 0.482845  [  256/  569]\n",
      "loss: 0.521860  [  384/  569]\n",
      "loss: 0.487370  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.9274e-26.\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "loss: 0.516685  [    0/  569]\n",
      "loss: 0.484745  [  128/  569]\n",
      "loss: 0.484241  [  256/  569]\n",
      "loss: 0.488354  [  384/  569]\n",
      "loss: 0.551509  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.1347e-26.\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "loss: 0.484904  [    0/  569]\n",
      "loss: 0.496058  [  128/  569]\n",
      "loss: 0.504869  [  256/  569]\n",
      "loss: 0.460461  [  384/  569]\n",
      "loss: 0.548499  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.4212e-26.\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "loss: 0.469618  [    0/  569]\n",
      "loss: 0.465457  [  128/  569]\n",
      "loss: 0.475204  [  256/  569]\n",
      "loss: 0.509846  [  384/  569]\n",
      "loss: 0.522330  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.7791e-26.\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "loss: 0.506505  [    0/  569]\n",
      "loss: 0.456643  [  128/  569]\n",
      "loss: 0.472466  [  256/  569]\n",
      "loss: 0.502718  [  384/  569]\n",
      "loss: 0.499507  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.2012e-26.\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "loss: 0.491060  [    0/  569]\n",
      "loss: 0.507470  [  128/  569]\n",
      "loss: 0.481500  [  256/  569]\n",
      "loss: 0.467908  [  384/  569]\n",
      "loss: 0.485041  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.6811e-26.\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "loss: 0.442118  [    0/  569]\n",
      "loss: 0.468934  [  128/  569]\n",
      "loss: 0.484227  [  256/  569]\n",
      "loss: 0.511048  [  384/  569]\n",
      "loss: 0.470712  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.2130e-26.\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "loss: 0.511001  [    0/  569]\n",
      "loss: 0.496436  [  128/  569]\n",
      "loss: 0.502221  [  256/  569]\n",
      "loss: 0.455325  [  384/  569]\n",
      "loss: 0.517874  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.7917e-26.\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "loss: 0.478396  [    0/  569]\n",
      "loss: 0.484329  [  128/  569]\n",
      "loss: 0.544768  [  256/  569]\n",
      "loss: 0.466342  [  384/  569]\n",
      "loss: 0.530509  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4125e-26.\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "loss: 0.495245  [    0/  569]\n",
      "loss: 0.511203  [  128/  569]\n",
      "loss: 0.463536  [  256/  569]\n",
      "loss: 0.443822  [  384/  569]\n",
      "loss: 0.497458  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.0712e-26.\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "loss: 0.497670  [    0/  569]\n",
      "loss: 0.507967  [  128/  569]\n",
      "loss: 0.464104  [  256/  569]\n",
      "loss: 0.507311  [  384/  569]\n",
      "loss: 0.517918  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.7641e-26.\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "loss: 0.442333  [    0/  569]\n",
      "loss: 0.483708  [  128/  569]\n",
      "loss: 0.487271  [  256/  569]\n",
      "loss: 0.502869  [  384/  569]\n",
      "loss: 0.493964  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4877e-26.\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "loss: 0.502634  [    0/  569]\n",
      "loss: 0.450324  [  128/  569]\n",
      "loss: 0.507533  [  256/  569]\n",
      "loss: 0.483348  [  384/  569]\n",
      "loss: 0.486533  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2389e-26.\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "loss: 0.445063  [    0/  569]\n",
      "loss: 0.508268  [  128/  569]\n",
      "loss: 0.457886  [  256/  569]\n",
      "loss: 0.473848  [  384/  569]\n",
      "loss: 0.566349  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0150e-26.\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "loss: 0.443788  [    0/  569]\n",
      "loss: 0.536408  [  128/  569]\n",
      "loss: 0.527671  [  256/  569]\n",
      "loss: 0.513107  [  384/  569]\n",
      "loss: 0.428710  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8135e-26.\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "loss: 0.481666  [    0/  569]\n",
      "loss: 0.504641  [  128/  569]\n",
      "loss: 0.468937  [  256/  569]\n",
      "loss: 0.501895  [  384/  569]\n",
      "loss: 0.540094  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6322e-26.\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "loss: 0.465300  [    0/  569]\n",
      "loss: 0.524007  [  128/  569]\n",
      "loss: 0.482541  [  256/  569]\n",
      "loss: 0.443050  [  384/  569]\n",
      "loss: 0.467583  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4690e-26.\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "loss: 0.546541  [    0/  569]\n",
      "loss: 0.464943  [  128/  569]\n",
      "loss: 0.489413  [  256/  569]\n",
      "loss: 0.548206  [  384/  569]\n",
      "loss: 0.516540  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3221e-26.\n",
      "Done! lr = 1.3220708194808223e-26\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dl, model, loss_fn, optimizer, scheduler=schedulerExponential, onPlateau=False)\n",
    "print(f\"Done! lr = {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso podemos ver que no se ha mejorado. Podría ser porque el valor del learning rate ha bajado tan rápido que se ha hecho muy pequeño y en el entrenamiento no mejora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducción del learning si la métrica no mejora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver el mismo ejemplo reduciendo el learning rate cuando la métrica no mejora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Se define una semilla para que la inicialización de los pesos aleatoria sea siempre la misma\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "model = CancerNeuralNetwork(31, 1)\n",
    "model.to(device)\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "LR = 1e-3\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "\n",
    "schedulerOnPlateau = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.010835  [    0/  569]\n",
      "loss: 4.976839  [  128/  569]\n",
      "loss: 1.088202  [  256/  569]\n",
      "loss: 0.989754  [  384/  569]\n",
      "loss: 2.056240  [  456/  569]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.452749  [    0/  569]\n",
      "loss: 0.852072  [  128/  569]\n",
      "loss: 1.305426  [  256/  569]\n",
      "loss: 1.547930  [  384/  569]\n",
      "loss: 0.906874  [  456/  569]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.809822  [    0/  569]\n",
      "loss: 0.795044  [  128/  569]\n",
      "loss: 0.638141  [  256/  569]\n",
      "loss: 0.690192  [  384/  569]\n",
      "loss: 1.183605  [  456/  569]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.689182  [    0/  569]\n",
      "loss: 0.708076  [  128/  569]\n",
      "loss: 0.727928  [  256/  569]\n",
      "loss: 0.612084  [  384/  569]\n",
      "loss: 0.569775  [  456/  569]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.583048  [    0/  569]\n",
      "loss: 0.605605  [  128/  569]\n",
      "loss: 0.597755  [  256/  569]\n",
      "loss: 0.593264  [  384/  569]\n",
      "loss: 0.555762  [  456/  569]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.560533  [    0/  569]\n",
      "loss: 0.606022  [  128/  569]\n",
      "loss: 0.649214  [  256/  569]\n",
      "loss: 0.582997  [  384/  569]\n",
      "loss: 0.674636  [  456/  569]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.589584  [    0/  569]\n",
      "loss: 0.567771  [  128/  569]\n",
      "loss: 0.540669  [  256/  569]\n",
      "loss: 0.572264  [  384/  569]\n",
      "loss: 0.547533  [  456/  569]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.549541  [    0/  569]\n",
      "loss: 0.527712  [  128/  569]\n",
      "loss: 0.706295  [  256/  569]\n",
      "loss: 0.676892  [  384/  569]\n",
      "loss: 0.608055  [  456/  569]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.576598  [    0/  569]\n",
      "loss: 0.563597  [  128/  569]\n",
      "loss: 0.565747  [  256/  569]\n",
      "loss: 0.560480  [  384/  569]\n",
      "loss: 0.542406  [  456/  569]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.533795  [    0/  569]\n",
      "loss: 0.524325  [  128/  569]\n",
      "loss: 0.508124  [  256/  569]\n",
      "loss: 0.552691  [  384/  569]\n",
      "loss: 0.673658  [  456/  569]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.740433  [    0/  569]\n",
      "loss: 0.502843  [  128/  569]\n",
      "loss: 0.480381  [  256/  569]\n",
      "loss: 0.501871  [  384/  569]\n",
      "loss: 0.523998  [  456/  569]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.462362  [    0/  569]\n",
      "loss: 0.687931  [  128/  569]\n",
      "loss: 0.607976  [  256/  569]\n",
      "loss: 0.617812  [  384/  569]\n",
      "loss: 0.524706  [  456/  569]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.649603  [    0/  569]\n",
      "loss: 0.719671  [  128/  569]\n",
      "loss: 0.527713  [  256/  569]\n",
      "loss: 0.476069  [  384/  569]\n",
      "loss: 0.582269  [  456/  569]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.595615  [    0/  569]\n",
      "loss: 0.548386  [  128/  569]\n",
      "loss: 0.668005  [  256/  569]\n",
      "loss: 0.610247  [  384/  569]\n",
      "loss: 0.571391  [  456/  569]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.648396  [    0/  569]\n",
      "loss: 0.442423  [  128/  569]\n",
      "loss: 0.425231  [  256/  569]\n",
      "loss: 0.532290  [  384/  569]\n",
      "loss: 0.506401  [  456/  569]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.522067  [    0/  569]\n",
      "loss: 0.485718  [  128/  569]\n",
      "loss: 0.497100  [  256/  569]\n",
      "loss: 0.680997  [  384/  569]\n",
      "loss: 0.562090  [  456/  569]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.529039  [    0/  569]\n",
      "loss: 0.449875  [  128/  569]\n",
      "loss: 0.484772  [  256/  569]\n",
      "loss: 0.465413  [  384/  569]\n",
      "loss: 0.405232  [  456/  569]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.508134  [    0/  569]\n",
      "loss: 0.465621  [  128/  569]\n",
      "loss: 0.483011  [  256/  569]\n",
      "loss: 0.436114  [  384/  569]\n",
      "loss: 0.419241  [  456/  569]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.444139  [    0/  569]\n",
      "loss: 0.469899  [  128/  569]\n",
      "loss: 0.420347  [  256/  569]\n",
      "loss: 0.415841  [  384/  569]\n",
      "loss: 0.468701  [  456/  569]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.404192  [    0/  569]\n",
      "loss: 0.448077  [  128/  569]\n",
      "loss: 0.495588  [  256/  569]\n",
      "loss: 0.531647  [  384/  569]\n",
      "loss: 0.425645  [  456/  569]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.396829  [    0/  569]\n",
      "loss: 0.604932  [  128/  569]\n",
      "loss: 0.737976  [  256/  569]\n",
      "loss: 0.791596  [  384/  569]\n",
      "loss: 0.433093  [  456/  569]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.479557  [    0/  569]\n",
      "loss: 0.442613  [  128/  569]\n",
      "loss: 0.462668  [  256/  569]\n",
      "loss: 0.396762  [  384/  569]\n",
      "loss: 0.456745  [  456/  569]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.447425  [    0/  569]\n",
      "loss: 0.498830  [  128/  569]\n",
      "loss: 0.384478  [  256/  569]\n",
      "loss: 0.507664  [  384/  569]\n",
      "loss: 0.487293  [  456/  569]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.528589  [    0/  569]\n",
      "loss: 0.455708  [  128/  569]\n",
      "loss: 0.454495  [  256/  569]\n",
      "loss: 0.456561  [  384/  569]\n",
      "loss: 0.414833  [  456/  569]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.460361  [    0/  569]\n",
      "loss: 0.422549  [  128/  569]\n",
      "loss: 0.443054  [  256/  569]\n",
      "loss: 0.474521  [  384/  569]\n",
      "loss: 0.525737  [  456/  569]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.464506  [    0/  569]\n",
      "loss: 0.376312  [  128/  569]\n",
      "loss: 0.418592  [  256/  569]\n",
      "loss: 0.373305  [  384/  569]\n",
      "loss: 0.538292  [  456/  569]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.468397  [    0/  569]\n",
      "loss: 0.503836  [  128/  569]\n",
      "loss: 0.418757  [  256/  569]\n",
      "loss: 0.520395  [  384/  569]\n",
      "loss: 0.389233  [  456/  569]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.525356  [    0/  569]\n",
      "loss: 0.403796  [  128/  569]\n",
      "loss: 0.420031  [  256/  569]\n",
      "loss: 0.373586  [  384/  569]\n",
      "loss: 0.296368  [  456/  569]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.374042  [    0/  569]\n",
      "loss: 0.398272  [  128/  569]\n",
      "loss: 0.429078  [  256/  569]\n",
      "loss: 0.406237  [  384/  569]\n",
      "loss: 0.374877  [  456/  569]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.398619  [    0/  569]\n",
      "loss: 0.412277  [  128/  569]\n",
      "loss: 0.402910  [  256/  569]\n",
      "loss: 0.349278  [  384/  569]\n",
      "loss: 0.450172  [  456/  569]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.369628  [    0/  569]\n",
      "loss: 0.372794  [  128/  569]\n",
      "loss: 0.312439  [  256/  569]\n",
      "loss: 0.974680  [  384/  569]\n",
      "loss: 0.638555  [  456/  569]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.757864  [    0/  569]\n",
      "loss: 0.429475  [  128/  569]\n",
      "loss: 0.364063  [  256/  569]\n",
      "loss: 0.333338  [  384/  569]\n",
      "loss: 0.420487  [  456/  569]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.371355  [    0/  569]\n",
      "loss: 0.373163  [  128/  569]\n",
      "loss: 0.487672  [  256/  569]\n",
      "loss: 0.495208  [  384/  569]\n",
      "loss: 0.661653  [  456/  569]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.451180  [    0/  569]\n",
      "loss: 0.521518  [  128/  569]\n",
      "loss: 0.615372  [  256/  569]\n",
      "loss: 0.487347  [  384/  569]\n",
      "loss: 0.328617  [  456/  569]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.431906  [    0/  569]\n",
      "loss: 0.327230  [  128/  569]\n",
      "loss: 0.392563  [  256/  569]\n",
      "loss: 0.352681  [  384/  569]\n",
      "loss: 0.443610  [  456/  569]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.498166  [    0/  569]\n",
      "loss: 0.368514  [  128/  569]\n",
      "loss: 0.465028  [  256/  569]\n",
      "loss: 0.411254  [  384/  569]\n",
      "loss: 0.326883  [  456/  569]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.412262  [    0/  569]\n",
      "loss: 0.412846  [  128/  569]\n",
      "loss: 0.421528  [  256/  569]\n",
      "loss: 0.510371  [  384/  569]\n",
      "loss: 0.416416  [  456/  569]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.448007  [    0/  569]\n",
      "loss: 0.346762  [  128/  569]\n",
      "loss: 0.339076  [  256/  569]\n",
      "loss: 0.327275  [  384/  569]\n",
      "loss: 0.601628  [  456/  569]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.429789  [    0/  569]\n",
      "loss: 0.376573  [  128/  569]\n",
      "loss: 0.351658  [  256/  569]\n",
      "loss: 0.345978  [  384/  569]\n",
      "loss: 0.439912  [  456/  569]\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.566837  [    0/  569]\n",
      "loss: 0.449734  [  128/  569]\n",
      "loss: 0.366011  [  256/  569]\n",
      "loss: 0.378336  [  384/  569]\n",
      "loss: 0.324864  [  456/  569]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.343913  [    0/  569]\n",
      "loss: 0.367610  [  128/  569]\n",
      "loss: 0.292992  [  256/  569]\n",
      "loss: 0.320843  [  384/  569]\n",
      "loss: 0.332647  [  456/  569]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.384569  [    0/  569]\n",
      "loss: 0.362420  [  128/  569]\n",
      "loss: 0.374169  [  256/  569]\n",
      "loss: 0.338735  [  384/  569]\n",
      "loss: 0.320705  [  456/  569]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.442678  [    0/  569]\n",
      "loss: 0.312397  [  128/  569]\n",
      "loss: 0.349706  [  256/  569]\n",
      "loss: 0.368426  [  384/  569]\n",
      "loss: 0.301040  [  456/  569]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.330822  [    0/  569]\n",
      "loss: 0.302330  [  128/  569]\n",
      "loss: 0.367049  [  256/  569]\n",
      "loss: 0.461618  [  384/  569]\n",
      "loss: 0.354564  [  456/  569]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.397576  [    0/  569]\n",
      "loss: 0.379787  [  128/  569]\n",
      "loss: 0.352638  [  256/  569]\n",
      "loss: 0.320831  [  384/  569]\n",
      "loss: 0.319909  [  456/  569]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.386256  [    0/  569]\n",
      "loss: 0.323518  [  128/  569]\n",
      "loss: 0.306359  [  256/  569]\n",
      "loss: 0.368034  [  384/  569]\n",
      "loss: 0.291924  [  456/  569]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.353219  [    0/  569]\n",
      "loss: 0.360300  [  128/  569]\n",
      "loss: 0.331647  [  256/  569]\n",
      "loss: 0.346166  [  384/  569]\n",
      "loss: 0.266520  [  456/  569]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.357055  [    0/  569]\n",
      "loss: 0.327552  [  128/  569]\n",
      "loss: 0.302351  [  256/  569]\n",
      "loss: 0.344722  [  384/  569]\n",
      "loss: 0.371008  [  456/  569]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.396428  [    0/  569]\n",
      "loss: 0.300325  [  128/  569]\n",
      "loss: 0.289464  [  256/  569]\n",
      "loss: 0.466424  [  384/  569]\n",
      "loss: 0.322050  [  456/  569]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.339906  [    0/  569]\n",
      "loss: 0.339553  [  128/  569]\n",
      "loss: 0.322561  [  256/  569]\n",
      "loss: 0.315440  [  384/  569]\n",
      "loss: 0.327223  [  456/  569]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.320785  [    0/  569]\n",
      "loss: 0.389428  [  128/  569]\n",
      "loss: 0.362982  [  256/  569]\n",
      "loss: 0.395933  [  384/  569]\n",
      "loss: 0.284564  [  456/  569]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.307659  [    0/  569]\n",
      "loss: 0.316736  [  128/  569]\n",
      "loss: 0.384471  [  256/  569]\n",
      "loss: 0.294467  [  384/  569]\n",
      "loss: 0.396161  [  456/  569]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.305187  [    0/  569]\n",
      "loss: 0.315663  [  128/  569]\n",
      "loss: 0.411919  [  256/  569]\n",
      "loss: 0.311135  [  384/  569]\n",
      "loss: 0.427174  [  456/  569]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.281446  [    0/  569]\n",
      "loss: 0.363280  [  128/  569]\n",
      "loss: 0.334736  [  256/  569]\n",
      "loss: 0.343984  [  384/  569]\n",
      "loss: 0.421408  [  456/  569]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.283456  [    0/  569]\n",
      "loss: 0.380188  [  128/  569]\n",
      "loss: 0.355482  [  256/  569]\n",
      "loss: 0.373332  [  384/  569]\n",
      "loss: 0.421573  [  456/  569]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.363779  [    0/  569]\n",
      "loss: 0.324737  [  128/  569]\n",
      "loss: 0.370381  [  256/  569]\n",
      "loss: 0.277273  [  384/  569]\n",
      "loss: 0.323702  [  456/  569]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.342596  [    0/  569]\n",
      "loss: 0.436976  [  128/  569]\n",
      "loss: 0.342133  [  256/  569]\n",
      "loss: 0.225025  [  384/  569]\n",
      "loss: 0.316456  [  456/  569]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.340553  [    0/  569]\n",
      "loss: 0.363386  [  128/  569]\n",
      "loss: 0.326980  [  256/  569]\n",
      "loss: 0.383907  [  384/  569]\n",
      "loss: 0.281480  [  456/  569]\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.313086  [    0/  569]\n",
      "loss: 0.314847  [  128/  569]\n",
      "loss: 0.306979  [  256/  569]\n",
      "loss: 0.371685  [  384/  569]\n",
      "loss: 0.365296  [  456/  569]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.373199  [    0/  569]\n",
      "loss: 0.327613  [  128/  569]\n",
      "loss: 0.402053  [  256/  569]\n",
      "loss: 0.325028  [  384/  569]\n",
      "loss: 0.294218  [  456/  569]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.413349  [    0/  569]\n",
      "loss: 0.370067  [  128/  569]\n",
      "loss: 0.322553  [  256/  569]\n",
      "loss: 0.297326  [  384/  569]\n",
      "loss: 0.304652  [  456/  569]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.376946  [    0/  569]\n",
      "loss: 0.284819  [  128/  569]\n",
      "loss: 0.338073  [  256/  569]\n",
      "loss: 0.332464  [  384/  569]\n",
      "loss: 0.279914  [  456/  569]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.298877  [    0/  569]\n",
      "loss: 0.295862  [  128/  569]\n",
      "loss: 0.407389  [  256/  569]\n",
      "loss: 0.317686  [  384/  569]\n",
      "loss: 0.332946  [  456/  569]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.294703  [    0/  569]\n",
      "loss: 0.326142  [  128/  569]\n",
      "loss: 0.334641  [  256/  569]\n",
      "loss: 0.394418  [  384/  569]\n",
      "loss: 0.311625  [  456/  569]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.317056  [    0/  569]\n",
      "loss: 0.265363  [  128/  569]\n",
      "loss: 0.322939  [  256/  569]\n",
      "loss: 0.352842  [  384/  569]\n",
      "loss: 0.413387  [  456/  569]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.400337  [    0/  569]\n",
      "loss: 0.416686  [  128/  569]\n",
      "loss: 0.290442  [  256/  569]\n",
      "loss: 0.265647  [  384/  569]\n",
      "loss: 0.364251  [  456/  569]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.342781  [    0/  569]\n",
      "loss: 0.305618  [  128/  569]\n",
      "loss: 0.354465  [  256/  569]\n",
      "loss: 0.313600  [  384/  569]\n",
      "loss: 0.292267  [  456/  569]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.364146  [    0/  569]\n",
      "loss: 0.331302  [  128/  569]\n",
      "loss: 0.354886  [  256/  569]\n",
      "loss: 0.344868  [  384/  569]\n",
      "loss: 0.329853  [  456/  569]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.378425  [    0/  569]\n",
      "loss: 0.345183  [  128/  569]\n",
      "loss: 0.376671  [  256/  569]\n",
      "loss: 0.350077  [  384/  569]\n",
      "loss: 0.264249  [  456/  569]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.408943  [    0/  569]\n",
      "loss: 0.298752  [  128/  569]\n",
      "loss: 0.269116  [  256/  569]\n",
      "loss: 0.294504  [  384/  569]\n",
      "loss: 0.340416  [  456/  569]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.371337  [    0/  569]\n",
      "loss: 0.344472  [  128/  569]\n",
      "loss: 0.291521  [  256/  569]\n",
      "loss: 0.311605  [  384/  569]\n",
      "loss: 0.361416  [  456/  569]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.421521  [    0/  569]\n",
      "loss: 0.361637  [  128/  569]\n",
      "loss: 0.333135  [  256/  569]\n",
      "loss: 0.300697  [  384/  569]\n",
      "loss: 0.331778  [  456/  569]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.295247  [    0/  569]\n",
      "loss: 0.334841  [  128/  569]\n",
      "loss: 0.322312  [  256/  569]\n",
      "loss: 0.362944  [  384/  569]\n",
      "loss: 0.344553  [  456/  569]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.407471  [    0/  569]\n",
      "loss: 0.307214  [  128/  569]\n",
      "loss: 0.309407  [  256/  569]\n",
      "loss: 0.229188  [  384/  569]\n",
      "loss: 0.326353  [  456/  569]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.340064  [    0/  569]\n",
      "loss: 0.322867  [  128/  569]\n",
      "loss: 0.354824  [  256/  569]\n",
      "loss: 0.355154  [  384/  569]\n",
      "loss: 0.347010  [  456/  569]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.307826  [    0/  569]\n",
      "loss: 0.313040  [  128/  569]\n",
      "loss: 0.353222  [  256/  569]\n",
      "loss: 0.373949  [  384/  569]\n",
      "loss: 0.373564  [  456/  569]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.401882  [    0/  569]\n",
      "loss: 0.299291  [  128/  569]\n",
      "loss: 0.275846  [  256/  569]\n",
      "loss: 0.401461  [  384/  569]\n",
      "loss: 0.339307  [  456/  569]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.371549  [    0/  569]\n",
      "loss: 0.351711  [  128/  569]\n",
      "loss: 0.336638  [  256/  569]\n",
      "loss: 0.359485  [  384/  569]\n",
      "loss: 0.366108  [  456/  569]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.310907  [    0/  569]\n",
      "loss: 0.334981  [  128/  569]\n",
      "loss: 0.303381  [  256/  569]\n",
      "loss: 0.320296  [  384/  569]\n",
      "loss: 0.297757  [  456/  569]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.315515  [    0/  569]\n",
      "loss: 0.279408  [  128/  569]\n",
      "loss: 0.331694  [  256/  569]\n",
      "loss: 0.319207  [  384/  569]\n",
      "loss: 0.393679  [  456/  569]\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.298009  [    0/  569]\n",
      "loss: 0.321707  [  128/  569]\n",
      "loss: 0.327983  [  256/  569]\n",
      "loss: 0.410689  [  384/  569]\n",
      "loss: 0.397261  [  456/  569]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.324995  [    0/  569]\n",
      "loss: 0.310072  [  128/  569]\n",
      "loss: 0.386152  [  256/  569]\n",
      "loss: 0.458564  [  384/  569]\n",
      "loss: 0.319832  [  456/  569]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.351814  [    0/  569]\n",
      "loss: 0.408069  [  128/  569]\n",
      "loss: 0.292241  [  256/  569]\n",
      "loss: 0.355636  [  384/  569]\n",
      "loss: 0.396992  [  456/  569]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.350069  [    0/  569]\n",
      "loss: 0.363469  [  128/  569]\n",
      "loss: 0.270834  [  256/  569]\n",
      "loss: 0.294084  [  384/  569]\n",
      "loss: 0.370656  [  456/  569]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.293680  [    0/  569]\n",
      "loss: 0.340099  [  128/  569]\n",
      "loss: 0.342808  [  256/  569]\n",
      "loss: 0.350880  [  384/  569]\n",
      "loss: 0.318048  [  456/  569]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.375333  [    0/  569]\n",
      "loss: 0.361612  [  128/  569]\n",
      "loss: 0.356790  [  256/  569]\n",
      "loss: 0.299800  [  384/  569]\n",
      "loss: 0.334436  [  456/  569]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.294106  [    0/  569]\n",
      "loss: 0.346892  [  128/  569]\n",
      "loss: 0.313402  [  256/  569]\n",
      "loss: 0.315984  [  384/  569]\n",
      "loss: 0.317230  [  456/  569]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.276865  [    0/  569]\n",
      "loss: 0.395521  [  128/  569]\n",
      "loss: 0.381076  [  256/  569]\n",
      "loss: 0.390860  [  384/  569]\n",
      "loss: 0.310162  [  456/  569]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.321407  [    0/  569]\n",
      "loss: 0.342892  [  128/  569]\n",
      "loss: 0.316703  [  256/  569]\n",
      "loss: 0.417344  [  384/  569]\n",
      "loss: 0.401860  [  456/  569]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.328005  [    0/  569]\n",
      "loss: 0.382493  [  128/  569]\n",
      "loss: 0.294825  [  256/  569]\n",
      "loss: 0.390374  [  384/  569]\n",
      "loss: 0.323766  [  456/  569]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.369343  [    0/  569]\n",
      "loss: 0.314941  [  128/  569]\n",
      "loss: 0.309567  [  256/  569]\n",
      "loss: 0.271263  [  384/  569]\n",
      "loss: 0.370291  [  456/  569]\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.314229  [    0/  569]\n",
      "loss: 0.319159  [  128/  569]\n",
      "loss: 0.335434  [  256/  569]\n",
      "loss: 0.373112  [  384/  569]\n",
      "loss: 0.351220  [  456/  569]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.323433  [    0/  569]\n",
      "loss: 0.354467  [  128/  569]\n",
      "loss: 0.355566  [  256/  569]\n",
      "loss: 0.351026  [  384/  569]\n",
      "loss: 0.251937  [  456/  569]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.378397  [    0/  569]\n",
      "loss: 0.281354  [  128/  569]\n",
      "loss: 0.384960  [  256/  569]\n",
      "loss: 0.305717  [  384/  569]\n",
      "loss: 0.326221  [  456/  569]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.282272  [    0/  569]\n",
      "loss: 0.366459  [  128/  569]\n",
      "loss: 0.329286  [  256/  569]\n",
      "loss: 0.330831  [  384/  569]\n",
      "loss: 0.295071  [  456/  569]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.260321  [    0/  569]\n",
      "loss: 0.277464  [  128/  569]\n",
      "loss: 0.356206  [  256/  569]\n",
      "loss: 0.440760  [  384/  569]\n",
      "loss: 0.272782  [  456/  569]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.316264  [    0/  569]\n",
      "loss: 0.323248  [  128/  569]\n",
      "loss: 0.399688  [  256/  569]\n",
      "loss: 0.300391  [  384/  569]\n",
      "loss: 0.298116  [  456/  569]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.374871  [    0/  569]\n",
      "loss: 0.321536  [  128/  569]\n",
      "loss: 0.365869  [  256/  569]\n",
      "loss: 0.326910  [  384/  569]\n",
      "loss: 0.367075  [  456/  569]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.302223  [    0/  569]\n",
      "loss: 0.276918  [  128/  569]\n",
      "loss: 0.288897  [  256/  569]\n",
      "loss: 0.384758  [  384/  569]\n",
      "loss: 0.280721  [  456/  569]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.363569  [    0/  569]\n",
      "loss: 0.352147  [  128/  569]\n",
      "loss: 0.256245  [  256/  569]\n",
      "loss: 0.374259  [  384/  569]\n",
      "loss: 0.348902  [  456/  569]\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.387974  [    0/  569]\n",
      "loss: 0.355650  [  128/  569]\n",
      "loss: 0.315610  [  256/  569]\n",
      "loss: 0.284292  [  384/  569]\n",
      "loss: 0.313781  [  456/  569]\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.328626  [    0/  569]\n",
      "loss: 0.327349  [  128/  569]\n",
      "loss: 0.307419  [  256/  569]\n",
      "loss: 0.309446  [  384/  569]\n",
      "loss: 0.381044  [  456/  569]\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.415229  [    0/  569]\n",
      "loss: 0.297970  [  128/  569]\n",
      "loss: 0.429730  [  256/  569]\n",
      "loss: 0.300507  [  384/  569]\n",
      "loss: 0.295783  [  456/  569]\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.330186  [    0/  569]\n",
      "loss: 0.350072  [  128/  569]\n",
      "loss: 0.414557  [  256/  569]\n",
      "loss: 0.304229  [  384/  569]\n",
      "loss: 0.307343  [  456/  569]\n",
      "Epoch   104: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.352758  [    0/  569]\n",
      "loss: 0.329415  [  128/  569]\n",
      "loss: 0.287515  [  256/  569]\n",
      "loss: 0.285838  [  384/  569]\n",
      "loss: 0.391499  [  456/  569]\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.265568  [    0/  569]\n",
      "loss: 0.403426  [  128/  569]\n",
      "loss: 0.298663  [  256/  569]\n",
      "loss: 0.263773  [  384/  569]\n",
      "loss: 0.330043  [  456/  569]\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.373211  [    0/  569]\n",
      "loss: 0.338532  [  128/  569]\n",
      "loss: 0.399289  [  256/  569]\n",
      "loss: 0.299668  [  384/  569]\n",
      "loss: 0.304063  [  456/  569]\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.366735  [    0/  569]\n",
      "loss: 0.294012  [  128/  569]\n",
      "loss: 0.344783  [  256/  569]\n",
      "loss: 0.296938  [  384/  569]\n",
      "loss: 0.327874  [  456/  569]\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.381711  [    0/  569]\n",
      "loss: 0.324666  [  128/  569]\n",
      "loss: 0.353618  [  256/  569]\n",
      "loss: 0.350023  [  384/  569]\n",
      "loss: 0.364624  [  456/  569]\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.349493  [    0/  569]\n",
      "loss: 0.341206  [  128/  569]\n",
      "loss: 0.392919  [  256/  569]\n",
      "loss: 0.298991  [  384/  569]\n",
      "loss: 0.426436  [  456/  569]\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.417367  [    0/  569]\n",
      "loss: 0.304443  [  128/  569]\n",
      "loss: 0.291118  [  256/  569]\n",
      "loss: 0.280044  [  384/  569]\n",
      "loss: 0.318655  [  456/  569]\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.278515  [    0/  569]\n",
      "loss: 0.380881  [  128/  569]\n",
      "loss: 0.332522  [  256/  569]\n",
      "loss: 0.351312  [  384/  569]\n",
      "loss: 0.339101  [  456/  569]\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.448898  [    0/  569]\n",
      "loss: 0.302538  [  128/  569]\n",
      "loss: 0.295747  [  256/  569]\n",
      "loss: 0.254593  [  384/  569]\n",
      "loss: 0.400756  [  456/  569]\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.319795  [    0/  569]\n",
      "loss: 0.317616  [  128/  569]\n",
      "loss: 0.345824  [  256/  569]\n",
      "loss: 0.248854  [  384/  569]\n",
      "loss: 0.375880  [  456/  569]\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.355563  [    0/  569]\n",
      "loss: 0.344901  [  128/  569]\n",
      "loss: 0.297071  [  256/  569]\n",
      "loss: 0.307823  [  384/  569]\n",
      "loss: 0.352512  [  456/  569]\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.377288  [    0/  569]\n",
      "loss: 0.393572  [  128/  569]\n",
      "loss: 0.298718  [  256/  569]\n",
      "loss: 0.262765  [  384/  569]\n",
      "loss: 0.373130  [  456/  569]\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.322691  [    0/  569]\n",
      "loss: 0.318866  [  128/  569]\n",
      "loss: 0.362081  [  256/  569]\n",
      "loss: 0.334442  [  384/  569]\n",
      "loss: 0.367068  [  456/  569]\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.297257  [    0/  569]\n",
      "loss: 0.313607  [  128/  569]\n",
      "loss: 0.380885  [  256/  569]\n",
      "loss: 0.400973  [  384/  569]\n",
      "loss: 0.329787  [  456/  569]\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.264404  [    0/  569]\n",
      "loss: 0.364953  [  128/  569]\n",
      "loss: 0.401431  [  256/  569]\n",
      "loss: 0.341614  [  384/  569]\n",
      "loss: 0.311784  [  456/  569]\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.385345  [    0/  569]\n",
      "loss: 0.305981  [  128/  569]\n",
      "loss: 0.352828  [  256/  569]\n",
      "loss: 0.332338  [  384/  569]\n",
      "loss: 0.279517  [  456/  569]\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.337423  [    0/  569]\n",
      "loss: 0.293882  [  128/  569]\n",
      "loss: 0.391459  [  256/  569]\n",
      "loss: 0.335841  [  384/  569]\n",
      "loss: 0.322653  [  456/  569]\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.346829  [    0/  569]\n",
      "loss: 0.287038  [  128/  569]\n",
      "loss: 0.358932  [  256/  569]\n",
      "loss: 0.337208  [  384/  569]\n",
      "loss: 0.462034  [  456/  569]\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.311016  [    0/  569]\n",
      "loss: 0.397581  [  128/  569]\n",
      "loss: 0.299609  [  256/  569]\n",
      "loss: 0.298364  [  384/  569]\n",
      "loss: 0.378289  [  456/  569]\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.312334  [    0/  569]\n",
      "loss: 0.353551  [  128/  569]\n",
      "loss: 0.305352  [  256/  569]\n",
      "loss: 0.291455  [  384/  569]\n",
      "loss: 0.240512  [  456/  569]\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.332325  [    0/  569]\n",
      "loss: 0.326637  [  128/  569]\n",
      "loss: 0.398106  [  256/  569]\n",
      "loss: 0.272811  [  384/  569]\n",
      "loss: 0.351101  [  456/  569]\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.343663  [    0/  569]\n",
      "loss: 0.312096  [  128/  569]\n",
      "loss: 0.319125  [  256/  569]\n",
      "loss: 0.327994  [  384/  569]\n",
      "loss: 0.407400  [  456/  569]\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.415499  [    0/  569]\n",
      "loss: 0.408082  [  128/  569]\n",
      "loss: 0.278386  [  256/  569]\n",
      "loss: 0.325793  [  384/  569]\n",
      "loss: 0.310143  [  456/  569]\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.335385  [    0/  569]\n",
      "loss: 0.337381  [  128/  569]\n",
      "loss: 0.329461  [  256/  569]\n",
      "loss: 0.317162  [  384/  569]\n",
      "loss: 0.323147  [  456/  569]\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.307942  [    0/  569]\n",
      "loss: 0.324715  [  128/  569]\n",
      "loss: 0.319726  [  256/  569]\n",
      "loss: 0.359326  [  384/  569]\n",
      "loss: 0.427911  [  456/  569]\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.441199  [    0/  569]\n",
      "loss: 0.376103  [  128/  569]\n",
      "loss: 0.323396  [  256/  569]\n",
      "loss: 0.368667  [  384/  569]\n",
      "loss: 0.286226  [  456/  569]\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.297683  [    0/  569]\n",
      "loss: 0.337590  [  128/  569]\n",
      "loss: 0.405261  [  256/  569]\n",
      "loss: 0.327615  [  384/  569]\n",
      "loss: 0.266766  [  456/  569]\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.297368  [    0/  569]\n",
      "loss: 0.321605  [  128/  569]\n",
      "loss: 0.350968  [  256/  569]\n",
      "loss: 0.352200  [  384/  569]\n",
      "loss: 0.418257  [  456/  569]\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.348915  [    0/  569]\n",
      "loss: 0.467789  [  128/  569]\n",
      "loss: 0.276832  [  256/  569]\n",
      "loss: 0.273453  [  384/  569]\n",
      "loss: 0.420494  [  456/  569]\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.388006  [    0/  569]\n",
      "loss: 0.278471  [  128/  569]\n",
      "loss: 0.296775  [  256/  569]\n",
      "loss: 0.354209  [  384/  569]\n",
      "loss: 0.291544  [  456/  569]\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.273191  [    0/  569]\n",
      "loss: 0.299786  [  128/  569]\n",
      "loss: 0.337888  [  256/  569]\n",
      "loss: 0.322872  [  384/  569]\n",
      "loss: 0.403521  [  456/  569]\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.282604  [    0/  569]\n",
      "loss: 0.333354  [  128/  569]\n",
      "loss: 0.295517  [  256/  569]\n",
      "loss: 0.331204  [  384/  569]\n",
      "loss: 0.271873  [  456/  569]\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.320202  [    0/  569]\n",
      "loss: 0.359374  [  128/  569]\n",
      "loss: 0.352780  [  256/  569]\n",
      "loss: 0.333496  [  384/  569]\n",
      "loss: 0.285903  [  456/  569]\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.365357  [    0/  569]\n",
      "loss: 0.391660  [  128/  569]\n",
      "loss: 0.291084  [  256/  569]\n",
      "loss: 0.312675  [  384/  569]\n",
      "loss: 0.344183  [  456/  569]\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.335882  [    0/  569]\n",
      "loss: 0.380423  [  128/  569]\n",
      "loss: 0.272620  [  256/  569]\n",
      "loss: 0.382520  [  384/  569]\n",
      "loss: 0.333070  [  456/  569]\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.311738  [    0/  569]\n",
      "loss: 0.385166  [  128/  569]\n",
      "loss: 0.279630  [  256/  569]\n",
      "loss: 0.310093  [  384/  569]\n",
      "loss: 0.377099  [  456/  569]\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.347657  [    0/  569]\n",
      "loss: 0.346128  [  128/  569]\n",
      "loss: 0.437483  [  256/  569]\n",
      "loss: 0.279984  [  384/  569]\n",
      "loss: 0.340231  [  456/  569]\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.379176  [    0/  569]\n",
      "loss: 0.344633  [  128/  569]\n",
      "loss: 0.375114  [  256/  569]\n",
      "loss: 0.311488  [  384/  569]\n",
      "loss: 0.327041  [  456/  569]\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.320083  [    0/  569]\n",
      "loss: 0.425694  [  128/  569]\n",
      "loss: 0.252029  [  256/  569]\n",
      "loss: 0.408303  [  384/  569]\n",
      "loss: 0.328292  [  456/  569]\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.390530  [    0/  569]\n",
      "loss: 0.374003  [  128/  569]\n",
      "loss: 0.326082  [  256/  569]\n",
      "loss: 0.365842  [  384/  569]\n",
      "loss: 0.309110  [  456/  569]\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.285416  [    0/  569]\n",
      "loss: 0.304352  [  128/  569]\n",
      "loss: 0.294471  [  256/  569]\n",
      "loss: 0.274332  [  384/  569]\n",
      "loss: 0.348087  [  456/  569]\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.349370  [    0/  569]\n",
      "loss: 0.355944  [  128/  569]\n",
      "loss: 0.323000  [  256/  569]\n",
      "loss: 0.341358  [  384/  569]\n",
      "loss: 0.295200  [  456/  569]\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.399185  [    0/  569]\n",
      "loss: 0.317048  [  128/  569]\n",
      "loss: 0.322619  [  256/  569]\n",
      "loss: 0.348350  [  384/  569]\n",
      "loss: 0.312137  [  456/  569]\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.257204  [    0/  569]\n",
      "loss: 0.337811  [  128/  569]\n",
      "loss: 0.286277  [  256/  569]\n",
      "loss: 0.338141  [  384/  569]\n",
      "loss: 0.369466  [  456/  569]\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.333809  [    0/  569]\n",
      "loss: 0.319317  [  128/  569]\n",
      "loss: 0.304210  [  256/  569]\n",
      "loss: 0.351892  [  384/  569]\n",
      "loss: 0.399858  [  456/  569]\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.364299  [    0/  569]\n",
      "loss: 0.360690  [  128/  569]\n",
      "loss: 0.333033  [  256/  569]\n",
      "loss: 0.310639  [  384/  569]\n",
      "loss: 0.356717  [  456/  569]\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.252502  [    0/  569]\n",
      "loss: 0.329868  [  128/  569]\n",
      "loss: 0.447958  [  256/  569]\n",
      "loss: 0.318025  [  384/  569]\n",
      "loss: 0.413228  [  456/  569]\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.362496  [    0/  569]\n",
      "loss: 0.379095  [  128/  569]\n",
      "loss: 0.337801  [  256/  569]\n",
      "loss: 0.287384  [  384/  569]\n",
      "loss: 0.405075  [  456/  569]\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.359526  [    0/  569]\n",
      "loss: 0.338918  [  128/  569]\n",
      "loss: 0.307908  [  256/  569]\n",
      "loss: 0.329769  [  384/  569]\n",
      "loss: 0.372319  [  456/  569]\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.353592  [    0/  569]\n",
      "loss: 0.383985  [  128/  569]\n",
      "loss: 0.324999  [  256/  569]\n",
      "loss: 0.319654  [  384/  569]\n",
      "loss: 0.303808  [  456/  569]\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.290066  [    0/  569]\n",
      "loss: 0.359032  [  128/  569]\n",
      "loss: 0.319602  [  256/  569]\n",
      "loss: 0.303044  [  384/  569]\n",
      "loss: 0.389688  [  456/  569]\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.424883  [    0/  569]\n",
      "loss: 0.313671  [  128/  569]\n",
      "loss: 0.348026  [  256/  569]\n",
      "loss: 0.321272  [  384/  569]\n",
      "loss: 0.369442  [  456/  569]\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.408979  [    0/  569]\n",
      "loss: 0.325976  [  128/  569]\n",
      "loss: 0.308423  [  256/  569]\n",
      "loss: 0.345981  [  384/  569]\n",
      "loss: 0.279112  [  456/  569]\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.338056  [    0/  569]\n",
      "loss: 0.432842  [  128/  569]\n",
      "loss: 0.231090  [  256/  569]\n",
      "loss: 0.405131  [  384/  569]\n",
      "loss: 0.304511  [  456/  569]\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.319275  [    0/  569]\n",
      "loss: 0.372727  [  128/  569]\n",
      "loss: 0.322922  [  256/  569]\n",
      "loss: 0.344762  [  384/  569]\n",
      "loss: 0.339050  [  456/  569]\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.370821  [    0/  569]\n",
      "loss: 0.276520  [  128/  569]\n",
      "loss: 0.290488  [  256/  569]\n",
      "loss: 0.339243  [  384/  569]\n",
      "loss: 0.278019  [  456/  569]\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.332924  [    0/  569]\n",
      "loss: 0.337579  [  128/  569]\n",
      "loss: 0.345125  [  256/  569]\n",
      "loss: 0.330659  [  384/  569]\n",
      "loss: 0.411644  [  456/  569]\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.296756  [    0/  569]\n",
      "loss: 0.319994  [  128/  569]\n",
      "loss: 0.340630  [  256/  569]\n",
      "loss: 0.299206  [  384/  569]\n",
      "loss: 0.342484  [  456/  569]\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.268541  [    0/  569]\n",
      "loss: 0.385286  [  128/  569]\n",
      "loss: 0.274293  [  256/  569]\n",
      "loss: 0.338307  [  384/  569]\n",
      "loss: 0.329067  [  456/  569]\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.364870  [    0/  569]\n",
      "loss: 0.368333  [  128/  569]\n",
      "loss: 0.255348  [  256/  569]\n",
      "loss: 0.291476  [  384/  569]\n",
      "loss: 0.292968  [  456/  569]\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.345737  [    0/  569]\n",
      "loss: 0.328421  [  128/  569]\n",
      "loss: 0.352370  [  256/  569]\n",
      "loss: 0.349498  [  384/  569]\n",
      "loss: 0.366730  [  456/  569]\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.337781  [    0/  569]\n",
      "loss: 0.337056  [  128/  569]\n",
      "loss: 0.326540  [  256/  569]\n",
      "loss: 0.367441  [  384/  569]\n",
      "loss: 0.333672  [  456/  569]\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.372671  [    0/  569]\n",
      "loss: 0.278648  [  128/  569]\n",
      "loss: 0.325627  [  256/  569]\n",
      "loss: 0.365551  [  384/  569]\n",
      "loss: 0.318779  [  456/  569]\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.246191  [    0/  569]\n",
      "loss: 0.277466  [  128/  569]\n",
      "loss: 0.315096  [  256/  569]\n",
      "loss: 0.363612  [  384/  569]\n",
      "loss: 0.306821  [  456/  569]\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.324401  [    0/  569]\n",
      "loss: 0.344102  [  128/  569]\n",
      "loss: 0.380496  [  256/  569]\n",
      "loss: 0.304871  [  384/  569]\n",
      "loss: 0.308066  [  456/  569]\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.386824  [    0/  569]\n",
      "loss: 0.401072  [  128/  569]\n",
      "loss: 0.401622  [  256/  569]\n",
      "loss: 0.308132  [  384/  569]\n",
      "loss: 0.335784  [  456/  569]\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.372120  [    0/  569]\n",
      "loss: 0.407808  [  128/  569]\n",
      "loss: 0.359553  [  256/  569]\n",
      "loss: 0.331018  [  384/  569]\n",
      "loss: 0.305389  [  456/  569]\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.304994  [    0/  569]\n",
      "loss: 0.328534  [  128/  569]\n",
      "loss: 0.367085  [  256/  569]\n",
      "loss: 0.400775  [  384/  569]\n",
      "loss: 0.265712  [  456/  569]\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.321870  [    0/  569]\n",
      "loss: 0.261411  [  128/  569]\n",
      "loss: 0.360540  [  256/  569]\n",
      "loss: 0.382929  [  384/  569]\n",
      "loss: 0.348917  [  456/  569]\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.276810  [    0/  569]\n",
      "loss: 0.371512  [  128/  569]\n",
      "loss: 0.367590  [  256/  569]\n",
      "loss: 0.294610  [  384/  569]\n",
      "loss: 0.360080  [  456/  569]\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.299325  [    0/  569]\n",
      "loss: 0.318149  [  128/  569]\n",
      "loss: 0.318696  [  256/  569]\n",
      "loss: 0.406495  [  384/  569]\n",
      "loss: 0.266680  [  456/  569]\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.382768  [    0/  569]\n",
      "loss: 0.339710  [  128/  569]\n",
      "loss: 0.319714  [  256/  569]\n",
      "loss: 0.341668  [  384/  569]\n",
      "loss: 0.355907  [  456/  569]\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.347816  [    0/  569]\n",
      "loss: 0.388188  [  128/  569]\n",
      "loss: 0.352391  [  256/  569]\n",
      "loss: 0.367196  [  384/  569]\n",
      "loss: 0.324270  [  456/  569]\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.351530  [    0/  569]\n",
      "loss: 0.372657  [  128/  569]\n",
      "loss: 0.385449  [  256/  569]\n",
      "loss: 0.302990  [  384/  569]\n",
      "loss: 0.350123  [  456/  569]\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.323433  [    0/  569]\n",
      "loss: 0.425533  [  128/  569]\n",
      "loss: 0.401293  [  256/  569]\n",
      "loss: 0.344421  [  384/  569]\n",
      "loss: 0.382725  [  456/  569]\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.370535  [    0/  569]\n",
      "loss: 0.316717  [  128/  569]\n",
      "loss: 0.391311  [  256/  569]\n",
      "loss: 0.308814  [  384/  569]\n",
      "loss: 0.380170  [  456/  569]\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.409985  [    0/  569]\n",
      "loss: 0.362778  [  128/  569]\n",
      "loss: 0.308341  [  256/  569]\n",
      "loss: 0.303430  [  384/  569]\n",
      "loss: 0.334228  [  456/  569]\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.380885  [    0/  569]\n",
      "loss: 0.346218  [  128/  569]\n",
      "loss: 0.355295  [  256/  569]\n",
      "loss: 0.336154  [  384/  569]\n",
      "loss: 0.363528  [  456/  569]\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.374106  [    0/  569]\n",
      "loss: 0.356522  [  128/  569]\n",
      "loss: 0.273110  [  256/  569]\n",
      "loss: 0.422913  [  384/  569]\n",
      "loss: 0.314723  [  456/  569]\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.313021  [    0/  569]\n",
      "loss: 0.353252  [  128/  569]\n",
      "loss: 0.363715  [  256/  569]\n",
      "loss: 0.266380  [  384/  569]\n",
      "loss: 0.291459  [  456/  569]\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.375249  [    0/  569]\n",
      "loss: 0.292873  [  128/  569]\n",
      "loss: 0.351122  [  256/  569]\n",
      "loss: 0.372150  [  384/  569]\n",
      "loss: 0.381195  [  456/  569]\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.318563  [    0/  569]\n",
      "loss: 0.314821  [  128/  569]\n",
      "loss: 0.304334  [  256/  569]\n",
      "loss: 0.322896  [  384/  569]\n",
      "loss: 0.335865  [  456/  569]\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.329721  [    0/  569]\n",
      "loss: 0.254839  [  128/  569]\n",
      "loss: 0.444847  [  256/  569]\n",
      "loss: 0.296547  [  384/  569]\n",
      "loss: 0.343192  [  456/  569]\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.313985  [    0/  569]\n",
      "loss: 0.313653  [  128/  569]\n",
      "loss: 0.379174  [  256/  569]\n",
      "loss: 0.362247  [  384/  569]\n",
      "loss: 0.335612  [  456/  569]\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.361900  [    0/  569]\n",
      "loss: 0.365401  [  128/  569]\n",
      "loss: 0.314249  [  256/  569]\n",
      "loss: 0.360892  [  384/  569]\n",
      "loss: 0.276056  [  456/  569]\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.364475  [    0/  569]\n",
      "loss: 0.268122  [  128/  569]\n",
      "loss: 0.402062  [  256/  569]\n",
      "loss: 0.297691  [  384/  569]\n",
      "loss: 0.350810  [  456/  569]\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.325746  [    0/  569]\n",
      "loss: 0.379854  [  128/  569]\n",
      "loss: 0.323578  [  256/  569]\n",
      "loss: 0.323054  [  384/  569]\n",
      "loss: 0.309844  [  456/  569]\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.320395  [    0/  569]\n",
      "loss: 0.369651  [  128/  569]\n",
      "loss: 0.314275  [  256/  569]\n",
      "loss: 0.327437  [  384/  569]\n",
      "loss: 0.361708  [  456/  569]\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.344079  [    0/  569]\n",
      "loss: 0.308809  [  128/  569]\n",
      "loss: 0.328795  [  256/  569]\n",
      "loss: 0.384797  [  384/  569]\n",
      "loss: 0.302017  [  456/  569]\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.309445  [    0/  569]\n",
      "loss: 0.333088  [  128/  569]\n",
      "loss: 0.386742  [  256/  569]\n",
      "loss: 0.392873  [  384/  569]\n",
      "loss: 0.313464  [  456/  569]\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.344779  [    0/  569]\n",
      "loss: 0.329387  [  128/  569]\n",
      "loss: 0.338804  [  256/  569]\n",
      "loss: 0.379662  [  384/  569]\n",
      "loss: 0.377524  [  456/  569]\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.332358  [    0/  569]\n",
      "loss: 0.261573  [  128/  569]\n",
      "loss: 0.368101  [  256/  569]\n",
      "loss: 0.317216  [  384/  569]\n",
      "loss: 0.368536  [  456/  569]\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.319260  [    0/  569]\n",
      "loss: 0.319193  [  128/  569]\n",
      "loss: 0.413605  [  256/  569]\n",
      "loss: 0.309055  [  384/  569]\n",
      "loss: 0.379756  [  456/  569]\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.339484  [    0/  569]\n",
      "loss: 0.367932  [  128/  569]\n",
      "loss: 0.290905  [  256/  569]\n",
      "loss: 0.381226  [  384/  569]\n",
      "loss: 0.279060  [  456/  569]\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.355936  [    0/  569]\n",
      "loss: 0.332743  [  128/  569]\n",
      "loss: 0.388969  [  256/  569]\n",
      "loss: 0.266904  [  384/  569]\n",
      "loss: 0.302669  [  456/  569]\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.304715  [    0/  569]\n",
      "loss: 0.397953  [  128/  569]\n",
      "loss: 0.372387  [  256/  569]\n",
      "loss: 0.299180  [  384/  569]\n",
      "loss: 0.339874  [  456/  569]\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.255746  [    0/  569]\n",
      "loss: 0.370205  [  128/  569]\n",
      "loss: 0.240084  [  256/  569]\n",
      "loss: 0.353124  [  384/  569]\n",
      "loss: 0.370401  [  456/  569]\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.369307  [    0/  569]\n",
      "loss: 0.273464  [  128/  569]\n",
      "loss: 0.402703  [  256/  569]\n",
      "loss: 0.282139  [  384/  569]\n",
      "loss: 0.321228  [  456/  569]\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.348315  [    0/  569]\n",
      "loss: 0.307909  [  128/  569]\n",
      "loss: 0.321642  [  256/  569]\n",
      "loss: 0.264240  [  384/  569]\n",
      "loss: 0.333875  [  456/  569]\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.305970  [    0/  569]\n",
      "loss: 0.289043  [  128/  569]\n",
      "loss: 0.313421  [  256/  569]\n",
      "loss: 0.389399  [  384/  569]\n",
      "loss: 0.323029  [  456/  569]\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.336675  [    0/  569]\n",
      "loss: 0.284296  [  128/  569]\n",
      "loss: 0.293292  [  256/  569]\n",
      "loss: 0.391500  [  384/  569]\n",
      "loss: 0.456277  [  456/  569]\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.293476  [    0/  569]\n",
      "loss: 0.304405  [  128/  569]\n",
      "loss: 0.404925  [  256/  569]\n",
      "loss: 0.312095  [  384/  569]\n",
      "loss: 0.327811  [  456/  569]\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.352816  [    0/  569]\n",
      "loss: 0.257235  [  128/  569]\n",
      "loss: 0.288512  [  256/  569]\n",
      "loss: 0.306959  [  384/  569]\n",
      "loss: 0.355955  [  456/  569]\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.339186  [    0/  569]\n",
      "loss: 0.341633  [  128/  569]\n",
      "loss: 0.408333  [  256/  569]\n",
      "loss: 0.305444  [  384/  569]\n",
      "loss: 0.268425  [  456/  569]\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.418147  [    0/  569]\n",
      "loss: 0.311631  [  128/  569]\n",
      "loss: 0.322303  [  256/  569]\n",
      "loss: 0.342704  [  384/  569]\n",
      "loss: 0.311017  [  456/  569]\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.309882  [    0/  569]\n",
      "loss: 0.367528  [  128/  569]\n",
      "loss: 0.414322  [  256/  569]\n",
      "loss: 0.345173  [  384/  569]\n",
      "loss: 0.321523  [  456/  569]\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.323217  [    0/  569]\n",
      "loss: 0.341160  [  128/  569]\n",
      "loss: 0.380877  [  256/  569]\n",
      "loss: 0.336906  [  384/  569]\n",
      "loss: 0.256646  [  456/  569]\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.370195  [    0/  569]\n",
      "loss: 0.347796  [  128/  569]\n",
      "loss: 0.391812  [  256/  569]\n",
      "loss: 0.268494  [  384/  569]\n",
      "loss: 0.291690  [  456/  569]\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.410179  [    0/  569]\n",
      "loss: 0.331990  [  128/  569]\n",
      "loss: 0.433407  [  256/  569]\n",
      "loss: 0.330211  [  384/  569]\n",
      "loss: 0.313092  [  456/  569]\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.298785  [    0/  569]\n",
      "loss: 0.300660  [  128/  569]\n",
      "loss: 0.291897  [  256/  569]\n",
      "loss: 0.352912  [  384/  569]\n",
      "loss: 0.277424  [  456/  569]\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.290939  [    0/  569]\n",
      "loss: 0.325959  [  128/  569]\n",
      "loss: 0.448654  [  256/  569]\n",
      "loss: 0.295491  [  384/  569]\n",
      "loss: 0.339031  [  456/  569]\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.358684  [    0/  569]\n",
      "loss: 0.311448  [  128/  569]\n",
      "loss: 0.401664  [  256/  569]\n",
      "loss: 0.277270  [  384/  569]\n",
      "loss: 0.350220  [  456/  569]\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.346482  [    0/  569]\n",
      "loss: 0.306957  [  128/  569]\n",
      "loss: 0.365811  [  256/  569]\n",
      "loss: 0.235674  [  384/  569]\n",
      "loss: 0.382624  [  456/  569]\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.337421  [    0/  569]\n",
      "loss: 0.427099  [  128/  569]\n",
      "loss: 0.349581  [  256/  569]\n",
      "loss: 0.260997  [  384/  569]\n",
      "loss: 0.290845  [  456/  569]\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.352032  [    0/  569]\n",
      "loss: 0.354643  [  128/  569]\n",
      "loss: 0.408038  [  256/  569]\n",
      "loss: 0.296937  [  384/  569]\n",
      "loss: 0.337875  [  456/  569]\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.338351  [    0/  569]\n",
      "loss: 0.335905  [  128/  569]\n",
      "loss: 0.335351  [  256/  569]\n",
      "loss: 0.299811  [  384/  569]\n",
      "loss: 0.462906  [  456/  569]\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.358302  [    0/  569]\n",
      "loss: 0.375035  [  128/  569]\n",
      "loss: 0.409400  [  256/  569]\n",
      "loss: 0.279638  [  384/  569]\n",
      "loss: 0.354933  [  456/  569]\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.388068  [    0/  569]\n",
      "loss: 0.336655  [  128/  569]\n",
      "loss: 0.338333  [  256/  569]\n",
      "loss: 0.313106  [  384/  569]\n",
      "loss: 0.275776  [  456/  569]\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.303427  [    0/  569]\n",
      "loss: 0.330520  [  128/  569]\n",
      "loss: 0.379705  [  256/  569]\n",
      "loss: 0.378018  [  384/  569]\n",
      "loss: 0.343365  [  456/  569]\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.341729  [    0/  569]\n",
      "loss: 0.364397  [  128/  569]\n",
      "loss: 0.395421  [  256/  569]\n",
      "loss: 0.364469  [  384/  569]\n",
      "loss: 0.364713  [  456/  569]\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.291358  [    0/  569]\n",
      "loss: 0.286382  [  128/  569]\n",
      "loss: 0.315428  [  256/  569]\n",
      "loss: 0.391976  [  384/  569]\n",
      "loss: 0.366330  [  456/  569]\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.279012  [    0/  569]\n",
      "loss: 0.344650  [  128/  569]\n",
      "loss: 0.264662  [  256/  569]\n",
      "loss: 0.340088  [  384/  569]\n",
      "loss: 0.336536  [  456/  569]\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.345855  [    0/  569]\n",
      "loss: 0.344467  [  128/  569]\n",
      "loss: 0.435167  [  256/  569]\n",
      "loss: 0.324837  [  384/  569]\n",
      "loss: 0.366636  [  456/  569]\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.276410  [    0/  569]\n",
      "loss: 0.404839  [  128/  569]\n",
      "loss: 0.365285  [  256/  569]\n",
      "loss: 0.312199  [  384/  569]\n",
      "loss: 0.344221  [  456/  569]\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.396760  [    0/  569]\n",
      "loss: 0.322332  [  128/  569]\n",
      "loss: 0.383083  [  256/  569]\n",
      "loss: 0.302023  [  384/  569]\n",
      "loss: 0.317222  [  456/  569]\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.425509  [    0/  569]\n",
      "loss: 0.328669  [  128/  569]\n",
      "loss: 0.348082  [  256/  569]\n",
      "loss: 0.318361  [  384/  569]\n",
      "loss: 0.290553  [  456/  569]\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.380765  [    0/  569]\n",
      "loss: 0.400277  [  128/  569]\n",
      "loss: 0.456524  [  256/  569]\n",
      "loss: 0.300059  [  384/  569]\n",
      "loss: 0.240830  [  456/  569]\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.313269  [    0/  569]\n",
      "loss: 0.347750  [  128/  569]\n",
      "loss: 0.349539  [  256/  569]\n",
      "loss: 0.342937  [  384/  569]\n",
      "loss: 0.288008  [  456/  569]\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.363076  [    0/  569]\n",
      "loss: 0.402843  [  128/  569]\n",
      "loss: 0.288043  [  256/  569]\n",
      "loss: 0.311904  [  384/  569]\n",
      "loss: 0.334192  [  456/  569]\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.265426  [    0/  569]\n",
      "loss: 0.313482  [  128/  569]\n",
      "loss: 0.435311  [  256/  569]\n",
      "loss: 0.351935  [  384/  569]\n",
      "loss: 0.381474  [  456/  569]\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.313772  [    0/  569]\n",
      "loss: 0.307382  [  128/  569]\n",
      "loss: 0.325441  [  256/  569]\n",
      "loss: 0.403848  [  384/  569]\n",
      "loss: 0.333363  [  456/  569]\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.359601  [    0/  569]\n",
      "loss: 0.360384  [  128/  569]\n",
      "loss: 0.272048  [  256/  569]\n",
      "loss: 0.397161  [  384/  569]\n",
      "loss: 0.361897  [  456/  569]\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.336917  [    0/  569]\n",
      "loss: 0.382770  [  128/  569]\n",
      "loss: 0.342213  [  256/  569]\n",
      "loss: 0.377182  [  384/  569]\n",
      "loss: 0.310342  [  456/  569]\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.318955  [    0/  569]\n",
      "loss: 0.362951  [  128/  569]\n",
      "loss: 0.354066  [  256/  569]\n",
      "loss: 0.314289  [  384/  569]\n",
      "loss: 0.356450  [  456/  569]\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.350187  [    0/  569]\n",
      "loss: 0.331093  [  128/  569]\n",
      "loss: 0.294420  [  256/  569]\n",
      "loss: 0.379452  [  384/  569]\n",
      "loss: 0.306219  [  456/  569]\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.407646  [    0/  569]\n",
      "loss: 0.378165  [  128/  569]\n",
      "loss: 0.360050  [  256/  569]\n",
      "loss: 0.337087  [  384/  569]\n",
      "loss: 0.331214  [  456/  569]\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.345672  [    0/  569]\n",
      "loss: 0.338518  [  128/  569]\n",
      "loss: 0.408817  [  256/  569]\n",
      "loss: 0.402806  [  384/  569]\n",
      "loss: 0.298050  [  456/  569]\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.344684  [    0/  569]\n",
      "loss: 0.380958  [  128/  569]\n",
      "loss: 0.281546  [  256/  569]\n",
      "loss: 0.327286  [  384/  569]\n",
      "loss: 0.322304  [  456/  569]\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.332086  [    0/  569]\n",
      "loss: 0.342876  [  128/  569]\n",
      "loss: 0.358769  [  256/  569]\n",
      "loss: 0.297838  [  384/  569]\n",
      "loss: 0.347484  [  456/  569]\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.340528  [    0/  569]\n",
      "loss: 0.393631  [  128/  569]\n",
      "loss: 0.366787  [  256/  569]\n",
      "loss: 0.310580  [  384/  569]\n",
      "loss: 0.322044  [  456/  569]\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.365211  [    0/  569]\n",
      "loss: 0.290122  [  128/  569]\n",
      "loss: 0.336200  [  256/  569]\n",
      "loss: 0.389337  [  384/  569]\n",
      "loss: 0.363960  [  456/  569]\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.287979  [    0/  569]\n",
      "loss: 0.384785  [  128/  569]\n",
      "loss: 0.272807  [  256/  569]\n",
      "loss: 0.325532  [  384/  569]\n",
      "loss: 0.310202  [  456/  569]\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.276948  [    0/  569]\n",
      "loss: 0.383837  [  128/  569]\n",
      "loss: 0.379507  [  256/  569]\n",
      "loss: 0.323907  [  384/  569]\n",
      "loss: 0.322789  [  456/  569]\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.348533  [    0/  569]\n",
      "loss: 0.355808  [  128/  569]\n",
      "loss: 0.366025  [  256/  569]\n",
      "loss: 0.367983  [  384/  569]\n",
      "loss: 0.335196  [  456/  569]\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.344449  [    0/  569]\n",
      "loss: 0.370481  [  128/  569]\n",
      "loss: 0.431274  [  256/  569]\n",
      "loss: 0.382753  [  384/  569]\n",
      "loss: 0.305191  [  456/  569]\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.358139  [    0/  569]\n",
      "loss: 0.332444  [  128/  569]\n",
      "loss: 0.322053  [  256/  569]\n",
      "loss: 0.348183  [  384/  569]\n",
      "loss: 0.309114  [  456/  569]\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.322462  [    0/  569]\n",
      "loss: 0.327776  [  128/  569]\n",
      "loss: 0.344578  [  256/  569]\n",
      "loss: 0.407575  [  384/  569]\n",
      "loss: 0.423053  [  456/  569]\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.373357  [    0/  569]\n",
      "loss: 0.322577  [  128/  569]\n",
      "loss: 0.304037  [  256/  569]\n",
      "loss: 0.301405  [  384/  569]\n",
      "loss: 0.388738  [  456/  569]\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.363721  [    0/  569]\n",
      "loss: 0.305719  [  128/  569]\n",
      "loss: 0.330031  [  256/  569]\n",
      "loss: 0.344564  [  384/  569]\n",
      "loss: 0.288187  [  456/  569]\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.506213  [    0/  569]\n",
      "loss: 0.365657  [  128/  569]\n",
      "loss: 0.315742  [  256/  569]\n",
      "loss: 0.380309  [  384/  569]\n",
      "loss: 0.334528  [  456/  569]\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.311713  [    0/  569]\n",
      "loss: 0.363958  [  128/  569]\n",
      "loss: 0.351062  [  256/  569]\n",
      "loss: 0.277042  [  384/  569]\n",
      "loss: 0.313372  [  456/  569]\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.300726  [    0/  569]\n",
      "loss: 0.392355  [  128/  569]\n",
      "loss: 0.341043  [  256/  569]\n",
      "loss: 0.290646  [  384/  569]\n",
      "loss: 0.426106  [  456/  569]\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.307578  [    0/  569]\n",
      "loss: 0.332430  [  128/  569]\n",
      "loss: 0.362994  [  256/  569]\n",
      "loss: 0.383191  [  384/  569]\n",
      "loss: 0.341578  [  456/  569]\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.291551  [    0/  569]\n",
      "loss: 0.360109  [  128/  569]\n",
      "loss: 0.456600  [  256/  569]\n",
      "loss: 0.268465  [  384/  569]\n",
      "loss: 0.277461  [  456/  569]\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.304040  [    0/  569]\n",
      "loss: 0.370989  [  128/  569]\n",
      "loss: 0.381599  [  256/  569]\n",
      "loss: 0.374840  [  384/  569]\n",
      "loss: 0.339799  [  456/  569]\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.343056  [    0/  569]\n",
      "loss: 0.291236  [  128/  569]\n",
      "loss: 0.299856  [  256/  569]\n",
      "loss: 0.308545  [  384/  569]\n",
      "loss: 0.341009  [  456/  569]\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.298950  [    0/  569]\n",
      "loss: 0.332359  [  128/  569]\n",
      "loss: 0.339454  [  256/  569]\n",
      "loss: 0.308471  [  384/  569]\n",
      "loss: 0.352694  [  456/  569]\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.327642  [    0/  569]\n",
      "loss: 0.310684  [  128/  569]\n",
      "loss: 0.428260  [  256/  569]\n",
      "loss: 0.378401  [  384/  569]\n",
      "loss: 0.295718  [  456/  569]\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.356320  [    0/  569]\n",
      "loss: 0.345064  [  128/  569]\n",
      "loss: 0.315724  [  256/  569]\n",
      "loss: 0.359990  [  384/  569]\n",
      "loss: 0.399485  [  456/  569]\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.363914  [    0/  569]\n",
      "loss: 0.312730  [  128/  569]\n",
      "loss: 0.363612  [  256/  569]\n",
      "loss: 0.356067  [  384/  569]\n",
      "loss: 0.369677  [  456/  569]\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.365832  [    0/  569]\n",
      "loss: 0.359642  [  128/  569]\n",
      "loss: 0.275561  [  256/  569]\n",
      "loss: 0.313525  [  384/  569]\n",
      "loss: 0.289085  [  456/  569]\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.302737  [    0/  569]\n",
      "loss: 0.329076  [  128/  569]\n",
      "loss: 0.320687  [  256/  569]\n",
      "loss: 0.377232  [  384/  569]\n",
      "loss: 0.279538  [  456/  569]\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.376786  [    0/  569]\n",
      "loss: 0.321873  [  128/  569]\n",
      "loss: 0.292349  [  256/  569]\n",
      "loss: 0.350111  [  384/  569]\n",
      "loss: 0.369081  [  456/  569]\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.379287  [    0/  569]\n",
      "loss: 0.292070  [  128/  569]\n",
      "loss: 0.278810  [  256/  569]\n",
      "loss: 0.334372  [  384/  569]\n",
      "loss: 0.388112  [  456/  569]\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.298583  [    0/  569]\n",
      "loss: 0.246642  [  128/  569]\n",
      "loss: 0.311890  [  256/  569]\n",
      "loss: 0.392437  [  384/  569]\n",
      "loss: 0.352333  [  456/  569]\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.302976  [    0/  569]\n",
      "loss: 0.340894  [  128/  569]\n",
      "loss: 0.340429  [  256/  569]\n",
      "loss: 0.365237  [  384/  569]\n",
      "loss: 0.434859  [  456/  569]\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.388948  [    0/  569]\n",
      "loss: 0.318825  [  128/  569]\n",
      "loss: 0.372077  [  256/  569]\n",
      "loss: 0.349653  [  384/  569]\n",
      "loss: 0.255573  [  456/  569]\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.317452  [    0/  569]\n",
      "loss: 0.296154  [  128/  569]\n",
      "loss: 0.390604  [  256/  569]\n",
      "loss: 0.291446  [  384/  569]\n",
      "loss: 0.395037  [  456/  569]\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.440100  [    0/  569]\n",
      "loss: 0.316247  [  128/  569]\n",
      "loss: 0.304445  [  256/  569]\n",
      "loss: 0.316760  [  384/  569]\n",
      "loss: 0.369970  [  456/  569]\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.348196  [    0/  569]\n",
      "loss: 0.322496  [  128/  569]\n",
      "loss: 0.382229  [  256/  569]\n",
      "loss: 0.293450  [  384/  569]\n",
      "loss: 0.375932  [  456/  569]\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.261788  [    0/  569]\n",
      "loss: 0.324120  [  128/  569]\n",
      "loss: 0.279701  [  256/  569]\n",
      "loss: 0.401242  [  384/  569]\n",
      "loss: 0.414057  [  456/  569]\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.357960  [    0/  569]\n",
      "loss: 0.323120  [  128/  569]\n",
      "loss: 0.362753  [  256/  569]\n",
      "loss: 0.411103  [  384/  569]\n",
      "loss: 0.332369  [  456/  569]\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.345844  [    0/  569]\n",
      "loss: 0.294347  [  128/  569]\n",
      "loss: 0.254461  [  256/  569]\n",
      "loss: 0.319860  [  384/  569]\n",
      "loss: 0.358713  [  456/  569]\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.344038  [    0/  569]\n",
      "loss: 0.355031  [  128/  569]\n",
      "loss: 0.374396  [  256/  569]\n",
      "loss: 0.415936  [  384/  569]\n",
      "loss: 0.343554  [  456/  569]\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.275463  [    0/  569]\n",
      "loss: 0.327612  [  128/  569]\n",
      "loss: 0.339158  [  256/  569]\n",
      "loss: 0.399027  [  384/  569]\n",
      "loss: 0.331727  [  456/  569]\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.357347  [    0/  569]\n",
      "loss: 0.330334  [  128/  569]\n",
      "loss: 0.342915  [  256/  569]\n",
      "loss: 0.306533  [  384/  569]\n",
      "loss: 0.339451  [  456/  569]\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.335135  [    0/  569]\n",
      "loss: 0.300384  [  128/  569]\n",
      "loss: 0.357217  [  256/  569]\n",
      "loss: 0.316586  [  384/  569]\n",
      "loss: 0.395273  [  456/  569]\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.344455  [    0/  569]\n",
      "loss: 0.296070  [  128/  569]\n",
      "loss: 0.353228  [  256/  569]\n",
      "loss: 0.355215  [  384/  569]\n",
      "loss: 0.341195  [  456/  569]\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.335164  [    0/  569]\n",
      "loss: 0.391346  [  128/  569]\n",
      "loss: 0.405452  [  256/  569]\n",
      "loss: 0.383961  [  384/  569]\n",
      "loss: 0.261900  [  456/  569]\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.429399  [    0/  569]\n",
      "loss: 0.309847  [  128/  569]\n",
      "loss: 0.311360  [  256/  569]\n",
      "loss: 0.257084  [  384/  569]\n",
      "loss: 0.332022  [  456/  569]\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.349348  [    0/  569]\n",
      "loss: 0.400115  [  128/  569]\n",
      "loss: 0.238483  [  256/  569]\n",
      "loss: 0.307531  [  384/  569]\n",
      "loss: 0.394092  [  456/  569]\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.423757  [    0/  569]\n",
      "loss: 0.350132  [  128/  569]\n",
      "loss: 0.257259  [  256/  569]\n",
      "loss: 0.388342  [  384/  569]\n",
      "loss: 0.408095  [  456/  569]\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.304672  [    0/  569]\n",
      "loss: 0.423130  [  128/  569]\n",
      "loss: 0.418377  [  256/  569]\n",
      "loss: 0.250878  [  384/  569]\n",
      "loss: 0.365061  [  456/  569]\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.271667  [    0/  569]\n",
      "loss: 0.383038  [  128/  569]\n",
      "loss: 0.351087  [  256/  569]\n",
      "loss: 0.388425  [  384/  569]\n",
      "loss: 0.347006  [  456/  569]\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.361249  [    0/  569]\n",
      "loss: 0.386840  [  128/  569]\n",
      "loss: 0.419746  [  256/  569]\n",
      "loss: 0.345582  [  384/  569]\n",
      "loss: 0.264247  [  456/  569]\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.395264  [    0/  569]\n",
      "loss: 0.286808  [  128/  569]\n",
      "loss: 0.322019  [  256/  569]\n",
      "loss: 0.323739  [  384/  569]\n",
      "loss: 0.300266  [  456/  569]\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.320080  [    0/  569]\n",
      "loss: 0.321158  [  128/  569]\n",
      "loss: 0.310602  [  256/  569]\n",
      "loss: 0.386968  [  384/  569]\n",
      "loss: 0.396458  [  456/  569]\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.308945  [    0/  569]\n",
      "loss: 0.366143  [  128/  569]\n",
      "loss: 0.357906  [  256/  569]\n",
      "loss: 0.275060  [  384/  569]\n",
      "loss: 0.275150  [  456/  569]\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.304121  [    0/  569]\n",
      "loss: 0.324097  [  128/  569]\n",
      "loss: 0.262918  [  256/  569]\n",
      "loss: 0.408859  [  384/  569]\n",
      "loss: 0.348966  [  456/  569]\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.341811  [    0/  569]\n",
      "loss: 0.341673  [  128/  569]\n",
      "loss: 0.358276  [  256/  569]\n",
      "loss: 0.315018  [  384/  569]\n",
      "loss: 0.329599  [  456/  569]\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.337600  [    0/  569]\n",
      "loss: 0.351707  [  128/  569]\n",
      "loss: 0.304301  [  256/  569]\n",
      "loss: 0.343153  [  384/  569]\n",
      "loss: 0.331202  [  456/  569]\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.330237  [    0/  569]\n",
      "loss: 0.303024  [  128/  569]\n",
      "loss: 0.280654  [  256/  569]\n",
      "loss: 0.351776  [  384/  569]\n",
      "loss: 0.319548  [  456/  569]\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.393115  [    0/  569]\n",
      "loss: 0.345553  [  128/  569]\n",
      "loss: 0.345835  [  256/  569]\n",
      "loss: 0.290310  [  384/  569]\n",
      "loss: 0.312535  [  456/  569]\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.354968  [    0/  569]\n",
      "loss: 0.274285  [  128/  569]\n",
      "loss: 0.359964  [  256/  569]\n",
      "loss: 0.286448  [  384/  569]\n",
      "loss: 0.398242  [  456/  569]\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.365199  [    0/  569]\n",
      "loss: 0.319737  [  128/  569]\n",
      "loss: 0.329928  [  256/  569]\n",
      "loss: 0.283361  [  384/  569]\n",
      "loss: 0.409173  [  456/  569]\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.341357  [    0/  569]\n",
      "loss: 0.325245  [  128/  569]\n",
      "loss: 0.274371  [  256/  569]\n",
      "loss: 0.363867  [  384/  569]\n",
      "loss: 0.340415  [  456/  569]\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.382766  [    0/  569]\n",
      "loss: 0.359038  [  128/  569]\n",
      "loss: 0.319740  [  256/  569]\n",
      "loss: 0.382579  [  384/  569]\n",
      "loss: 0.246810  [  456/  569]\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.362115  [    0/  569]\n",
      "loss: 0.360744  [  128/  569]\n",
      "loss: 0.330995  [  256/  569]\n",
      "loss: 0.331650  [  384/  569]\n",
      "loss: 0.357249  [  456/  569]\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.256063  [    0/  569]\n",
      "loss: 0.403308  [  128/  569]\n",
      "loss: 0.334949  [  256/  569]\n",
      "loss: 0.320399  [  384/  569]\n",
      "loss: 0.298747  [  456/  569]\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.315390  [    0/  569]\n",
      "loss: 0.327514  [  128/  569]\n",
      "loss: 0.338978  [  256/  569]\n",
      "loss: 0.303309  [  384/  569]\n",
      "loss: 0.427104  [  456/  569]\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.378850  [    0/  569]\n",
      "loss: 0.269203  [  128/  569]\n",
      "loss: 0.361820  [  256/  569]\n",
      "loss: 0.338949  [  384/  569]\n",
      "loss: 0.295131  [  456/  569]\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.381902  [    0/  569]\n",
      "loss: 0.381312  [  128/  569]\n",
      "loss: 0.297928  [  256/  569]\n",
      "loss: 0.358499  [  384/  569]\n",
      "loss: 0.333572  [  456/  569]\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.317470  [    0/  569]\n",
      "loss: 0.437982  [  128/  569]\n",
      "loss: 0.296085  [  256/  569]\n",
      "loss: 0.351297  [  384/  569]\n",
      "loss: 0.266837  [  456/  569]\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.306456  [    0/  569]\n",
      "loss: 0.336786  [  128/  569]\n",
      "loss: 0.371814  [  256/  569]\n",
      "loss: 0.406125  [  384/  569]\n",
      "loss: 0.290343  [  456/  569]\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.339670  [    0/  569]\n",
      "loss: 0.316897  [  128/  569]\n",
      "loss: 0.268340  [  256/  569]\n",
      "loss: 0.371236  [  384/  569]\n",
      "loss: 0.379021  [  456/  569]\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.361524  [    0/  569]\n",
      "loss: 0.290784  [  128/  569]\n",
      "loss: 0.383990  [  256/  569]\n",
      "loss: 0.326140  [  384/  569]\n",
      "loss: 0.334538  [  456/  569]\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.370449  [    0/  569]\n",
      "loss: 0.265949  [  128/  569]\n",
      "loss: 0.284691  [  256/  569]\n",
      "loss: 0.385779  [  384/  569]\n",
      "loss: 0.327920  [  456/  569]\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.265054  [    0/  569]\n",
      "loss: 0.374820  [  128/  569]\n",
      "loss: 0.340028  [  256/  569]\n",
      "loss: 0.377611  [  384/  569]\n",
      "loss: 0.327166  [  456/  569]\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.365664  [    0/  569]\n",
      "loss: 0.303965  [  128/  569]\n",
      "loss: 0.291750  [  256/  569]\n",
      "loss: 0.342128  [  384/  569]\n",
      "loss: 0.310524  [  456/  569]\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.412413  [    0/  569]\n",
      "loss: 0.307095  [  128/  569]\n",
      "loss: 0.310688  [  256/  569]\n",
      "loss: 0.351644  [  384/  569]\n",
      "loss: 0.354821  [  456/  569]\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.309152  [    0/  569]\n",
      "loss: 0.323755  [  128/  569]\n",
      "loss: 0.343844  [  256/  569]\n",
      "loss: 0.323677  [  384/  569]\n",
      "loss: 0.401637  [  456/  569]\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.411120  [    0/  569]\n",
      "loss: 0.383798  [  128/  569]\n",
      "loss: 0.308716  [  256/  569]\n",
      "loss: 0.331667  [  384/  569]\n",
      "loss: 0.345388  [  456/  569]\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.334917  [    0/  569]\n",
      "loss: 0.330737  [  128/  569]\n",
      "loss: 0.417155  [  256/  569]\n",
      "loss: 0.281207  [  384/  569]\n",
      "loss: 0.342772  [  456/  569]\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.353222  [    0/  569]\n",
      "loss: 0.354632  [  128/  569]\n",
      "loss: 0.383287  [  256/  569]\n",
      "loss: 0.355728  [  384/  569]\n",
      "loss: 0.314940  [  456/  569]\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.307583  [    0/  569]\n",
      "loss: 0.310966  [  128/  569]\n",
      "loss: 0.328592  [  256/  569]\n",
      "loss: 0.391440  [  384/  569]\n",
      "loss: 0.347951  [  456/  569]\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.350663  [    0/  569]\n",
      "loss: 0.314374  [  128/  569]\n",
      "loss: 0.325474  [  256/  569]\n",
      "loss: 0.324926  [  384/  569]\n",
      "loss: 0.349194  [  456/  569]\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.394632  [    0/  569]\n",
      "loss: 0.323056  [  128/  569]\n",
      "loss: 0.319041  [  256/  569]\n",
      "loss: 0.353382  [  384/  569]\n",
      "loss: 0.359090  [  456/  569]\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.348178  [    0/  569]\n",
      "loss: 0.289907  [  128/  569]\n",
      "loss: 0.354173  [  256/  569]\n",
      "loss: 0.287147  [  384/  569]\n",
      "loss: 0.337346  [  456/  569]\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.294789  [    0/  569]\n",
      "loss: 0.319117  [  128/  569]\n",
      "loss: 0.264138  [  256/  569]\n",
      "loss: 0.376917  [  384/  569]\n",
      "loss: 0.388399  [  456/  569]\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.293988  [    0/  569]\n",
      "loss: 0.332696  [  128/  569]\n",
      "loss: 0.338572  [  256/  569]\n",
      "loss: 0.279411  [  384/  569]\n",
      "loss: 0.402800  [  456/  569]\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.331375  [    0/  569]\n",
      "loss: 0.328095  [  128/  569]\n",
      "loss: 0.335499  [  256/  569]\n",
      "loss: 0.353856  [  384/  569]\n",
      "loss: 0.354703  [  456/  569]\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.231795  [    0/  569]\n",
      "loss: 0.349889  [  128/  569]\n",
      "loss: 0.328718  [  256/  569]\n",
      "loss: 0.382981  [  384/  569]\n",
      "loss: 0.302205  [  456/  569]\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.328883  [    0/  569]\n",
      "loss: 0.314665  [  128/  569]\n",
      "loss: 0.283981  [  256/  569]\n",
      "loss: 0.400458  [  384/  569]\n",
      "loss: 0.434789  [  456/  569]\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.339672  [    0/  569]\n",
      "loss: 0.310955  [  128/  569]\n",
      "loss: 0.376319  [  256/  569]\n",
      "loss: 0.322954  [  384/  569]\n",
      "loss: 0.266217  [  456/  569]\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.336136  [    0/  569]\n",
      "loss: 0.321015  [  128/  569]\n",
      "loss: 0.338831  [  256/  569]\n",
      "loss: 0.379056  [  384/  569]\n",
      "loss: 0.347515  [  456/  569]\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.389951  [    0/  569]\n",
      "loss: 0.327720  [  128/  569]\n",
      "loss: 0.276217  [  256/  569]\n",
      "loss: 0.291240  [  384/  569]\n",
      "loss: 0.313820  [  456/  569]\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.324749  [    0/  569]\n",
      "loss: 0.315293  [  128/  569]\n",
      "loss: 0.383479  [  256/  569]\n",
      "loss: 0.306682  [  384/  569]\n",
      "loss: 0.293063  [  456/  569]\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.319636  [    0/  569]\n",
      "loss: 0.335733  [  128/  569]\n",
      "loss: 0.378373  [  256/  569]\n",
      "loss: 0.346380  [  384/  569]\n",
      "loss: 0.270721  [  456/  569]\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.348441  [    0/  569]\n",
      "loss: 0.260343  [  128/  569]\n",
      "loss: 0.324170  [  256/  569]\n",
      "loss: 0.314979  [  384/  569]\n",
      "loss: 0.364709  [  456/  569]\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.405878  [    0/  569]\n",
      "loss: 0.367974  [  128/  569]\n",
      "loss: 0.308215  [  256/  569]\n",
      "loss: 0.321105  [  384/  569]\n",
      "loss: 0.336660  [  456/  569]\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.295772  [    0/  569]\n",
      "loss: 0.284268  [  128/  569]\n",
      "loss: 0.347401  [  256/  569]\n",
      "loss: 0.382701  [  384/  569]\n",
      "loss: 0.289295  [  456/  569]\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.289252  [    0/  569]\n",
      "loss: 0.351289  [  128/  569]\n",
      "loss: 0.379603  [  256/  569]\n",
      "loss: 0.345393  [  384/  569]\n",
      "loss: 0.372806  [  456/  569]\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.338717  [    0/  569]\n",
      "loss: 0.346047  [  128/  569]\n",
      "loss: 0.281949  [  256/  569]\n",
      "loss: 0.353569  [  384/  569]\n",
      "loss: 0.378478  [  456/  569]\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.322911  [    0/  569]\n",
      "loss: 0.291063  [  128/  569]\n",
      "loss: 0.268156  [  256/  569]\n",
      "loss: 0.317421  [  384/  569]\n",
      "loss: 0.280672  [  456/  569]\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.299041  [    0/  569]\n",
      "loss: 0.328039  [  128/  569]\n",
      "loss: 0.320095  [  256/  569]\n",
      "loss: 0.391447  [  384/  569]\n",
      "loss: 0.355313  [  456/  569]\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.383125  [    0/  569]\n",
      "loss: 0.358351  [  128/  569]\n",
      "loss: 0.290680  [  256/  569]\n",
      "loss: 0.312694  [  384/  569]\n",
      "loss: 0.294428  [  456/  569]\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.438581  [    0/  569]\n",
      "loss: 0.287026  [  128/  569]\n",
      "loss: 0.319748  [  256/  569]\n",
      "loss: 0.304734  [  384/  569]\n",
      "loss: 0.374733  [  456/  569]\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.341911  [    0/  569]\n",
      "loss: 0.310587  [  128/  569]\n",
      "loss: 0.363724  [  256/  569]\n",
      "loss: 0.320592  [  384/  569]\n",
      "loss: 0.339988  [  456/  569]\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.406239  [    0/  569]\n",
      "loss: 0.338638  [  128/  569]\n",
      "loss: 0.328546  [  256/  569]\n",
      "loss: 0.368346  [  384/  569]\n",
      "loss: 0.349959  [  456/  569]\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.266163  [    0/  569]\n",
      "loss: 0.387623  [  128/  569]\n",
      "loss: 0.352051  [  256/  569]\n",
      "loss: 0.329089  [  384/  569]\n",
      "loss: 0.378277  [  456/  569]\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.333936  [    0/  569]\n",
      "loss: 0.358042  [  128/  569]\n",
      "loss: 0.298271  [  256/  569]\n",
      "loss: 0.332268  [  384/  569]\n",
      "loss: 0.403093  [  456/  569]\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.349276  [    0/  569]\n",
      "loss: 0.375323  [  128/  569]\n",
      "loss: 0.413071  [  256/  569]\n",
      "loss: 0.295673  [  384/  569]\n",
      "loss: 0.326061  [  456/  569]\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.350529  [    0/  569]\n",
      "loss: 0.265659  [  128/  569]\n",
      "loss: 0.269683  [  256/  569]\n",
      "loss: 0.373926  [  384/  569]\n",
      "loss: 0.349949  [  456/  569]\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.343029  [    0/  569]\n",
      "loss: 0.292016  [  128/  569]\n",
      "loss: 0.294874  [  256/  569]\n",
      "loss: 0.361896  [  384/  569]\n",
      "loss: 0.383546  [  456/  569]\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.318998  [    0/  569]\n",
      "loss: 0.402315  [  128/  569]\n",
      "loss: 0.332627  [  256/  569]\n",
      "loss: 0.342772  [  384/  569]\n",
      "loss: 0.262382  [  456/  569]\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.291504  [    0/  569]\n",
      "loss: 0.359822  [  128/  569]\n",
      "loss: 0.416075  [  256/  569]\n",
      "loss: 0.318611  [  384/  569]\n",
      "loss: 0.398156  [  456/  569]\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.462327  [    0/  569]\n",
      "loss: 0.248692  [  128/  569]\n",
      "loss: 0.407172  [  256/  569]\n",
      "loss: 0.352069  [  384/  569]\n",
      "loss: 0.365358  [  456/  569]\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.343649  [    0/  569]\n",
      "loss: 0.338418  [  128/  569]\n",
      "loss: 0.340225  [  256/  569]\n",
      "loss: 0.345446  [  384/  569]\n",
      "loss: 0.285803  [  456/  569]\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.312300  [    0/  569]\n",
      "loss: 0.382572  [  128/  569]\n",
      "loss: 0.325727  [  256/  569]\n",
      "loss: 0.291707  [  384/  569]\n",
      "loss: 0.304699  [  456/  569]\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.314895  [    0/  569]\n",
      "loss: 0.389363  [  128/  569]\n",
      "loss: 0.308331  [  256/  569]\n",
      "loss: 0.277145  [  384/  569]\n",
      "loss: 0.312259  [  456/  569]\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.259151  [    0/  569]\n",
      "loss: 0.323849  [  128/  569]\n",
      "loss: 0.339795  [  256/  569]\n",
      "loss: 0.436415  [  384/  569]\n",
      "loss: 0.325766  [  456/  569]\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.334962  [    0/  569]\n",
      "loss: 0.348414  [  128/  569]\n",
      "loss: 0.384309  [  256/  569]\n",
      "loss: 0.288294  [  384/  569]\n",
      "loss: 0.376683  [  456/  569]\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.373526  [    0/  569]\n",
      "loss: 0.296341  [  128/  569]\n",
      "loss: 0.334162  [  256/  569]\n",
      "loss: 0.359586  [  384/  569]\n",
      "loss: 0.300209  [  456/  569]\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.258234  [    0/  569]\n",
      "loss: 0.381332  [  128/  569]\n",
      "loss: 0.271212  [  256/  569]\n",
      "loss: 0.321848  [  384/  569]\n",
      "loss: 0.385187  [  456/  569]\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.253152  [    0/  569]\n",
      "loss: 0.437031  [  128/  569]\n",
      "loss: 0.260864  [  256/  569]\n",
      "loss: 0.330164  [  384/  569]\n",
      "loss: 0.448534  [  456/  569]\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.290965  [    0/  569]\n",
      "loss: 0.302892  [  128/  569]\n",
      "loss: 0.287005  [  256/  569]\n",
      "loss: 0.369848  [  384/  569]\n",
      "loss: 0.420544  [  456/  569]\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.294207  [    0/  569]\n",
      "loss: 0.287922  [  128/  569]\n",
      "loss: 0.463945  [  256/  569]\n",
      "loss: 0.274810  [  384/  569]\n",
      "loss: 0.332750  [  456/  569]\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.205071  [    0/  569]\n",
      "loss: 0.323842  [  128/  569]\n",
      "loss: 0.361755  [  256/  569]\n",
      "loss: 0.405417  [  384/  569]\n",
      "loss: 0.374642  [  456/  569]\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.289836  [    0/  569]\n",
      "loss: 0.322355  [  128/  569]\n",
      "loss: 0.275353  [  256/  569]\n",
      "loss: 0.286538  [  384/  569]\n",
      "loss: 0.375832  [  456/  569]\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.260685  [    0/  569]\n",
      "loss: 0.310221  [  128/  569]\n",
      "loss: 0.315275  [  256/  569]\n",
      "loss: 0.440043  [  384/  569]\n",
      "loss: 0.317095  [  456/  569]\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.327898  [    0/  569]\n",
      "loss: 0.318974  [  128/  569]\n",
      "loss: 0.323968  [  256/  569]\n",
      "loss: 0.450532  [  384/  569]\n",
      "loss: 0.254402  [  456/  569]\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.310170  [    0/  569]\n",
      "loss: 0.343552  [  128/  569]\n",
      "loss: 0.426751  [  256/  569]\n",
      "loss: 0.364163  [  384/  569]\n",
      "loss: 0.305974  [  456/  569]\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.287800  [    0/  569]\n",
      "loss: 0.339752  [  128/  569]\n",
      "loss: 0.301527  [  256/  569]\n",
      "loss: 0.373613  [  384/  569]\n",
      "loss: 0.337127  [  456/  569]\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.353344  [    0/  569]\n",
      "loss: 0.312072  [  128/  569]\n",
      "loss: 0.341401  [  256/  569]\n",
      "loss: 0.444840  [  384/  569]\n",
      "loss: 0.319633  [  456/  569]\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.356878  [    0/  569]\n",
      "loss: 0.353027  [  128/  569]\n",
      "loss: 0.380566  [  256/  569]\n",
      "loss: 0.337658  [  384/  569]\n",
      "loss: 0.303258  [  456/  569]\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.349818  [    0/  569]\n",
      "loss: 0.402792  [  128/  569]\n",
      "loss: 0.347442  [  256/  569]\n",
      "loss: 0.310451  [  384/  569]\n",
      "loss: 0.318833  [  456/  569]\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.284429  [    0/  569]\n",
      "loss: 0.358797  [  128/  569]\n",
      "loss: 0.381611  [  256/  569]\n",
      "loss: 0.384887  [  384/  569]\n",
      "loss: 0.323420  [  456/  569]\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.301423  [    0/  569]\n",
      "loss: 0.301548  [  128/  569]\n",
      "loss: 0.385619  [  256/  569]\n",
      "loss: 0.340733  [  384/  569]\n",
      "loss: 0.310874  [  456/  569]\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.313662  [    0/  569]\n",
      "loss: 0.378197  [  128/  569]\n",
      "loss: 0.370301  [  256/  569]\n",
      "loss: 0.307646  [  384/  569]\n",
      "loss: 0.329876  [  456/  569]\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.288459  [    0/  569]\n",
      "loss: 0.405511  [  128/  569]\n",
      "loss: 0.368014  [  256/  569]\n",
      "loss: 0.353211  [  384/  569]\n",
      "loss: 0.331104  [  456/  569]\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.388449  [    0/  569]\n",
      "loss: 0.363489  [  128/  569]\n",
      "loss: 0.373561  [  256/  569]\n",
      "loss: 0.348854  [  384/  569]\n",
      "loss: 0.314379  [  456/  569]\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.391647  [    0/  569]\n",
      "loss: 0.323224  [  128/  569]\n",
      "loss: 0.395854  [  256/  569]\n",
      "loss: 0.363421  [  384/  569]\n",
      "loss: 0.262437  [  456/  569]\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.382225  [    0/  569]\n",
      "loss: 0.358453  [  128/  569]\n",
      "loss: 0.353972  [  256/  569]\n",
      "loss: 0.290624  [  384/  569]\n",
      "loss: 0.349014  [  456/  569]\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.346213  [    0/  569]\n",
      "loss: 0.269146  [  128/  569]\n",
      "loss: 0.281809  [  256/  569]\n",
      "loss: 0.323966  [  384/  569]\n",
      "loss: 0.382052  [  456/  569]\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.359613  [    0/  569]\n",
      "loss: 0.320913  [  128/  569]\n",
      "loss: 0.394055  [  256/  569]\n",
      "loss: 0.319760  [  384/  569]\n",
      "loss: 0.393472  [  456/  569]\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.361945  [    0/  569]\n",
      "loss: 0.299644  [  128/  569]\n",
      "loss: 0.366717  [  256/  569]\n",
      "loss: 0.339590  [  384/  569]\n",
      "loss: 0.324596  [  456/  569]\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.284678  [    0/  569]\n",
      "loss: 0.347025  [  128/  569]\n",
      "loss: 0.383906  [  256/  569]\n",
      "loss: 0.341869  [  384/  569]\n",
      "loss: 0.438328  [  456/  569]\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.330840  [    0/  569]\n",
      "loss: 0.288985  [  128/  569]\n",
      "loss: 0.334833  [  256/  569]\n",
      "loss: 0.362573  [  384/  569]\n",
      "loss: 0.343994  [  456/  569]\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.319803  [    0/  569]\n",
      "loss: 0.323845  [  128/  569]\n",
      "loss: 0.323902  [  256/  569]\n",
      "loss: 0.366184  [  384/  569]\n",
      "loss: 0.246874  [  456/  569]\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.319982  [    0/  569]\n",
      "loss: 0.341071  [  128/  569]\n",
      "loss: 0.251161  [  256/  569]\n",
      "loss: 0.302000  [  384/  569]\n",
      "loss: 0.375185  [  456/  569]\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.294918  [    0/  569]\n",
      "loss: 0.380940  [  128/  569]\n",
      "loss: 0.363046  [  256/  569]\n",
      "loss: 0.318243  [  384/  569]\n",
      "loss: 0.318114  [  456/  569]\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.241685  [    0/  569]\n",
      "loss: 0.431840  [  128/  569]\n",
      "loss: 0.414555  [  256/  569]\n",
      "loss: 0.233135  [  384/  569]\n",
      "loss: 0.265597  [  456/  569]\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.280733  [    0/  569]\n",
      "loss: 0.316682  [  128/  569]\n",
      "loss: 0.364589  [  256/  569]\n",
      "loss: 0.335392  [  384/  569]\n",
      "loss: 0.346934  [  456/  569]\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.347667  [    0/  569]\n",
      "loss: 0.364846  [  128/  569]\n",
      "loss: 0.380146  [  256/  569]\n",
      "loss: 0.337480  [  384/  569]\n",
      "loss: 0.324644  [  456/  569]\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.314326  [    0/  569]\n",
      "loss: 0.378488  [  128/  569]\n",
      "loss: 0.362977  [  256/  569]\n",
      "loss: 0.262379  [  384/  569]\n",
      "loss: 0.310731  [  456/  569]\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.289493  [    0/  569]\n",
      "loss: 0.311765  [  128/  569]\n",
      "loss: 0.341371  [  256/  569]\n",
      "loss: 0.258203  [  384/  569]\n",
      "loss: 0.414781  [  456/  569]\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 0.273812  [    0/  569]\n",
      "loss: 0.310703  [  128/  569]\n",
      "loss: 0.371357  [  256/  569]\n",
      "loss: 0.286287  [  384/  569]\n",
      "loss: 0.373605  [  456/  569]\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.439022  [    0/  569]\n",
      "loss: 0.311511  [  128/  569]\n",
      "loss: 0.305530  [  256/  569]\n",
      "loss: 0.346566  [  384/  569]\n",
      "loss: 0.298812  [  456/  569]\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.304021  [    0/  569]\n",
      "loss: 0.403706  [  128/  569]\n",
      "loss: 0.426674  [  256/  569]\n",
      "loss: 0.278219  [  384/  569]\n",
      "loss: 0.260902  [  456/  569]\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.334822  [    0/  569]\n",
      "loss: 0.368629  [  128/  569]\n",
      "loss: 0.297131  [  256/  569]\n",
      "loss: 0.355629  [  384/  569]\n",
      "loss: 0.320045  [  456/  569]\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.408615  [    0/  569]\n",
      "loss: 0.343107  [  128/  569]\n",
      "loss: 0.255539  [  256/  569]\n",
      "loss: 0.355643  [  384/  569]\n",
      "loss: 0.281880  [  456/  569]\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.298936  [    0/  569]\n",
      "loss: 0.363262  [  128/  569]\n",
      "loss: 0.333395  [  256/  569]\n",
      "loss: 0.342528  [  384/  569]\n",
      "loss: 0.307344  [  456/  569]\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.339600  [    0/  569]\n",
      "loss: 0.287525  [  128/  569]\n",
      "loss: 0.428205  [  256/  569]\n",
      "loss: 0.430994  [  384/  569]\n",
      "loss: 0.357898  [  456/  569]\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.336365  [    0/  569]\n",
      "loss: 0.289217  [  128/  569]\n",
      "loss: 0.288259  [  256/  569]\n",
      "loss: 0.379315  [  384/  569]\n",
      "loss: 0.353350  [  456/  569]\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.399815  [    0/  569]\n",
      "loss: 0.399284  [  128/  569]\n",
      "loss: 0.304505  [  256/  569]\n",
      "loss: 0.293295  [  384/  569]\n",
      "loss: 0.264623  [  456/  569]\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.323766  [    0/  569]\n",
      "loss: 0.382712  [  128/  569]\n",
      "loss: 0.339515  [  256/  569]\n",
      "loss: 0.302694  [  384/  569]\n",
      "loss: 0.364545  [  456/  569]\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.333845  [    0/  569]\n",
      "loss: 0.269144  [  128/  569]\n",
      "loss: 0.349935  [  256/  569]\n",
      "loss: 0.321905  [  384/  569]\n",
      "loss: 0.426828  [  456/  569]\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.370617  [    0/  569]\n",
      "loss: 0.289016  [  128/  569]\n",
      "loss: 0.357510  [  256/  569]\n",
      "loss: 0.326249  [  384/  569]\n",
      "loss: 0.323837  [  456/  569]\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.363981  [    0/  569]\n",
      "loss: 0.332384  [  128/  569]\n",
      "loss: 0.285798  [  256/  569]\n",
      "loss: 0.391754  [  384/  569]\n",
      "loss: 0.304048  [  456/  569]\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.343542  [    0/  569]\n",
      "loss: 0.286865  [  128/  569]\n",
      "loss: 0.338172  [  256/  569]\n",
      "loss: 0.271881  [  384/  569]\n",
      "loss: 0.274340  [  456/  569]\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.260779  [    0/  569]\n",
      "loss: 0.354706  [  128/  569]\n",
      "loss: 0.395496  [  256/  569]\n",
      "loss: 0.264334  [  384/  569]\n",
      "loss: 0.422738  [  456/  569]\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.393358  [    0/  569]\n",
      "loss: 0.340727  [  128/  569]\n",
      "loss: 0.347135  [  256/  569]\n",
      "loss: 0.300190  [  384/  569]\n",
      "loss: 0.344908  [  456/  569]\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.320911  [    0/  569]\n",
      "loss: 0.365789  [  128/  569]\n",
      "loss: 0.339350  [  256/  569]\n",
      "loss: 0.273721  [  384/  569]\n",
      "loss: 0.323480  [  456/  569]\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.296197  [    0/  569]\n",
      "loss: 0.333139  [  128/  569]\n",
      "loss: 0.352532  [  256/  569]\n",
      "loss: 0.331635  [  384/  569]\n",
      "loss: 0.319689  [  456/  569]\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.399780  [    0/  569]\n",
      "loss: 0.339045  [  128/  569]\n",
      "loss: 0.340388  [  256/  569]\n",
      "loss: 0.292713  [  384/  569]\n",
      "loss: 0.377989  [  456/  569]\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.358737  [    0/  569]\n",
      "loss: 0.261792  [  128/  569]\n",
      "loss: 0.285791  [  256/  569]\n",
      "loss: 0.370749  [  384/  569]\n",
      "loss: 0.388328  [  456/  569]\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.329801  [    0/  569]\n",
      "loss: 0.363456  [  128/  569]\n",
      "loss: 0.285685  [  256/  569]\n",
      "loss: 0.319107  [  384/  569]\n",
      "loss: 0.385891  [  456/  569]\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.286619  [    0/  569]\n",
      "loss: 0.341956  [  128/  569]\n",
      "loss: 0.345082  [  256/  569]\n",
      "loss: 0.334235  [  384/  569]\n",
      "loss: 0.366205  [  456/  569]\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.337423  [    0/  569]\n",
      "loss: 0.313782  [  128/  569]\n",
      "loss: 0.344018  [  256/  569]\n",
      "loss: 0.350200  [  384/  569]\n",
      "loss: 0.285127  [  456/  569]\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.374177  [    0/  569]\n",
      "loss: 0.316390  [  128/  569]\n",
      "loss: 0.294994  [  256/  569]\n",
      "loss: 0.373184  [  384/  569]\n",
      "loss: 0.279679  [  456/  569]\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.304708  [    0/  569]\n",
      "loss: 0.370926  [  128/  569]\n",
      "loss: 0.377751  [  256/  569]\n",
      "loss: 0.315859  [  384/  569]\n",
      "loss: 0.288272  [  456/  569]\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.368585  [    0/  569]\n",
      "loss: 0.325786  [  128/  569]\n",
      "loss: 0.251733  [  256/  569]\n",
      "loss: 0.350810  [  384/  569]\n",
      "loss: 0.293573  [  456/  569]\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.315675  [    0/  569]\n",
      "loss: 0.269242  [  128/  569]\n",
      "loss: 0.400023  [  256/  569]\n",
      "loss: 0.241509  [  384/  569]\n",
      "loss: 0.450142  [  456/  569]\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.323709  [    0/  569]\n",
      "loss: 0.447982  [  128/  569]\n",
      "loss: 0.340428  [  256/  569]\n",
      "loss: 0.315786  [  384/  569]\n",
      "loss: 0.324150  [  456/  569]\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.388685  [    0/  569]\n",
      "loss: 0.421502  [  128/  569]\n",
      "loss: 0.322490  [  256/  569]\n",
      "loss: 0.369786  [  384/  569]\n",
      "loss: 0.346101  [  456/  569]\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.351478  [    0/  569]\n",
      "loss: 0.367889  [  128/  569]\n",
      "loss: 0.342463  [  256/  569]\n",
      "loss: 0.295819  [  384/  569]\n",
      "loss: 0.354107  [  456/  569]\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.405349  [    0/  569]\n",
      "loss: 0.353262  [  128/  569]\n",
      "loss: 0.302783  [  256/  569]\n",
      "loss: 0.299155  [  384/  569]\n",
      "loss: 0.446491  [  456/  569]\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.310331  [    0/  569]\n",
      "loss: 0.370783  [  128/  569]\n",
      "loss: 0.324973  [  256/  569]\n",
      "loss: 0.292840  [  384/  569]\n",
      "loss: 0.410315  [  456/  569]\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.317574  [    0/  569]\n",
      "loss: 0.328482  [  128/  569]\n",
      "loss: 0.305678  [  256/  569]\n",
      "loss: 0.433229  [  384/  569]\n",
      "loss: 0.342553  [  456/  569]\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.236682  [    0/  569]\n",
      "loss: 0.335795  [  128/  569]\n",
      "loss: 0.261965  [  256/  569]\n",
      "loss: 0.391642  [  384/  569]\n",
      "loss: 0.365071  [  456/  569]\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.402218  [    0/  569]\n",
      "loss: 0.387197  [  128/  569]\n",
      "loss: 0.340560  [  256/  569]\n",
      "loss: 0.264670  [  384/  569]\n",
      "loss: 0.404965  [  456/  569]\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.374110  [    0/  569]\n",
      "loss: 0.324128  [  128/  569]\n",
      "loss: 0.289030  [  256/  569]\n",
      "loss: 0.295918  [  384/  569]\n",
      "loss: 0.336515  [  456/  569]\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 0.358747  [    0/  569]\n",
      "loss: 0.338203  [  128/  569]\n",
      "loss: 0.352866  [  256/  569]\n",
      "loss: 0.253341  [  384/  569]\n",
      "loss: 0.368571  [  456/  569]\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.309550  [    0/  569]\n",
      "loss: 0.356383  [  128/  569]\n",
      "loss: 0.315006  [  256/  569]\n",
      "loss: 0.299967  [  384/  569]\n",
      "loss: 0.325621  [  456/  569]\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.324197  [    0/  569]\n",
      "loss: 0.280408  [  128/  569]\n",
      "loss: 0.358007  [  256/  569]\n",
      "loss: 0.305442  [  384/  569]\n",
      "loss: 0.362785  [  456/  569]\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.320497  [    0/  569]\n",
      "loss: 0.366628  [  128/  569]\n",
      "loss: 0.385643  [  256/  569]\n",
      "loss: 0.319476  [  384/  569]\n",
      "loss: 0.254575  [  456/  569]\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.330627  [    0/  569]\n",
      "loss: 0.335060  [  128/  569]\n",
      "loss: 0.415285  [  256/  569]\n",
      "loss: 0.291065  [  384/  569]\n",
      "loss: 0.337423  [  456/  569]\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.355181  [    0/  569]\n",
      "loss: 0.350661  [  128/  569]\n",
      "loss: 0.322255  [  256/  569]\n",
      "loss: 0.384789  [  384/  569]\n",
      "loss: 0.366704  [  456/  569]\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.410281  [    0/  569]\n",
      "loss: 0.427701  [  128/  569]\n",
      "loss: 0.327674  [  256/  569]\n",
      "loss: 0.349876  [  384/  569]\n",
      "loss: 0.342382  [  456/  569]\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.400167  [    0/  569]\n",
      "loss: 0.366134  [  128/  569]\n",
      "loss: 0.370154  [  256/  569]\n",
      "loss: 0.281199  [  384/  569]\n",
      "loss: 0.302516  [  456/  569]\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.353766  [    0/  569]\n",
      "loss: 0.332217  [  128/  569]\n",
      "loss: 0.396234  [  256/  569]\n",
      "loss: 0.292752  [  384/  569]\n",
      "loss: 0.415560  [  456/  569]\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.262187  [    0/  569]\n",
      "loss: 0.303683  [  128/  569]\n",
      "loss: 0.311837  [  256/  569]\n",
      "loss: 0.361593  [  384/  569]\n",
      "loss: 0.396608  [  456/  569]\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.350194  [    0/  569]\n",
      "loss: 0.391464  [  128/  569]\n",
      "loss: 0.372260  [  256/  569]\n",
      "loss: 0.309384  [  384/  569]\n",
      "loss: 0.261023  [  456/  569]\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.286026  [    0/  569]\n",
      "loss: 0.358069  [  128/  569]\n",
      "loss: 0.331137  [  256/  569]\n",
      "loss: 0.427044  [  384/  569]\n",
      "loss: 0.421311  [  456/  569]\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.303284  [    0/  569]\n",
      "loss: 0.351776  [  128/  569]\n",
      "loss: 0.361729  [  256/  569]\n",
      "loss: 0.318051  [  384/  569]\n",
      "loss: 0.299504  [  456/  569]\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "loss: 0.348930  [    0/  569]\n",
      "loss: 0.298024  [  128/  569]\n",
      "loss: 0.282275  [  256/  569]\n",
      "loss: 0.291218  [  384/  569]\n",
      "loss: 0.398229  [  456/  569]\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "loss: 0.296074  [    0/  569]\n",
      "loss: 0.345071  [  128/  569]\n",
      "loss: 0.368143  [  256/  569]\n",
      "loss: 0.341793  [  384/  569]\n",
      "loss: 0.322922  [  456/  569]\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "loss: 0.333482  [    0/  569]\n",
      "loss: 0.379105  [  128/  569]\n",
      "loss: 0.262912  [  256/  569]\n",
      "loss: 0.347116  [  384/  569]\n",
      "loss: 0.302306  [  456/  569]\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "loss: 0.314225  [    0/  569]\n",
      "loss: 0.297440  [  128/  569]\n",
      "loss: 0.349923  [  256/  569]\n",
      "loss: 0.375787  [  384/  569]\n",
      "loss: 0.348618  [  456/  569]\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "loss: 0.380039  [    0/  569]\n",
      "loss: 0.294084  [  128/  569]\n",
      "loss: 0.286721  [  256/  569]\n",
      "loss: 0.384407  [  384/  569]\n",
      "loss: 0.279262  [  456/  569]\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "loss: 0.360374  [    0/  569]\n",
      "loss: 0.358306  [  128/  569]\n",
      "loss: 0.304241  [  256/  569]\n",
      "loss: 0.342612  [  384/  569]\n",
      "loss: 0.320721  [  456/  569]\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "loss: 0.267466  [    0/  569]\n",
      "loss: 0.390778  [  128/  569]\n",
      "loss: 0.341796  [  256/  569]\n",
      "loss: 0.283514  [  384/  569]\n",
      "loss: 0.443906  [  456/  569]\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "loss: 0.363956  [    0/  569]\n",
      "loss: 0.309002  [  128/  569]\n",
      "loss: 0.336375  [  256/  569]\n",
      "loss: 0.321035  [  384/  569]\n",
      "loss: 0.284592  [  456/  569]\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "loss: 0.373965  [    0/  569]\n",
      "loss: 0.297226  [  128/  569]\n",
      "loss: 0.304408  [  256/  569]\n",
      "loss: 0.325554  [  384/  569]\n",
      "loss: 0.382923  [  456/  569]\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "loss: 0.341392  [    0/  569]\n",
      "loss: 0.305958  [  128/  569]\n",
      "loss: 0.407209  [  256/  569]\n",
      "loss: 0.374168  [  384/  569]\n",
      "loss: 0.254902  [  456/  569]\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "loss: 0.307750  [    0/  569]\n",
      "loss: 0.238150  [  128/  569]\n",
      "loss: 0.303496  [  256/  569]\n",
      "loss: 0.383352  [  384/  569]\n",
      "loss: 0.375469  [  456/  569]\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 0.345695  [    0/  569]\n",
      "loss: 0.345472  [  128/  569]\n",
      "loss: 0.350390  [  256/  569]\n",
      "loss: 0.371317  [  384/  569]\n",
      "loss: 0.302932  [  456/  569]\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "loss: 0.296101  [    0/  569]\n",
      "loss: 0.402490  [  128/  569]\n",
      "loss: 0.321419  [  256/  569]\n",
      "loss: 0.330567  [  384/  569]\n",
      "loss: 0.298598  [  456/  569]\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "loss: 0.349263  [    0/  569]\n",
      "loss: 0.322585  [  128/  569]\n",
      "loss: 0.399618  [  256/  569]\n",
      "loss: 0.315481  [  384/  569]\n",
      "loss: 0.405103  [  456/  569]\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "loss: 0.389594  [    0/  569]\n",
      "loss: 0.321530  [  128/  569]\n",
      "loss: 0.379816  [  256/  569]\n",
      "loss: 0.280306  [  384/  569]\n",
      "loss: 0.286614  [  456/  569]\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "loss: 0.311528  [    0/  569]\n",
      "loss: 0.301060  [  128/  569]\n",
      "loss: 0.341328  [  256/  569]\n",
      "loss: 0.406087  [  384/  569]\n",
      "loss: 0.302849  [  456/  569]\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "loss: 0.290927  [    0/  569]\n",
      "loss: 0.352341  [  128/  569]\n",
      "loss: 0.361529  [  256/  569]\n",
      "loss: 0.415108  [  384/  569]\n",
      "loss: 0.281850  [  456/  569]\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "loss: 0.289070  [    0/  569]\n",
      "loss: 0.343239  [  128/  569]\n",
      "loss: 0.390849  [  256/  569]\n",
      "loss: 0.417407  [  384/  569]\n",
      "loss: 0.301242  [  456/  569]\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "loss: 0.300405  [    0/  569]\n",
      "loss: 0.332512  [  128/  569]\n",
      "loss: 0.343672  [  256/  569]\n",
      "loss: 0.363544  [  384/  569]\n",
      "loss: 0.280848  [  456/  569]\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "loss: 0.349735  [    0/  569]\n",
      "loss: 0.384583  [  128/  569]\n",
      "loss: 0.278625  [  256/  569]\n",
      "loss: 0.280843  [  384/  569]\n",
      "loss: 0.373159  [  456/  569]\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "loss: 0.274987  [    0/  569]\n",
      "loss: 0.345087  [  128/  569]\n",
      "loss: 0.321199  [  256/  569]\n",
      "loss: 0.349041  [  384/  569]\n",
      "loss: 0.287335  [  456/  569]\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "loss: 0.363957  [    0/  569]\n",
      "loss: 0.324694  [  128/  569]\n",
      "loss: 0.328320  [  256/  569]\n",
      "loss: 0.374367  [  384/  569]\n",
      "loss: 0.306599  [  456/  569]\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "loss: 0.414056  [    0/  569]\n",
      "loss: 0.372042  [  128/  569]\n",
      "loss: 0.331741  [  256/  569]\n",
      "loss: 0.320885  [  384/  569]\n",
      "loss: 0.388748  [  456/  569]\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "loss: 0.310499  [    0/  569]\n",
      "loss: 0.355121  [  128/  569]\n",
      "loss: 0.287523  [  256/  569]\n",
      "loss: 0.294041  [  384/  569]\n",
      "loss: 0.414404  [  456/  569]\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "loss: 0.370294  [    0/  569]\n",
      "loss: 0.408970  [  128/  569]\n",
      "loss: 0.300990  [  256/  569]\n",
      "loss: 0.325609  [  384/  569]\n",
      "loss: 0.336876  [  456/  569]\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "loss: 0.330389  [    0/  569]\n",
      "loss: 0.348189  [  128/  569]\n",
      "loss: 0.363172  [  256/  569]\n",
      "loss: 0.390389  [  384/  569]\n",
      "loss: 0.226770  [  456/  569]\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "loss: 0.232819  [    0/  569]\n",
      "loss: 0.335351  [  128/  569]\n",
      "loss: 0.390496  [  256/  569]\n",
      "loss: 0.354161  [  384/  569]\n",
      "loss: 0.316178  [  456/  569]\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "loss: 0.367516  [    0/  569]\n",
      "loss: 0.401895  [  128/  569]\n",
      "loss: 0.284896  [  256/  569]\n",
      "loss: 0.341277  [  384/  569]\n",
      "loss: 0.343790  [  456/  569]\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "loss: 0.341698  [    0/  569]\n",
      "loss: 0.332935  [  128/  569]\n",
      "loss: 0.394842  [  256/  569]\n",
      "loss: 0.361788  [  384/  569]\n",
      "loss: 0.314381  [  456/  569]\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "loss: 0.325374  [    0/  569]\n",
      "loss: 0.371386  [  128/  569]\n",
      "loss: 0.397515  [  256/  569]\n",
      "loss: 0.283118  [  384/  569]\n",
      "loss: 0.365398  [  456/  569]\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "loss: 0.312767  [    0/  569]\n",
      "loss: 0.328436  [  128/  569]\n",
      "loss: 0.314198  [  256/  569]\n",
      "loss: 0.352369  [  384/  569]\n",
      "loss: 0.272433  [  456/  569]\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "loss: 0.386530  [    0/  569]\n",
      "loss: 0.366722  [  128/  569]\n",
      "loss: 0.378313  [  256/  569]\n",
      "loss: 0.257459  [  384/  569]\n",
      "loss: 0.321051  [  456/  569]\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "loss: 0.464310  [    0/  569]\n",
      "loss: 0.345404  [  128/  569]\n",
      "loss: 0.300669  [  256/  569]\n",
      "loss: 0.341862  [  384/  569]\n",
      "loss: 0.364509  [  456/  569]\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "loss: 0.358348  [    0/  569]\n",
      "loss: 0.307531  [  128/  569]\n",
      "loss: 0.351139  [  256/  569]\n",
      "loss: 0.319292  [  384/  569]\n",
      "loss: 0.364979  [  456/  569]\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "loss: 0.348062  [    0/  569]\n",
      "loss: 0.292951  [  128/  569]\n",
      "loss: 0.344311  [  256/  569]\n",
      "loss: 0.378451  [  384/  569]\n",
      "loss: 0.410661  [  456/  569]\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "loss: 0.319862  [    0/  569]\n",
      "loss: 0.372008  [  128/  569]\n",
      "loss: 0.373953  [  256/  569]\n",
      "loss: 0.325827  [  384/  569]\n",
      "loss: 0.332001  [  456/  569]\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "loss: 0.360343  [    0/  569]\n",
      "loss: 0.294612  [  128/  569]\n",
      "loss: 0.416988  [  256/  569]\n",
      "loss: 0.349994  [  384/  569]\n",
      "loss: 0.286981  [  456/  569]\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "loss: 0.328111  [    0/  569]\n",
      "loss: 0.313713  [  128/  569]\n",
      "loss: 0.314411  [  256/  569]\n",
      "loss: 0.340613  [  384/  569]\n",
      "loss: 0.372962  [  456/  569]\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "loss: 0.286159  [    0/  569]\n",
      "loss: 0.324376  [  128/  569]\n",
      "loss: 0.296267  [  256/  569]\n",
      "loss: 0.431276  [  384/  569]\n",
      "loss: 0.323832  [  456/  569]\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "loss: 0.422872  [    0/  569]\n",
      "loss: 0.339493  [  128/  569]\n",
      "loss: 0.336732  [  256/  569]\n",
      "loss: 0.335735  [  384/  569]\n",
      "loss: 0.311402  [  456/  569]\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "loss: 0.360060  [    0/  569]\n",
      "loss: 0.386704  [  128/  569]\n",
      "loss: 0.347412  [  256/  569]\n",
      "loss: 0.325917  [  384/  569]\n",
      "loss: 0.329643  [  456/  569]\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "loss: 0.326129  [    0/  569]\n",
      "loss: 0.360801  [  128/  569]\n",
      "loss: 0.310397  [  256/  569]\n",
      "loss: 0.269796  [  384/  569]\n",
      "loss: 0.401074  [  456/  569]\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "loss: 0.324517  [    0/  569]\n",
      "loss: 0.282862  [  128/  569]\n",
      "loss: 0.302294  [  256/  569]\n",
      "loss: 0.393279  [  384/  569]\n",
      "loss: 0.317271  [  456/  569]\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "loss: 0.277782  [    0/  569]\n",
      "loss: 0.353450  [  128/  569]\n",
      "loss: 0.302026  [  256/  569]\n",
      "loss: 0.402797  [  384/  569]\n",
      "loss: 0.346412  [  456/  569]\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "loss: 0.349354  [    0/  569]\n",
      "loss: 0.327830  [  128/  569]\n",
      "loss: 0.321863  [  256/  569]\n",
      "loss: 0.325522  [  384/  569]\n",
      "loss: 0.419230  [  456/  569]\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "loss: 0.307976  [    0/  569]\n",
      "loss: 0.328644  [  128/  569]\n",
      "loss: 0.352648  [  256/  569]\n",
      "loss: 0.318164  [  384/  569]\n",
      "loss: 0.391353  [  456/  569]\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "loss: 0.323811  [    0/  569]\n",
      "loss: 0.314309  [  128/  569]\n",
      "loss: 0.288184  [  256/  569]\n",
      "loss: 0.368071  [  384/  569]\n",
      "loss: 0.371590  [  456/  569]\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "loss: 0.352507  [    0/  569]\n",
      "loss: 0.321814  [  128/  569]\n",
      "loss: 0.317206  [  256/  569]\n",
      "loss: 0.353387  [  384/  569]\n",
      "loss: 0.315745  [  456/  569]\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "loss: 0.366662  [    0/  569]\n",
      "loss: 0.342857  [  128/  569]\n",
      "loss: 0.338047  [  256/  569]\n",
      "loss: 0.302573  [  384/  569]\n",
      "loss: 0.299312  [  456/  569]\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "loss: 0.228068  [    0/  569]\n",
      "loss: 0.314281  [  128/  569]\n",
      "loss: 0.296451  [  256/  569]\n",
      "loss: 0.359689  [  384/  569]\n",
      "loss: 0.286988  [  456/  569]\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "loss: 0.386697  [    0/  569]\n",
      "loss: 0.332336  [  128/  569]\n",
      "loss: 0.286328  [  256/  569]\n",
      "loss: 0.281332  [  384/  569]\n",
      "loss: 0.398241  [  456/  569]\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "loss: 0.309735  [    0/  569]\n",
      "loss: 0.302878  [  128/  569]\n",
      "loss: 0.416219  [  256/  569]\n",
      "loss: 0.337567  [  384/  569]\n",
      "loss: 0.379772  [  456/  569]\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "loss: 0.334424  [    0/  569]\n",
      "loss: 0.324872  [  128/  569]\n",
      "loss: 0.284885  [  256/  569]\n",
      "loss: 0.294498  [  384/  569]\n",
      "loss: 0.339144  [  456/  569]\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "loss: 0.347073  [    0/  569]\n",
      "loss: 0.349147  [  128/  569]\n",
      "loss: 0.300903  [  256/  569]\n",
      "loss: 0.342316  [  384/  569]\n",
      "loss: 0.318268  [  456/  569]\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "loss: 0.279113  [    0/  569]\n",
      "loss: 0.316193  [  128/  569]\n",
      "loss: 0.311354  [  256/  569]\n",
      "loss: 0.347155  [  384/  569]\n",
      "loss: 0.316355  [  456/  569]\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "loss: 0.339731  [    0/  569]\n",
      "loss: 0.278116  [  128/  569]\n",
      "loss: 0.357592  [  256/  569]\n",
      "loss: 0.323472  [  384/  569]\n",
      "loss: 0.334539  [  456/  569]\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "loss: 0.257386  [    0/  569]\n",
      "loss: 0.354518  [  128/  569]\n",
      "loss: 0.283698  [  256/  569]\n",
      "loss: 0.315691  [  384/  569]\n",
      "loss: 0.423626  [  456/  569]\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "loss: 0.273584  [    0/  569]\n",
      "loss: 0.397923  [  128/  569]\n",
      "loss: 0.367409  [  256/  569]\n",
      "loss: 0.356959  [  384/  569]\n",
      "loss: 0.271871  [  456/  569]\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "loss: 0.341864  [    0/  569]\n",
      "loss: 0.349456  [  128/  569]\n",
      "loss: 0.308627  [  256/  569]\n",
      "loss: 0.355295  [  384/  569]\n",
      "loss: 0.393914  [  456/  569]\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "loss: 0.305827  [    0/  569]\n",
      "loss: 0.377479  [  128/  569]\n",
      "loss: 0.322899  [  256/  569]\n",
      "loss: 0.286666  [  384/  569]\n",
      "loss: 0.284890  [  456/  569]\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "loss: 0.455272  [    0/  569]\n",
      "loss: 0.302473  [  128/  569]\n",
      "loss: 0.302835  [  256/  569]\n",
      "loss: 0.452112  [  384/  569]\n",
      "loss: 0.344156  [  456/  569]\n",
      "Done! lr = 1.0000000000000004e-08\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dl, model, loss_fn, optimizer, scheduler=schedulerOnPlateau, onPlateau=True)\n",
    "print(f\"Done! lr = {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso tampoco ha mejorado. Probablemente para este problema un lr de 10<sup>-3</sup> es suficiente y no hace falta reducir el learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducción del learning exponencialmente y si la métrica no mejora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver el mismo ejemplo usando los dos métodos. Rededfinimos la función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_prints = 4\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, scheduler1=None, scheduler2=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # X and y to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        logits, probs = model(X)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % int(len(dataloader)/num_prints) == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    if scheduler1 is not None and scheduler2 is not None:\n",
    "        scheduler1.step(loss)\n",
    "        scheduler2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "# Se define una semilla para que la inicialización de los pesos aleatoria sea siempre la misma\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "model = CancerNeuralNetwork(31, 1)\n",
    "model.to(device)\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "LR = 1e-3\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "\n",
    "schedulerExponential = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, verbose=True)\n",
    "schedulerOnPlateau = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.010835  [    0/  569]\n",
      "loss: 4.976839  [  128/  569]\n",
      "loss: 1.088202  [  256/  569]\n",
      "loss: 0.989754  [  384/  569]\n",
      "loss: 2.056240  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.452749  [    0/  569]\n",
      "loss: 0.817804  [  128/  569]\n",
      "loss: 1.101999  [  256/  569]\n",
      "loss: 0.719499  [  384/  569]\n",
      "loss: 0.860600  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.076765  [    0/  569]\n",
      "loss: 0.695022  [  128/  569]\n",
      "loss: 0.957789  [  256/  569]\n",
      "loss: 0.903349  [  384/  569]\n",
      "loss: 1.135585  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.2900e-04.\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.701673  [    0/  569]\n",
      "loss: 0.648564  [  128/  569]\n",
      "loss: 0.670574  [  256/  569]\n",
      "loss: 0.607591  [  384/  569]\n",
      "loss: 0.579254  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.5610e-04.\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.584377  [    0/  569]\n",
      "loss: 0.629451  [  128/  569]\n",
      "loss: 0.612009  [  256/  569]\n",
      "loss: 0.597891  [  384/  569]\n",
      "loss: 0.555261  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.9049e-04.\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.551084  [    0/  569]\n",
      "loss: 0.593376  [  128/  569]\n",
      "loss: 0.605462  [  256/  569]\n",
      "loss: 0.653608  [  384/  569]\n",
      "loss: 0.568694  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.3144e-04.\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.559495  [    0/  569]\n",
      "loss: 0.591203  [  128/  569]\n",
      "loss: 0.563477  [  256/  569]\n",
      "loss: 0.571932  [  384/  569]\n",
      "loss: 0.587132  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.7830e-04.\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.581155  [    0/  569]\n",
      "loss: 0.557200  [  128/  569]\n",
      "loss: 0.615105  [  256/  569]\n",
      "loss: 0.528208  [  384/  569]\n",
      "loss: 0.549739  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.3047e-04.\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.585598  [    0/  569]\n",
      "loss: 0.561335  [  128/  569]\n",
      "loss: 0.540029  [  256/  569]\n",
      "loss: 0.558765  [  384/  569]\n",
      "loss: 0.551368  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.8742e-04.\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.559206  [    0/  569]\n",
      "loss: 0.553010  [  128/  569]\n",
      "loss: 0.525550  [  256/  569]\n",
      "loss: 0.551794  [  384/  569]\n",
      "loss: 0.568441  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4868e-04.\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.556216  [    0/  569]\n",
      "loss: 0.521318  [  128/  569]\n",
      "loss: 0.510400  [  256/  569]\n",
      "loss: 0.530822  [  384/  569]\n",
      "loss: 0.559207  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.1381e-04.\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.502698  [    0/  569]\n",
      "loss: 0.552587  [  128/  569]\n",
      "loss: 0.550381  [  256/  569]\n",
      "loss: 0.525957  [  384/  569]\n",
      "loss: 0.499059  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8243e-04.\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.507806  [    0/  569]\n",
      "loss: 0.562871  [  128/  569]\n",
      "loss: 0.551181  [  256/  569]\n",
      "loss: 0.507538  [  384/  569]\n",
      "loss: 0.534216  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5419e-04.\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.553730  [    0/  569]\n",
      "loss: 0.543440  [  128/  569]\n",
      "loss: 0.561131  [  256/  569]\n",
      "loss: 0.552931  [  384/  569]\n",
      "loss: 0.527676  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2877e-04.\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.529662  [    0/  569]\n",
      "loss: 0.510787  [  128/  569]\n",
      "loss: 0.481543  [  256/  569]\n",
      "loss: 0.552605  [  384/  569]\n",
      "loss: 0.553198  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0589e-04.\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.545856  [    0/  569]\n",
      "loss: 0.495802  [  128/  569]\n",
      "loss: 0.532656  [  256/  569]\n",
      "loss: 0.512769  [  384/  569]\n",
      "loss: 0.549671  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8530e-04.\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.521138  [    0/  569]\n",
      "loss: 0.501720  [  128/  569]\n",
      "loss: 0.537137  [  256/  569]\n",
      "loss: 0.513372  [  384/  569]\n",
      "loss: 0.401879  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6677e-04.\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.510076  [    0/  569]\n",
      "loss: 0.532782  [  128/  569]\n",
      "loss: 0.486431  [  256/  569]\n",
      "loss: 0.504868  [  384/  569]\n",
      "loss: 0.496710  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5009e-04.\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.513390  [    0/  569]\n",
      "loss: 0.500137  [  128/  569]\n",
      "loss: 0.480049  [  256/  569]\n",
      "loss: 0.467917  [  384/  569]\n",
      "loss: 0.528345  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3509e-04.\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.482476  [    0/  569]\n",
      "loss: 0.526606  [  128/  569]\n",
      "loss: 0.542273  [  256/  569]\n",
      "loss: 0.514688  [  384/  569]\n",
      "loss: 0.487813  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2158e-04.\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.458018  [    0/  569]\n",
      "loss: 0.503167  [  128/  569]\n",
      "loss: 0.568097  [  256/  569]\n",
      "loss: 0.517542  [  384/  569]\n",
      "loss: 0.509591  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0942e-04.\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.523150  [    0/  569]\n",
      "loss: 0.516567  [  128/  569]\n",
      "loss: 0.539929  [  256/  569]\n",
      "loss: 0.490629  [  384/  569]\n",
      "loss: 0.536520  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.8477e-05.\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.516190  [    0/  569]\n",
      "loss: 0.534259  [  128/  569]\n",
      "loss: 0.483823  [  256/  569]\n",
      "loss: 0.540584  [  384/  569]\n",
      "loss: 0.515317  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.8629e-05.\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.488848  [    0/  569]\n",
      "loss: 0.488586  [  128/  569]\n",
      "loss: 0.527949  [  256/  569]\n",
      "loss: 0.517626  [  384/  569]\n",
      "loss: 0.506905  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.9766e-05.\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.534676  [    0/  569]\n",
      "loss: 0.496036  [  128/  569]\n",
      "loss: 0.518143  [  256/  569]\n",
      "loss: 0.491111  [  384/  569]\n",
      "loss: 0.480157  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.1790e-05.\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.457590  [    0/  569]\n",
      "loss: 0.468447  [  128/  569]\n",
      "loss: 0.485586  [  256/  569]\n",
      "loss: 0.480727  [  384/  569]\n",
      "loss: 0.496449  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.4611e-05.\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.448108  [    0/  569]\n",
      "loss: 0.506405  [  128/  569]\n",
      "loss: 0.491574  [  256/  569]\n",
      "loss: 0.542300  [  384/  569]\n",
      "loss: 0.498557  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.8150e-05.\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.571007  [    0/  569]\n",
      "loss: 0.500370  [  128/  569]\n",
      "loss: 0.526144  [  256/  569]\n",
      "loss: 0.517554  [  384/  569]\n",
      "loss: 0.418860  [  456/  569]\n",
      "Epoch    28: reducing learning rate of group 0 to 5.8150e-06.\n",
      "Adjusting learning rate of group 0 to 5.2335e-06.\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.489645  [    0/  569]\n",
      "loss: 0.492351  [  128/  569]\n",
      "loss: 0.496978  [  256/  569]\n",
      "loss: 0.525298  [  384/  569]\n",
      "loss: 0.508995  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.7101e-06.\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.514177  [    0/  569]\n",
      "loss: 0.506340  [  128/  569]\n",
      "loss: 0.483574  [  256/  569]\n",
      "loss: 0.468951  [  384/  569]\n",
      "loss: 0.516703  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.2391e-06.\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.474034  [    0/  569]\n",
      "loss: 0.464246  [  128/  569]\n",
      "loss: 0.464721  [  256/  569]\n",
      "loss: 0.551459  [  384/  569]\n",
      "loss: 0.550250  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.8152e-06.\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.565981  [    0/  569]\n",
      "loss: 0.479160  [  128/  569]\n",
      "loss: 0.467786  [  256/  569]\n",
      "loss: 0.463023  [  384/  569]\n",
      "loss: 0.522150  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4337e-06.\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.503162  [    0/  569]\n",
      "loss: 0.473298  [  128/  569]\n",
      "loss: 0.482926  [  256/  569]\n",
      "loss: 0.514762  [  384/  569]\n",
      "loss: 0.501151  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.0903e-06.\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.515978  [    0/  569]\n",
      "loss: 0.534461  [  128/  569]\n",
      "loss: 0.494581  [  256/  569]\n",
      "loss: 0.535272  [  384/  569]\n",
      "loss: 0.480809  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.7813e-06.\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.544763  [    0/  569]\n",
      "loss: 0.462865  [  128/  569]\n",
      "loss: 0.504965  [  256/  569]\n",
      "loss: 0.476462  [  384/  569]\n",
      "loss: 0.527642  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5032e-06.\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.533280  [    0/  569]\n",
      "loss: 0.455568  [  128/  569]\n",
      "loss: 0.485134  [  256/  569]\n",
      "loss: 0.526014  [  384/  569]\n",
      "loss: 0.469377  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2528e-06.\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.512446  [    0/  569]\n",
      "loss: 0.526083  [  128/  569]\n",
      "loss: 0.493418  [  256/  569]\n",
      "loss: 0.484525  [  384/  569]\n",
      "loss: 0.482707  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0276e-06.\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.523854  [    0/  569]\n",
      "loss: 0.459284  [  128/  569]\n",
      "loss: 0.497989  [  256/  569]\n",
      "loss: 0.459617  [  384/  569]\n",
      "loss: 0.556009  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8248e-06.\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.503792  [    0/  569]\n",
      "loss: 0.480959  [  128/  569]\n",
      "loss: 0.485887  [  256/  569]\n",
      "loss: 0.481655  [  384/  569]\n",
      "loss: 0.525394  [  456/  569]\n",
      "Epoch    39: reducing learning rate of group 0 to 1.8248e-07.\n",
      "Adjusting learning rate of group 0 to 1.6423e-07.\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.509147  [    0/  569]\n",
      "loss: 0.489762  [  128/  569]\n",
      "loss: 0.488609  [  256/  569]\n",
      "loss: 0.526735  [  384/  569]\n",
      "loss: 0.471635  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4781e-07.\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.480528  [    0/  569]\n",
      "loss: 0.505411  [  128/  569]\n",
      "loss: 0.446923  [  256/  569]\n",
      "loss: 0.479282  [  384/  569]\n",
      "loss: 0.501033  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3303e-07.\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.524723  [    0/  569]\n",
      "loss: 0.500688  [  128/  569]\n",
      "loss: 0.516862  [  256/  569]\n",
      "loss: 0.488644  [  384/  569]\n",
      "loss: 0.501923  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1973e-07.\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.533288  [    0/  569]\n",
      "loss: 0.495574  [  128/  569]\n",
      "loss: 0.481400  [  256/  569]\n",
      "loss: 0.508282  [  384/  569]\n",
      "loss: 0.475131  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0775e-07.\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.508008  [    0/  569]\n",
      "loss: 0.489548  [  128/  569]\n",
      "loss: 0.515560  [  256/  569]\n",
      "loss: 0.544541  [  384/  569]\n",
      "loss: 0.499372  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.6977e-08.\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.547262  [    0/  569]\n",
      "loss: 0.521270  [  128/  569]\n",
      "loss: 0.508880  [  256/  569]\n",
      "loss: 0.487327  [  384/  569]\n",
      "loss: 0.499080  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.7280e-08.\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.515671  [    0/  569]\n",
      "loss: 0.471854  [  128/  569]\n",
      "loss: 0.488569  [  256/  569]\n",
      "loss: 0.524615  [  384/  569]\n",
      "loss: 0.469596  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.8552e-08.\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.507216  [    0/  569]\n",
      "loss: 0.505355  [  128/  569]\n",
      "loss: 0.495559  [  256/  569]\n",
      "loss: 0.502806  [  384/  569]\n",
      "loss: 0.456264  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.0697e-08.\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.511314  [    0/  569]\n",
      "loss: 0.487989  [  128/  569]\n",
      "loss: 0.462986  [  256/  569]\n",
      "loss: 0.492516  [  384/  569]\n",
      "loss: 0.507429  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.3627e-08.\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.537525  [    0/  569]\n",
      "loss: 0.468725  [  128/  569]\n",
      "loss: 0.491651  [  256/  569]\n",
      "loss: 0.556484  [  384/  569]\n",
      "loss: 0.479615  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.7264e-08.\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.489260  [    0/  569]\n",
      "loss: 0.494270  [  128/  569]\n",
      "loss: 0.478553  [  256/  569]\n",
      "loss: 0.487755  [  384/  569]\n",
      "loss: 0.480932  [  456/  569]\n",
      "Epoch    50: reducing learning rate of group 0 to 5.7264e-09.\n",
      "Adjusting learning rate of group 0 to 5.1538e-09.\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.478070  [    0/  569]\n",
      "loss: 0.555249  [  128/  569]\n",
      "loss: 0.486675  [  256/  569]\n",
      "loss: 0.550303  [  384/  569]\n",
      "loss: 0.477327  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.6384e-09.\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.474884  [    0/  569]\n",
      "loss: 0.474977  [  128/  569]\n",
      "loss: 0.523788  [  256/  569]\n",
      "loss: 0.466264  [  384/  569]\n",
      "loss: 0.531741  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.1746e-09.\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.458968  [    0/  569]\n",
      "loss: 0.478127  [  128/  569]\n",
      "loss: 0.534191  [  256/  569]\n",
      "loss: 0.489255  [  384/  569]\n",
      "loss: 0.582012  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.7571e-09.\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.462583  [    0/  569]\n",
      "loss: 0.511540  [  128/  569]\n",
      "loss: 0.512821  [  256/  569]\n",
      "loss: 0.473329  [  384/  569]\n",
      "loss: 0.550573  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.3814e-09.\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.475824  [    0/  569]\n",
      "loss: 0.520681  [  128/  569]\n",
      "loss: 0.531899  [  256/  569]\n",
      "loss: 0.511333  [  384/  569]\n",
      "loss: 0.545316  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.0433e-09.\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.527091  [    0/  569]\n",
      "loss: 0.474074  [  128/  569]\n",
      "loss: 0.531470  [  256/  569]\n",
      "loss: 0.460955  [  384/  569]\n",
      "loss: 0.482020  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.7389e-09.\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.498888  [    0/  569]\n",
      "loss: 0.546263  [  128/  569]\n",
      "loss: 0.485368  [  256/  569]\n",
      "loss: 0.438900  [  384/  569]\n",
      "loss: 0.450980  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4650e-09.\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.505714  [    0/  569]\n",
      "loss: 0.514156  [  128/  569]\n",
      "loss: 0.484941  [  256/  569]\n",
      "loss: 0.511032  [  384/  569]\n",
      "loss: 0.471130  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2185e-09.\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.504757  [    0/  569]\n",
      "loss: 0.479622  [  128/  569]\n",
      "loss: 0.477045  [  256/  569]\n",
      "loss: 0.515337  [  384/  569]\n",
      "loss: 0.499285  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9967e-09.\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.504115  [    0/  569]\n",
      "loss: 0.505457  [  128/  569]\n",
      "loss: 0.546969  [  256/  569]\n",
      "loss: 0.496528  [  384/  569]\n",
      "loss: 0.463909  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7970e-09.\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.540258  [    0/  569]\n",
      "loss: 0.538900  [  128/  569]\n",
      "loss: 0.477812  [  256/  569]\n",
      "loss: 0.475594  [  384/  569]\n",
      "loss: 0.490827  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6173e-09.\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.528225  [    0/  569]\n",
      "loss: 0.466919  [  128/  569]\n",
      "loss: 0.500132  [  256/  569]\n",
      "loss: 0.510016  [  384/  569]\n",
      "loss: 0.474327  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4556e-09.\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.466182  [    0/  569]\n",
      "loss: 0.463003  [  128/  569]\n",
      "loss: 0.539605  [  256/  569]\n",
      "loss: 0.507052  [  384/  569]\n",
      "loss: 0.494144  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3100e-09.\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.469382  [    0/  569]\n",
      "loss: 0.517278  [  128/  569]\n",
      "loss: 0.501022  [  256/  569]\n",
      "loss: 0.547259  [  384/  569]\n",
      "loss: 0.489662  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1790e-09.\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.489871  [    0/  569]\n",
      "loss: 0.455900  [  128/  569]\n",
      "loss: 0.501261  [  256/  569]\n",
      "loss: 0.508319  [  384/  569]\n",
      "loss: 0.547618  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0611e-09.\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.544480  [    0/  569]\n",
      "loss: 0.533091  [  128/  569]\n",
      "loss: 0.496924  [  256/  569]\n",
      "loss: 0.445548  [  384/  569]\n",
      "loss: 0.507230  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.5500e-10.\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.517034  [    0/  569]\n",
      "loss: 0.506184  [  128/  569]\n",
      "loss: 0.495331  [  256/  569]\n",
      "loss: 0.493389  [  384/  569]\n",
      "loss: 0.462502  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.5950e-10.\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.495515  [    0/  569]\n",
      "loss: 0.487748  [  128/  569]\n",
      "loss: 0.510486  [  256/  569]\n",
      "loss: 0.506239  [  384/  569]\n",
      "loss: 0.507630  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.7355e-10.\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.511184  [    0/  569]\n",
      "loss: 0.509017  [  128/  569]\n",
      "loss: 0.528722  [  256/  569]\n",
      "loss: 0.487461  [  384/  569]\n",
      "loss: 0.455201  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.9620e-10.\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.529193  [    0/  569]\n",
      "loss: 0.480591  [  128/  569]\n",
      "loss: 0.459722  [  256/  569]\n",
      "loss: 0.462968  [  384/  569]\n",
      "loss: 0.488526  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.2658e-10.\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.544589  [    0/  569]\n",
      "loss: 0.478984  [  128/  569]\n",
      "loss: 0.476635  [  256/  569]\n",
      "loss: 0.499041  [  384/  569]\n",
      "loss: 0.530381  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.6392e-10.\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.547231  [    0/  569]\n",
      "loss: 0.514326  [  128/  569]\n",
      "loss: 0.509216  [  256/  569]\n",
      "loss: 0.476351  [  384/  569]\n",
      "loss: 0.513303  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.0753e-10.\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.442867  [    0/  569]\n",
      "loss: 0.513048  [  128/  569]\n",
      "loss: 0.487988  [  256/  569]\n",
      "loss: 0.497743  [  384/  569]\n",
      "loss: 0.517869  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.5678e-10.\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.528598  [    0/  569]\n",
      "loss: 0.492662  [  128/  569]\n",
      "loss: 0.499053  [  256/  569]\n",
      "loss: 0.436989  [  384/  569]\n",
      "loss: 0.473751  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.1110e-10.\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.511197  [    0/  569]\n",
      "loss: 0.478010  [  128/  569]\n",
      "loss: 0.521174  [  256/  569]\n",
      "loss: 0.508601  [  384/  569]\n",
      "loss: 0.505819  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.6999e-10.\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.468385  [    0/  569]\n",
      "loss: 0.478909  [  128/  569]\n",
      "loss: 0.537562  [  256/  569]\n",
      "loss: 0.517813  [  384/  569]\n",
      "loss: 0.520740  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.3299e-10.\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.538166  [    0/  569]\n",
      "loss: 0.467489  [  128/  569]\n",
      "loss: 0.476773  [  256/  569]\n",
      "loss: 0.526441  [  384/  569]\n",
      "loss: 0.515628  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9969e-10.\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.512058  [    0/  569]\n",
      "loss: 0.512533  [  128/  569]\n",
      "loss: 0.499015  [  256/  569]\n",
      "loss: 0.521492  [  384/  569]\n",
      "loss: 0.518408  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6972e-10.\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.495540  [    0/  569]\n",
      "loss: 0.483849  [  128/  569]\n",
      "loss: 0.460444  [  256/  569]\n",
      "loss: 0.481780  [  384/  569]\n",
      "loss: 0.453971  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4275e-10.\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.489256  [    0/  569]\n",
      "loss: 0.473403  [  128/  569]\n",
      "loss: 0.506930  [  256/  569]\n",
      "loss: 0.491528  [  384/  569]\n",
      "loss: 0.533484  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1847e-10.\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.471221  [    0/  569]\n",
      "loss: 0.494244  [  128/  569]\n",
      "loss: 0.498454  [  256/  569]\n",
      "loss: 0.542599  [  384/  569]\n",
      "loss: 0.534668  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9663e-10.\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.465882  [    0/  569]\n",
      "loss: 0.510530  [  128/  569]\n",
      "loss: 0.538264  [  256/  569]\n",
      "loss: 0.560260  [  384/  569]\n",
      "loss: 0.459739  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7696e-10.\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.531984  [    0/  569]\n",
      "loss: 0.545156  [  128/  569]\n",
      "loss: 0.483803  [  256/  569]\n",
      "loss: 0.491703  [  384/  569]\n",
      "loss: 0.522034  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5927e-10.\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.521863  [    0/  569]\n",
      "loss: 0.508721  [  128/  569]\n",
      "loss: 0.448368  [  256/  569]\n",
      "loss: 0.487397  [  384/  569]\n",
      "loss: 0.509731  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4334e-10.\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.469272  [    0/  569]\n",
      "loss: 0.504610  [  128/  569]\n",
      "loss: 0.487518  [  256/  569]\n",
      "loss: 0.515350  [  384/  569]\n",
      "loss: 0.492900  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2901e-10.\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.506559  [    0/  569]\n",
      "loss: 0.505726  [  128/  569]\n",
      "loss: 0.534777  [  256/  569]\n",
      "loss: 0.484176  [  384/  569]\n",
      "loss: 0.485883  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1611e-10.\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.468654  [    0/  569]\n",
      "loss: 0.514190  [  128/  569]\n",
      "loss: 0.493239  [  256/  569]\n",
      "loss: 0.477275  [  384/  569]\n",
      "loss: 0.496789  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0450e-10.\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.461657  [    0/  569]\n",
      "loss: 0.536970  [  128/  569]\n",
      "loss: 0.549942  [  256/  569]\n",
      "loss: 0.524668  [  384/  569]\n",
      "loss: 0.483575  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.4046e-11.\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.464885  [    0/  569]\n",
      "loss: 0.518763  [  128/  569]\n",
      "loss: 0.464531  [  256/  569]\n",
      "loss: 0.542580  [  384/  569]\n",
      "loss: 0.546742  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.4641e-11.\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.488966  [    0/  569]\n",
      "loss: 0.518310  [  128/  569]\n",
      "loss: 0.497357  [  256/  569]\n",
      "loss: 0.541406  [  384/  569]\n",
      "loss: 0.482687  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.6177e-11.\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.510094  [    0/  569]\n",
      "loss: 0.472004  [  128/  569]\n",
      "loss: 0.488668  [  256/  569]\n",
      "loss: 0.460119  [  384/  569]\n",
      "loss: 0.498717  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.8560e-11.\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.483243  [    0/  569]\n",
      "loss: 0.503060  [  128/  569]\n",
      "loss: 0.493318  [  256/  569]\n",
      "loss: 0.514052  [  384/  569]\n",
      "loss: 0.510354  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.1704e-11.\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.513585  [    0/  569]\n",
      "loss: 0.515113  [  128/  569]\n",
      "loss: 0.486824  [  256/  569]\n",
      "loss: 0.514460  [  384/  569]\n",
      "loss: 0.437147  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.5533e-11.\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.542374  [    0/  569]\n",
      "loss: 0.488225  [  128/  569]\n",
      "loss: 0.527367  [  256/  569]\n",
      "loss: 0.482860  [  384/  569]\n",
      "loss: 0.486179  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.9980e-11.\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.481979  [    0/  569]\n",
      "loss: 0.508090  [  128/  569]\n",
      "loss: 0.502920  [  256/  569]\n",
      "loss: 0.513001  [  384/  569]\n",
      "loss: 0.459207  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.4982e-11.\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.458351  [    0/  569]\n",
      "loss: 0.460693  [  128/  569]\n",
      "loss: 0.485231  [  256/  569]\n",
      "loss: 0.554710  [  384/  569]\n",
      "loss: 0.483311  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.0484e-11.\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.496888  [    0/  569]\n",
      "loss: 0.476655  [  128/  569]\n",
      "loss: 0.536535  [  256/  569]\n",
      "loss: 0.458409  [  384/  569]\n",
      "loss: 0.455738  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.6435e-11.\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.518613  [    0/  569]\n",
      "loss: 0.485974  [  128/  569]\n",
      "loss: 0.507691  [  256/  569]\n",
      "loss: 0.490814  [  384/  569]\n",
      "loss: 0.526696  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.2792e-11.\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.501810  [    0/  569]\n",
      "loss: 0.465781  [  128/  569]\n",
      "loss: 0.462413  [  256/  569]\n",
      "loss: 0.518169  [  384/  569]\n",
      "loss: 0.482741  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9513e-11.\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.543606  [    0/  569]\n",
      "loss: 0.505454  [  128/  569]\n",
      "loss: 0.428046  [  256/  569]\n",
      "loss: 0.521903  [  384/  569]\n",
      "loss: 0.496830  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6561e-11.\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.528019  [    0/  569]\n",
      "loss: 0.515907  [  128/  569]\n",
      "loss: 0.491923  [  256/  569]\n",
      "loss: 0.460697  [  384/  569]\n",
      "loss: 0.494803  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3905e-11.\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.490653  [    0/  569]\n",
      "loss: 0.487086  [  128/  569]\n",
      "loss: 0.490656  [  256/  569]\n",
      "loss: 0.501073  [  384/  569]\n",
      "loss: 0.522510  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1515e-11.\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.550023  [    0/  569]\n",
      "loss: 0.475324  [  128/  569]\n",
      "loss: 0.540184  [  256/  569]\n",
      "loss: 0.477216  [  384/  569]\n",
      "loss: 0.483316  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9363e-11.\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.499180  [    0/  569]\n",
      "loss: 0.510597  [  128/  569]\n",
      "loss: 0.560493  [  256/  569]\n",
      "loss: 0.497407  [  384/  569]\n",
      "loss: 0.480743  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7427e-11.\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.494684  [    0/  569]\n",
      "loss: 0.481558  [  128/  569]\n",
      "loss: 0.469160  [  256/  569]\n",
      "loss: 0.463141  [  384/  569]\n",
      "loss: 0.527847  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5684e-11.\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.448616  [    0/  569]\n",
      "loss: 0.539103  [  128/  569]\n",
      "loss: 0.475011  [  256/  569]\n",
      "loss: 0.449572  [  384/  569]\n",
      "loss: 0.505098  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4116e-11.\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.504201  [    0/  569]\n",
      "loss: 0.506068  [  128/  569]\n",
      "loss: 0.542072  [  256/  569]\n",
      "loss: 0.470538  [  384/  569]\n",
      "loss: 0.486948  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2704e-11.\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.531184  [    0/  569]\n",
      "loss: 0.466311  [  128/  569]\n",
      "loss: 0.522754  [  256/  569]\n",
      "loss: 0.478720  [  384/  569]\n",
      "loss: 0.462963  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1434e-11.\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.564438  [    0/  569]\n",
      "loss: 0.482891  [  128/  569]\n",
      "loss: 0.489007  [  256/  569]\n",
      "loss: 0.503587  [  384/  569]\n",
      "loss: 0.506566  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0290e-11.\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.511012  [    0/  569]\n",
      "loss: 0.494740  [  128/  569]\n",
      "loss: 0.510648  [  256/  569]\n",
      "loss: 0.482035  [  384/  569]\n",
      "loss: 0.575570  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.2614e-12.\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.552126  [    0/  569]\n",
      "loss: 0.453966  [  128/  569]\n",
      "loss: 0.445600  [  256/  569]\n",
      "loss: 0.484005  [  384/  569]\n",
      "loss: 0.510270  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.3352e-12.\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.479530  [    0/  569]\n",
      "loss: 0.510075  [  128/  569]\n",
      "loss: 0.487840  [  256/  569]\n",
      "loss: 0.512492  [  384/  569]\n",
      "loss: 0.509951  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.5017e-12.\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.588910  [    0/  569]\n",
      "loss: 0.473921  [  128/  569]\n",
      "loss: 0.476425  [  256/  569]\n",
      "loss: 0.451225  [  384/  569]\n",
      "loss: 0.540173  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.7516e-12.\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.493967  [    0/  569]\n",
      "loss: 0.480966  [  128/  569]\n",
      "loss: 0.507361  [  256/  569]\n",
      "loss: 0.441245  [  384/  569]\n",
      "loss: 0.523414  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.0764e-12.\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.512787  [    0/  569]\n",
      "loss: 0.492659  [  128/  569]\n",
      "loss: 0.476000  [  256/  569]\n",
      "loss: 0.493111  [  384/  569]\n",
      "loss: 0.513154  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.4688e-12.\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.515412  [    0/  569]\n",
      "loss: 0.535044  [  128/  569]\n",
      "loss: 0.460570  [  256/  569]\n",
      "loss: 0.446926  [  384/  569]\n",
      "loss: 0.512063  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.9219e-12.\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.482819  [    0/  569]\n",
      "loss: 0.495465  [  128/  569]\n",
      "loss: 0.509157  [  256/  569]\n",
      "loss: 0.505263  [  384/  569]\n",
      "loss: 0.526350  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.4297e-12.\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.472842  [    0/  569]\n",
      "loss: 0.458773  [  128/  569]\n",
      "loss: 0.517007  [  256/  569]\n",
      "loss: 0.562091  [  384/  569]\n",
      "loss: 0.506213  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.9867e-12.\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.450741  [    0/  569]\n",
      "loss: 0.502797  [  128/  569]\n",
      "loss: 0.512904  [  256/  569]\n",
      "loss: 0.510964  [  384/  569]\n",
      "loss: 0.503396  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.5881e-12.\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.527593  [    0/  569]\n",
      "loss: 0.494285  [  128/  569]\n",
      "loss: 0.476006  [  256/  569]\n",
      "loss: 0.483218  [  384/  569]\n",
      "loss: 0.458619  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.2292e-12.\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.477557  [    0/  569]\n",
      "loss: 0.467548  [  128/  569]\n",
      "loss: 0.521820  [  256/  569]\n",
      "loss: 0.495497  [  384/  569]\n",
      "loss: 0.496830  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9063e-12.\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.500565  [    0/  569]\n",
      "loss: 0.453307  [  128/  569]\n",
      "loss: 0.496626  [  256/  569]\n",
      "loss: 0.508812  [  384/  569]\n",
      "loss: 0.570848  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6157e-12.\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.474291  [    0/  569]\n",
      "loss: 0.552022  [  128/  569]\n",
      "loss: 0.459204  [  256/  569]\n",
      "loss: 0.475775  [  384/  569]\n",
      "loss: 0.531517  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3541e-12.\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.517363  [    0/  569]\n",
      "loss: 0.501189  [  128/  569]\n",
      "loss: 0.468440  [  256/  569]\n",
      "loss: 0.460089  [  384/  569]\n",
      "loss: 0.456230  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1187e-12.\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.492747  [    0/  569]\n",
      "loss: 0.516515  [  128/  569]\n",
      "loss: 0.529709  [  256/  569]\n",
      "loss: 0.413670  [  384/  569]\n",
      "loss: 0.528989  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9068e-12.\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.507641  [    0/  569]\n",
      "loss: 0.497262  [  128/  569]\n",
      "loss: 0.462657  [  256/  569]\n",
      "loss: 0.492829  [  384/  569]\n",
      "loss: 0.555047  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7162e-12.\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.561316  [    0/  569]\n",
      "loss: 0.533379  [  128/  569]\n",
      "loss: 0.473767  [  256/  569]\n",
      "loss: 0.494417  [  384/  569]\n",
      "loss: 0.498940  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5445e-12.\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.512937  [    0/  569]\n",
      "loss: 0.490882  [  128/  569]\n",
      "loss: 0.480246  [  256/  569]\n",
      "loss: 0.491655  [  384/  569]\n",
      "loss: 0.506824  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3901e-12.\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.510897  [    0/  569]\n",
      "loss: 0.487770  [  128/  569]\n",
      "loss: 0.501157  [  256/  569]\n",
      "loss: 0.512338  [  384/  569]\n",
      "loss: 0.551652  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2511e-12.\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.560898  [    0/  569]\n",
      "loss: 0.500954  [  128/  569]\n",
      "loss: 0.482901  [  256/  569]\n",
      "loss: 0.524340  [  384/  569]\n",
      "loss: 0.468772  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1260e-12.\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.455694  [    0/  569]\n",
      "loss: 0.507631  [  128/  569]\n",
      "loss: 0.530842  [  256/  569]\n",
      "loss: 0.498616  [  384/  569]\n",
      "loss: 0.477604  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0134e-12.\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.467357  [    0/  569]\n",
      "loss: 0.499926  [  128/  569]\n",
      "loss: 0.528849  [  256/  569]\n",
      "loss: 0.516168  [  384/  569]\n",
      "loss: 0.540790  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.1203e-13.\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.501815  [    0/  569]\n",
      "loss: 0.572973  [  128/  569]\n",
      "loss: 0.448580  [  256/  569]\n",
      "loss: 0.431323  [  384/  569]\n",
      "loss: 0.558813  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.2083e-13.\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.526354  [    0/  569]\n",
      "loss: 0.480863  [  128/  569]\n",
      "loss: 0.477408  [  256/  569]\n",
      "loss: 0.504237  [  384/  569]\n",
      "loss: 0.471204  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.3875e-13.\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.468574  [    0/  569]\n",
      "loss: 0.474566  [  128/  569]\n",
      "loss: 0.510452  [  256/  569]\n",
      "loss: 0.503130  [  384/  569]\n",
      "loss: 0.532016  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.6487e-13.\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.464325  [    0/  569]\n",
      "loss: 0.515763  [  128/  569]\n",
      "loss: 0.492339  [  256/  569]\n",
      "loss: 0.496224  [  384/  569]\n",
      "loss: 0.442114  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.9839e-13.\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.509163  [    0/  569]\n",
      "loss: 0.508567  [  128/  569]\n",
      "loss: 0.520809  [  256/  569]\n",
      "loss: 0.491551  [  384/  569]\n",
      "loss: 0.450116  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.3855e-13.\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.512252  [    0/  569]\n",
      "loss: 0.549549  [  128/  569]\n",
      "loss: 0.463445  [  256/  569]\n",
      "loss: 0.484445  [  384/  569]\n",
      "loss: 0.497782  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.8469e-13.\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.516500  [    0/  569]\n",
      "loss: 0.516795  [  128/  569]\n",
      "loss: 0.475910  [  256/  569]\n",
      "loss: 0.517134  [  384/  569]\n",
      "loss: 0.484728  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.3622e-13.\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.496452  [    0/  569]\n",
      "loss: 0.508657  [  128/  569]\n",
      "loss: 0.458428  [  256/  569]\n",
      "loss: 0.498024  [  384/  569]\n",
      "loss: 0.520147  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.9260e-13.\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.489795  [    0/  569]\n",
      "loss: 0.488979  [  128/  569]\n",
      "loss: 0.562903  [  256/  569]\n",
      "loss: 0.467321  [  384/  569]\n",
      "loss: 0.499087  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.5334e-13.\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.520460  [    0/  569]\n",
      "loss: 0.501561  [  128/  569]\n",
      "loss: 0.519644  [  256/  569]\n",
      "loss: 0.491827  [  384/  569]\n",
      "loss: 0.532334  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.1801e-13.\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.487667  [    0/  569]\n",
      "loss: 0.546342  [  128/  569]\n",
      "loss: 0.437461  [  256/  569]\n",
      "loss: 0.545293  [  384/  569]\n",
      "loss: 0.495657  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8621e-13.\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.523943  [    0/  569]\n",
      "loss: 0.551305  [  128/  569]\n",
      "loss: 0.501731  [  256/  569]\n",
      "loss: 0.518740  [  384/  569]\n",
      "loss: 0.463369  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5759e-13.\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.461034  [    0/  569]\n",
      "loss: 0.484188  [  128/  569]\n",
      "loss: 0.494195  [  256/  569]\n",
      "loss: 0.461414  [  384/  569]\n",
      "loss: 0.500961  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3183e-13.\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.504134  [    0/  569]\n",
      "loss: 0.518764  [  128/  569]\n",
      "loss: 0.497661  [  256/  569]\n",
      "loss: 0.491247  [  384/  569]\n",
      "loss: 0.462965  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0864e-13.\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.537605  [    0/  569]\n",
      "loss: 0.507553  [  128/  569]\n",
      "loss: 0.482770  [  256/  569]\n",
      "loss: 0.526611  [  384/  569]\n",
      "loss: 0.479571  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8778e-13.\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.451117  [    0/  569]\n",
      "loss: 0.477581  [  128/  569]\n",
      "loss: 0.495029  [  256/  569]\n",
      "loss: 0.516379  [  384/  569]\n",
      "loss: 0.526880  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6900e-13.\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.492257  [    0/  569]\n",
      "loss: 0.500952  [  128/  569]\n",
      "loss: 0.493363  [  256/  569]\n",
      "loss: 0.482377  [  384/  569]\n",
      "loss: 0.528384  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5210e-13.\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.510572  [    0/  569]\n",
      "loss: 0.510922  [  128/  569]\n",
      "loss: 0.487814  [  256/  569]\n",
      "loss: 0.469054  [  384/  569]\n",
      "loss: 0.513631  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3689e-13.\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.451356  [    0/  569]\n",
      "loss: 0.480286  [  128/  569]\n",
      "loss: 0.582353  [  256/  569]\n",
      "loss: 0.465388  [  384/  569]\n",
      "loss: 0.550198  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2320e-13.\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.519319  [    0/  569]\n",
      "loss: 0.520557  [  128/  569]\n",
      "loss: 0.486170  [  256/  569]\n",
      "loss: 0.486935  [  384/  569]\n",
      "loss: 0.527075  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1088e-13.\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.517002  [    0/  569]\n",
      "loss: 0.494048  [  128/  569]\n",
      "loss: 0.492537  [  256/  569]\n",
      "loss: 0.506634  [  384/  569]\n",
      "loss: 0.538270  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.9794e-14.\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.490111  [    0/  569]\n",
      "loss: 0.538932  [  128/  569]\n",
      "loss: 0.477367  [  256/  569]\n",
      "loss: 0.492942  [  384/  569]\n",
      "loss: 0.478052  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.9814e-14.\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.453152  [    0/  569]\n",
      "loss: 0.500596  [  128/  569]\n",
      "loss: 0.495698  [  256/  569]\n",
      "loss: 0.480492  [  384/  569]\n",
      "loss: 0.567362  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.0833e-14.\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.546849  [    0/  569]\n",
      "loss: 0.489891  [  128/  569]\n",
      "loss: 0.495336  [  256/  569]\n",
      "loss: 0.491221  [  384/  569]\n",
      "loss: 0.509491  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.2750e-14.\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.542795  [    0/  569]\n",
      "loss: 0.482663  [  128/  569]\n",
      "loss: 0.502174  [  256/  569]\n",
      "loss: 0.519827  [  384/  569]\n",
      "loss: 0.455177  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.5475e-14.\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.494802  [    0/  569]\n",
      "loss: 0.580008  [  128/  569]\n",
      "loss: 0.424957  [  256/  569]\n",
      "loss: 0.545045  [  384/  569]\n",
      "loss: 0.481816  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.8927e-14.\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.505078  [    0/  569]\n",
      "loss: 0.529750  [  128/  569]\n",
      "loss: 0.486220  [  256/  569]\n",
      "loss: 0.520822  [  384/  569]\n",
      "loss: 0.507890  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.3035e-14.\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.510465  [    0/  569]\n",
      "loss: 0.456398  [  128/  569]\n",
      "loss: 0.475966  [  256/  569]\n",
      "loss: 0.491645  [  384/  569]\n",
      "loss: 0.470829  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.7731e-14.\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.512082  [    0/  569]\n",
      "loss: 0.484488  [  128/  569]\n",
      "loss: 0.513838  [  256/  569]\n",
      "loss: 0.487711  [  384/  569]\n",
      "loss: 0.537047  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.2958e-14.\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.463105  [    0/  569]\n",
      "loss: 0.465952  [  128/  569]\n",
      "loss: 0.531200  [  256/  569]\n",
      "loss: 0.484937  [  384/  569]\n",
      "loss: 0.514926  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.8662e-14.\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.462659  [    0/  569]\n",
      "loss: 0.534569  [  128/  569]\n",
      "loss: 0.462836  [  256/  569]\n",
      "loss: 0.496244  [  384/  569]\n",
      "loss: 0.492265  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4796e-14.\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.516116  [    0/  569]\n",
      "loss: 0.487251  [  128/  569]\n",
      "loss: 0.456919  [  256/  569]\n",
      "loss: 0.481269  [  384/  569]\n",
      "loss: 0.480343  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.1316e-14.\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.518299  [    0/  569]\n",
      "loss: 0.477653  [  128/  569]\n",
      "loss: 0.533875  [  256/  569]\n",
      "loss: 0.511435  [  384/  569]\n",
      "loss: 0.510596  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8185e-14.\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.500228  [    0/  569]\n",
      "loss: 0.500993  [  128/  569]\n",
      "loss: 0.491220  [  256/  569]\n",
      "loss: 0.508503  [  384/  569]\n",
      "loss: 0.516329  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5366e-14.\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.503728  [    0/  569]\n",
      "loss: 0.467043  [  128/  569]\n",
      "loss: 0.520371  [  256/  569]\n",
      "loss: 0.514476  [  384/  569]\n",
      "loss: 0.476302  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2830e-14.\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.417506  [    0/  569]\n",
      "loss: 0.465370  [  128/  569]\n",
      "loss: 0.503033  [  256/  569]\n",
      "loss: 0.517396  [  384/  569]\n",
      "loss: 0.496021  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0547e-14.\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.504853  [    0/  569]\n",
      "loss: 0.511941  [  128/  569]\n",
      "loss: 0.532412  [  256/  569]\n",
      "loss: 0.468701  [  384/  569]\n",
      "loss: 0.441687  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8492e-14.\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.520251  [    0/  569]\n",
      "loss: 0.537704  [  128/  569]\n",
      "loss: 0.544160  [  256/  569]\n",
      "loss: 0.493417  [  384/  569]\n",
      "loss: 0.508370  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6643e-14.\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.543527  [    0/  569]\n",
      "loss: 0.523901  [  128/  569]\n",
      "loss: 0.515636  [  256/  569]\n",
      "loss: 0.476600  [  384/  569]\n",
      "loss: 0.470543  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4979e-14.\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.486665  [    0/  569]\n",
      "loss: 0.479056  [  128/  569]\n",
      "loss: 0.514605  [  256/  569]\n",
      "loss: 0.556586  [  384/  569]\n",
      "loss: 0.447672  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3481e-14.\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.488394  [    0/  569]\n",
      "loss: 0.451616  [  128/  569]\n",
      "loss: 0.500093  [  256/  569]\n",
      "loss: 0.536339  [  384/  569]\n",
      "loss: 0.471672  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2133e-14.\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.436453  [    0/  569]\n",
      "loss: 0.518313  [  128/  569]\n",
      "loss: 0.533180  [  256/  569]\n",
      "loss: 0.474981  [  384/  569]\n",
      "loss: 0.515828  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0919e-14.\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.485319  [    0/  569]\n",
      "loss: 0.485986  [  128/  569]\n",
      "loss: 0.491267  [  256/  569]\n",
      "loss: 0.534494  [  384/  569]\n",
      "loss: 0.473731  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.8274e-15.\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.532200  [    0/  569]\n",
      "loss: 0.487913  [  128/  569]\n",
      "loss: 0.488740  [  256/  569]\n",
      "loss: 0.512836  [  384/  569]\n",
      "loss: 0.548860  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.8447e-15.\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.502004  [    0/  569]\n",
      "loss: 0.517035  [  128/  569]\n",
      "loss: 0.490583  [  256/  569]\n",
      "loss: 0.518344  [  384/  569]\n",
      "loss: 0.480092  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.9602e-15.\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.498790  [    0/  569]\n",
      "loss: 0.508726  [  128/  569]\n",
      "loss: 0.541805  [  256/  569]\n",
      "loss: 0.491751  [  384/  569]\n",
      "loss: 0.511688  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.1642e-15.\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.495658  [    0/  569]\n",
      "loss: 0.531825  [  128/  569]\n",
      "loss: 0.525024  [  256/  569]\n",
      "loss: 0.518774  [  384/  569]\n",
      "loss: 0.551653  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.4478e-15.\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.533215  [    0/  569]\n",
      "loss: 0.479237  [  128/  569]\n",
      "loss: 0.515124  [  256/  569]\n",
      "loss: 0.489412  [  384/  569]\n",
      "loss: 0.535774  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.8030e-15.\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.554592  [    0/  569]\n",
      "loss: 0.513988  [  128/  569]\n",
      "loss: 0.483060  [  256/  569]\n",
      "loss: 0.491284  [  384/  569]\n",
      "loss: 0.474992  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.2227e-15.\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.522555  [    0/  569]\n",
      "loss: 0.491559  [  128/  569]\n",
      "loss: 0.514299  [  256/  569]\n",
      "loss: 0.503876  [  384/  569]\n",
      "loss: 0.512343  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.7004e-15.\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.511942  [    0/  569]\n",
      "loss: 0.488453  [  128/  569]\n",
      "loss: 0.457377  [  256/  569]\n",
      "loss: 0.563836  [  384/  569]\n",
      "loss: 0.486006  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.2304e-15.\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.501652  [    0/  569]\n",
      "loss: 0.522642  [  128/  569]\n",
      "loss: 0.508984  [  256/  569]\n",
      "loss: 0.448507  [  384/  569]\n",
      "loss: 0.488969  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.8073e-15.\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.525836  [    0/  569]\n",
      "loss: 0.436563  [  128/  569]\n",
      "loss: 0.511442  [  256/  569]\n",
      "loss: 0.541805  [  384/  569]\n",
      "loss: 0.524896  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4266e-15.\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.491900  [    0/  569]\n",
      "loss: 0.482274  [  128/  569]\n",
      "loss: 0.490986  [  256/  569]\n",
      "loss: 0.472464  [  384/  569]\n",
      "loss: 0.476203  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.0839e-15.\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.522933  [    0/  569]\n",
      "loss: 0.431353  [  128/  569]\n",
      "loss: 0.545916  [  256/  569]\n",
      "loss: 0.492759  [  384/  569]\n",
      "loss: 0.488925  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.7756e-15.\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.484284  [    0/  569]\n",
      "loss: 0.487066  [  128/  569]\n",
      "loss: 0.551673  [  256/  569]\n",
      "loss: 0.501119  [  384/  569]\n",
      "loss: 0.481092  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4980e-15.\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.521042  [    0/  569]\n",
      "loss: 0.498071  [  128/  569]\n",
      "loss: 0.499942  [  256/  569]\n",
      "loss: 0.518905  [  384/  569]\n",
      "loss: 0.458581  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2482e-15.\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.514530  [    0/  569]\n",
      "loss: 0.466137  [  128/  569]\n",
      "loss: 0.541270  [  256/  569]\n",
      "loss: 0.473958  [  384/  569]\n",
      "loss: 0.516724  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0234e-15.\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.483280  [    0/  569]\n",
      "loss: 0.518715  [  128/  569]\n",
      "loss: 0.492146  [  256/  569]\n",
      "loss: 0.507777  [  384/  569]\n",
      "loss: 0.490120  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8210e-15.\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.481500  [    0/  569]\n",
      "loss: 0.538893  [  128/  569]\n",
      "loss: 0.477823  [  256/  569]\n",
      "loss: 0.496603  [  384/  569]\n",
      "loss: 0.503596  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6389e-15.\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.489460  [    0/  569]\n",
      "loss: 0.486408  [  128/  569]\n",
      "loss: 0.496787  [  256/  569]\n",
      "loss: 0.513922  [  384/  569]\n",
      "loss: 0.483097  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4750e-15.\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.496376  [    0/  569]\n",
      "loss: 0.509013  [  128/  569]\n",
      "loss: 0.512522  [  256/  569]\n",
      "loss: 0.536303  [  384/  569]\n",
      "loss: 0.483818  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3275e-15.\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.496708  [    0/  569]\n",
      "loss: 0.499758  [  128/  569]\n",
      "loss: 0.492769  [  256/  569]\n",
      "loss: 0.501804  [  384/  569]\n",
      "loss: 0.530123  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1948e-15.\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.515770  [    0/  569]\n",
      "loss: 0.423395  [  128/  569]\n",
      "loss: 0.510710  [  256/  569]\n",
      "loss: 0.485405  [  384/  569]\n",
      "loss: 0.527287  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0753e-15.\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.485933  [    0/  569]\n",
      "loss: 0.510580  [  128/  569]\n",
      "loss: 0.554716  [  256/  569]\n",
      "loss: 0.481081  [  384/  569]\n",
      "loss: 0.514008  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.6777e-16.\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.501099  [    0/  569]\n",
      "loss: 0.521275  [  128/  569]\n",
      "loss: 0.484005  [  256/  569]\n",
      "loss: 0.561713  [  384/  569]\n",
      "loss: 0.438174  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.7100e-16.\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.518991  [    0/  569]\n",
      "loss: 0.506850  [  128/  569]\n",
      "loss: 0.510866  [  256/  569]\n",
      "loss: 0.460397  [  384/  569]\n",
      "loss: 0.492411  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.8390e-16.\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.495857  [    0/  569]\n",
      "loss: 0.521680  [  128/  569]\n",
      "loss: 0.518392  [  256/  569]\n",
      "loss: 0.457526  [  384/  569]\n",
      "loss: 0.517470  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.0551e-16.\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.431295  [    0/  569]\n",
      "loss: 0.520390  [  128/  569]\n",
      "loss: 0.439626  [  256/  569]\n",
      "loss: 0.513611  [  384/  569]\n",
      "loss: 0.520754  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.3496e-16.\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.525096  [    0/  569]\n",
      "loss: 0.470094  [  128/  569]\n",
      "loss: 0.527239  [  256/  569]\n",
      "loss: 0.491543  [  384/  569]\n",
      "loss: 0.480479  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.7146e-16.\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.503134  [    0/  569]\n",
      "loss: 0.473585  [  128/  569]\n",
      "loss: 0.487402  [  256/  569]\n",
      "loss: 0.424344  [  384/  569]\n",
      "loss: 0.507044  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.1432e-16.\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.481059  [    0/  569]\n",
      "loss: 0.483981  [  128/  569]\n",
      "loss: 0.489483  [  256/  569]\n",
      "loss: 0.526950  [  384/  569]\n",
      "loss: 0.489719  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.6288e-16.\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.488715  [    0/  569]\n",
      "loss: 0.455081  [  128/  569]\n",
      "loss: 0.476222  [  256/  569]\n",
      "loss: 0.545213  [  384/  569]\n",
      "loss: 0.571282  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.1660e-16.\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.479617  [    0/  569]\n",
      "loss: 0.478485  [  128/  569]\n",
      "loss: 0.533868  [  256/  569]\n",
      "loss: 0.466925  [  384/  569]\n",
      "loss: 0.494423  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.7494e-16.\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.527047  [    0/  569]\n",
      "loss: 0.443975  [  128/  569]\n",
      "loss: 0.468878  [  256/  569]\n",
      "loss: 0.492136  [  384/  569]\n",
      "loss: 0.490109  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.3744e-16.\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.501873  [    0/  569]\n",
      "loss: 0.496523  [  128/  569]\n",
      "loss: 0.544441  [  256/  569]\n",
      "loss: 0.478232  [  384/  569]\n",
      "loss: 0.465638  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.0370e-16.\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.553350  [    0/  569]\n",
      "loss: 0.485297  [  128/  569]\n",
      "loss: 0.492835  [  256/  569]\n",
      "loss: 0.499914  [  384/  569]\n",
      "loss: 0.503023  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.7333e-16.\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.494562  [    0/  569]\n",
      "loss: 0.501672  [  128/  569]\n",
      "loss: 0.564173  [  256/  569]\n",
      "loss: 0.479838  [  384/  569]\n",
      "loss: 0.482143  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4600e-16.\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.485221  [    0/  569]\n",
      "loss: 0.473226  [  128/  569]\n",
      "loss: 0.534691  [  256/  569]\n",
      "loss: 0.512745  [  384/  569]\n",
      "loss: 0.453756  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2140e-16.\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.520075  [    0/  569]\n",
      "loss: 0.495811  [  128/  569]\n",
      "loss: 0.519395  [  256/  569]\n",
      "loss: 0.470967  [  384/  569]\n",
      "loss: 0.485241  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9926e-16.\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.550611  [    0/  569]\n",
      "loss: 0.484570  [  128/  569]\n",
      "loss: 0.547552  [  256/  569]\n",
      "loss: 0.494555  [  384/  569]\n",
      "loss: 0.484409  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7933e-16.\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.485953  [    0/  569]\n",
      "loss: 0.461850  [  128/  569]\n",
      "loss: 0.476038  [  256/  569]\n",
      "loss: 0.510378  [  384/  569]\n",
      "loss: 0.477562  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6140e-16.\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.477115  [    0/  569]\n",
      "loss: 0.474345  [  128/  569]\n",
      "loss: 0.558079  [  256/  569]\n",
      "loss: 0.475876  [  384/  569]\n",
      "loss: 0.508040  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4526e-16.\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.525565  [    0/  569]\n",
      "loss: 0.494860  [  128/  569]\n",
      "loss: 0.505410  [  256/  569]\n",
      "loss: 0.469292  [  384/  569]\n",
      "loss: 0.524490  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3073e-16.\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.508326  [    0/  569]\n",
      "loss: 0.469715  [  128/  569]\n",
      "loss: 0.516361  [  256/  569]\n",
      "loss: 0.449993  [  384/  569]\n",
      "loss: 0.535807  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1766e-16.\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.491390  [    0/  569]\n",
      "loss: 0.534934  [  128/  569]\n",
      "loss: 0.513528  [  256/  569]\n",
      "loss: 0.444045  [  384/  569]\n",
      "loss: 0.470322  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0589e-16.\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.512583  [    0/  569]\n",
      "loss: 0.493544  [  128/  569]\n",
      "loss: 0.554342  [  256/  569]\n",
      "loss: 0.470209  [  384/  569]\n",
      "loss: 0.506974  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.5304e-17.\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.488051  [    0/  569]\n",
      "loss: 0.496478  [  128/  569]\n",
      "loss: 0.509692  [  256/  569]\n",
      "loss: 0.485341  [  384/  569]\n",
      "loss: 0.550352  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.5773e-17.\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.503165  [    0/  569]\n",
      "loss: 0.525244  [  128/  569]\n",
      "loss: 0.539261  [  256/  569]\n",
      "loss: 0.446058  [  384/  569]\n",
      "loss: 0.545669  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.7196e-17.\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.533989  [    0/  569]\n",
      "loss: 0.509272  [  128/  569]\n",
      "loss: 0.505346  [  256/  569]\n",
      "loss: 0.495243  [  384/  569]\n",
      "loss: 0.466119  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.9476e-17.\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.492856  [    0/  569]\n",
      "loss: 0.517591  [  128/  569]\n",
      "loss: 0.541626  [  256/  569]\n",
      "loss: 0.514749  [  384/  569]\n",
      "loss: 0.498665  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.2529e-17.\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.520530  [    0/  569]\n",
      "loss: 0.495404  [  128/  569]\n",
      "loss: 0.522115  [  256/  569]\n",
      "loss: 0.529529  [  384/  569]\n",
      "loss: 0.508201  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.6276e-17.\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.475208  [    0/  569]\n",
      "loss: 0.465175  [  128/  569]\n",
      "loss: 0.489456  [  256/  569]\n",
      "loss: 0.533810  [  384/  569]\n",
      "loss: 0.513915  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.0648e-17.\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.476701  [    0/  569]\n",
      "loss: 0.501287  [  128/  569]\n",
      "loss: 0.451599  [  256/  569]\n",
      "loss: 0.503276  [  384/  569]\n",
      "loss: 0.529436  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.5583e-17.\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.498472  [    0/  569]\n",
      "loss: 0.529768  [  128/  569]\n",
      "loss: 0.539615  [  256/  569]\n",
      "loss: 0.487293  [  384/  569]\n",
      "loss: 0.548846  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.1025e-17.\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.451279  [    0/  569]\n",
      "loss: 0.540834  [  128/  569]\n",
      "loss: 0.513540  [  256/  569]\n",
      "loss: 0.497431  [  384/  569]\n",
      "loss: 0.491923  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.6923e-17.\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.524553  [    0/  569]\n",
      "loss: 0.480812  [  128/  569]\n",
      "loss: 0.525307  [  256/  569]\n",
      "loss: 0.483391  [  384/  569]\n",
      "loss: 0.495077  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.3230e-17.\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.546470  [    0/  569]\n",
      "loss: 0.498942  [  128/  569]\n",
      "loss: 0.516149  [  256/  569]\n",
      "loss: 0.482448  [  384/  569]\n",
      "loss: 0.481311  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9907e-17.\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.514280  [    0/  569]\n",
      "loss: 0.549734  [  128/  569]\n",
      "loss: 0.567272  [  256/  569]\n",
      "loss: 0.493713  [  384/  569]\n",
      "loss: 0.442208  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6917e-17.\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.497759  [    0/  569]\n",
      "loss: 0.496631  [  128/  569]\n",
      "loss: 0.494670  [  256/  569]\n",
      "loss: 0.501502  [  384/  569]\n",
      "loss: 0.473889  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4225e-17.\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.518352  [    0/  569]\n",
      "loss: 0.542373  [  128/  569]\n",
      "loss: 0.477027  [  256/  569]\n",
      "loss: 0.496224  [  384/  569]\n",
      "loss: 0.509257  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1802e-17.\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.447424  [    0/  569]\n",
      "loss: 0.485468  [  128/  569]\n",
      "loss: 0.542457  [  256/  569]\n",
      "loss: 0.529665  [  384/  569]\n",
      "loss: 0.540453  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9622e-17.\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.487447  [    0/  569]\n",
      "loss: 0.491255  [  128/  569]\n",
      "loss: 0.502450  [  256/  569]\n",
      "loss: 0.542493  [  384/  569]\n",
      "loss: 0.495620  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7660e-17.\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.515627  [    0/  569]\n",
      "loss: 0.509625  [  128/  569]\n",
      "loss: 0.461007  [  256/  569]\n",
      "loss: 0.524715  [  384/  569]\n",
      "loss: 0.520179  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5894e-17.\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.495143  [    0/  569]\n",
      "loss: 0.525243  [  128/  569]\n",
      "loss: 0.501817  [  256/  569]\n",
      "loss: 0.525385  [  384/  569]\n",
      "loss: 0.507907  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4305e-17.\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.488193  [    0/  569]\n",
      "loss: 0.525397  [  128/  569]\n",
      "loss: 0.536534  [  256/  569]\n",
      "loss: 0.463270  [  384/  569]\n",
      "loss: 0.492126  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2874e-17.\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.500335  [    0/  569]\n",
      "loss: 0.508253  [  128/  569]\n",
      "loss: 0.480301  [  256/  569]\n",
      "loss: 0.541278  [  384/  569]\n",
      "loss: 0.481186  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1587e-17.\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.534910  [    0/  569]\n",
      "loss: 0.531174  [  128/  569]\n",
      "loss: 0.510791  [  256/  569]\n",
      "loss: 0.486943  [  384/  569]\n",
      "loss: 0.491136  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0428e-17.\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.494888  [    0/  569]\n",
      "loss: 0.491607  [  128/  569]\n",
      "loss: 0.557963  [  256/  569]\n",
      "loss: 0.535112  [  384/  569]\n",
      "loss: 0.481242  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.3852e-18.\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.516907  [    0/  569]\n",
      "loss: 0.523410  [  128/  569]\n",
      "loss: 0.466844  [  256/  569]\n",
      "loss: 0.484839  [  384/  569]\n",
      "loss: 0.501056  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.4467e-18.\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.489874  [    0/  569]\n",
      "loss: 0.515453  [  128/  569]\n",
      "loss: 0.510642  [  256/  569]\n",
      "loss: 0.486082  [  384/  569]\n",
      "loss: 0.472731  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.6020e-18.\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.495556  [    0/  569]\n",
      "loss: 0.535929  [  128/  569]\n",
      "loss: 0.505783  [  256/  569]\n",
      "loss: 0.479162  [  384/  569]\n",
      "loss: 0.480257  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.8418e-18.\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.516846  [    0/  569]\n",
      "loss: 0.485128  [  128/  569]\n",
      "loss: 0.505421  [  256/  569]\n",
      "loss: 0.527791  [  384/  569]\n",
      "loss: 0.505980  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.1576e-18.\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.461862  [    0/  569]\n",
      "loss: 0.527918  [  128/  569]\n",
      "loss: 0.464746  [  256/  569]\n",
      "loss: 0.481554  [  384/  569]\n",
      "loss: 0.495204  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.5419e-18.\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.473531  [    0/  569]\n",
      "loss: 0.547046  [  128/  569]\n",
      "loss: 0.529267  [  256/  569]\n",
      "loss: 0.477073  [  384/  569]\n",
      "loss: 0.477212  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.9877e-18.\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.507140  [    0/  569]\n",
      "loss: 0.517483  [  128/  569]\n",
      "loss: 0.513984  [  256/  569]\n",
      "loss: 0.506209  [  384/  569]\n",
      "loss: 0.489892  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.4889e-18.\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.525520  [    0/  569]\n",
      "loss: 0.524801  [  128/  569]\n",
      "loss: 0.556599  [  256/  569]\n",
      "loss: 0.513145  [  384/  569]\n",
      "loss: 0.481885  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.0400e-18.\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.494185  [    0/  569]\n",
      "loss: 0.496109  [  128/  569]\n",
      "loss: 0.481572  [  256/  569]\n",
      "loss: 0.482041  [  384/  569]\n",
      "loss: 0.490940  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.6360e-18.\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.492317  [    0/  569]\n",
      "loss: 0.507903  [  128/  569]\n",
      "loss: 0.501246  [  256/  569]\n",
      "loss: 0.551020  [  384/  569]\n",
      "loss: 0.553190  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.2724e-18.\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.525579  [    0/  569]\n",
      "loss: 0.488819  [  128/  569]\n",
      "loss: 0.478513  [  256/  569]\n",
      "loss: 0.482569  [  384/  569]\n",
      "loss: 0.538539  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9452e-18.\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.508370  [    0/  569]\n",
      "loss: 0.489647  [  128/  569]\n",
      "loss: 0.485938  [  256/  569]\n",
      "loss: 0.502047  [  384/  569]\n",
      "loss: 0.477188  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6507e-18.\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.604078  [    0/  569]\n",
      "loss: 0.526384  [  128/  569]\n",
      "loss: 0.487022  [  256/  569]\n",
      "loss: 0.529298  [  384/  569]\n",
      "loss: 0.476989  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3856e-18.\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.512414  [    0/  569]\n",
      "loss: 0.509962  [  128/  569]\n",
      "loss: 0.492727  [  256/  569]\n",
      "loss: 0.468249  [  384/  569]\n",
      "loss: 0.497641  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1470e-18.\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.465220  [    0/  569]\n",
      "loss: 0.537465  [  128/  569]\n",
      "loss: 0.518251  [  256/  569]\n",
      "loss: 0.471852  [  384/  569]\n",
      "loss: 0.559464  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9323e-18.\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.489105  [    0/  569]\n",
      "loss: 0.490021  [  128/  569]\n",
      "loss: 0.513324  [  256/  569]\n",
      "loss: 0.525192  [  384/  569]\n",
      "loss: 0.486652  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7391e-18.\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.464545  [    0/  569]\n",
      "loss: 0.521043  [  128/  569]\n",
      "loss: 0.572588  [  256/  569]\n",
      "loss: 0.450427  [  384/  569]\n",
      "loss: 0.463993  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5652e-18.\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.492157  [    0/  569]\n",
      "loss: 0.506233  [  128/  569]\n",
      "loss: 0.506468  [  256/  569]\n",
      "loss: 0.511387  [  384/  569]\n",
      "loss: 0.503165  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4087e-18.\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.509598  [    0/  569]\n",
      "loss: 0.470877  [  128/  569]\n",
      "loss: 0.474334  [  256/  569]\n",
      "loss: 0.503249  [  384/  569]\n",
      "loss: 0.485579  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2678e-18.\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.458542  [    0/  569]\n",
      "loss: 0.508862  [  128/  569]\n",
      "loss: 0.514975  [  256/  569]\n",
      "loss: 0.486015  [  384/  569]\n",
      "loss: 0.505199  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1410e-18.\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.484853  [    0/  569]\n",
      "loss: 0.483027  [  128/  569]\n",
      "loss: 0.561265  [  256/  569]\n",
      "loss: 0.514363  [  384/  569]\n",
      "loss: 0.470031  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0269e-18.\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.519288  [    0/  569]\n",
      "loss: 0.492880  [  128/  569]\n",
      "loss: 0.474773  [  256/  569]\n",
      "loss: 0.508323  [  384/  569]\n",
      "loss: 0.522408  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.2423e-19.\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.515204  [    0/  569]\n",
      "loss: 0.467772  [  128/  569]\n",
      "loss: 0.512652  [  256/  569]\n",
      "loss: 0.521719  [  384/  569]\n",
      "loss: 0.528055  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.3181e-19.\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.519983  [    0/  569]\n",
      "loss: 0.534381  [  128/  569]\n",
      "loss: 0.481204  [  256/  569]\n",
      "loss: 0.475173  [  384/  569]\n",
      "loss: 0.457685  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.4863e-19.\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.488835  [    0/  569]\n",
      "loss: 0.512877  [  128/  569]\n",
      "loss: 0.470183  [  256/  569]\n",
      "loss: 0.503292  [  384/  569]\n",
      "loss: 0.460610  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.7376e-19.\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.506125  [    0/  569]\n",
      "loss: 0.509027  [  128/  569]\n",
      "loss: 0.457406  [  256/  569]\n",
      "loss: 0.520885  [  384/  569]\n",
      "loss: 0.541851  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.0639e-19.\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.520634  [    0/  569]\n",
      "loss: 0.483615  [  128/  569]\n",
      "loss: 0.480690  [  256/  569]\n",
      "loss: 0.476900  [  384/  569]\n",
      "loss: 0.522132  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.4575e-19.\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.489048  [    0/  569]\n",
      "loss: 0.454480  [  128/  569]\n",
      "loss: 0.470214  [  256/  569]\n",
      "loss: 0.536088  [  384/  569]\n",
      "loss: 0.492755  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.9117e-19.\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.472070  [    0/  569]\n",
      "loss: 0.501184  [  128/  569]\n",
      "loss: 0.493943  [  256/  569]\n",
      "loss: 0.510795  [  384/  569]\n",
      "loss: 0.573912  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.4206e-19.\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.554626  [    0/  569]\n",
      "loss: 0.498140  [  128/  569]\n",
      "loss: 0.530301  [  256/  569]\n",
      "loss: 0.466120  [  384/  569]\n",
      "loss: 0.467309  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.9785e-19.\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.496704  [    0/  569]\n",
      "loss: 0.481796  [  128/  569]\n",
      "loss: 0.533190  [  256/  569]\n",
      "loss: 0.466524  [  384/  569]\n",
      "loss: 0.547725  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.5807e-19.\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.544311  [    0/  569]\n",
      "loss: 0.483790  [  128/  569]\n",
      "loss: 0.501757  [  256/  569]\n",
      "loss: 0.480902  [  384/  569]\n",
      "loss: 0.528563  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.2226e-19.\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.515322  [    0/  569]\n",
      "loss: 0.489748  [  128/  569]\n",
      "loss: 0.530701  [  256/  569]\n",
      "loss: 0.465942  [  384/  569]\n",
      "loss: 0.550788  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9003e-19.\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.441579  [    0/  569]\n",
      "loss: 0.488495  [  128/  569]\n",
      "loss: 0.459389  [  256/  569]\n",
      "loss: 0.554520  [  384/  569]\n",
      "loss: 0.550004  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6103e-19.\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.526295  [    0/  569]\n",
      "loss: 0.513079  [  128/  569]\n",
      "loss: 0.517423  [  256/  569]\n",
      "loss: 0.547613  [  384/  569]\n",
      "loss: 0.476344  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3493e-19.\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.503330  [    0/  569]\n",
      "loss: 0.476082  [  128/  569]\n",
      "loss: 0.464236  [  256/  569]\n",
      "loss: 0.490325  [  384/  569]\n",
      "loss: 0.500123  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1143e-19.\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.487767  [    0/  569]\n",
      "loss: 0.512422  [  128/  569]\n",
      "loss: 0.529116  [  256/  569]\n",
      "loss: 0.564013  [  384/  569]\n",
      "loss: 0.516117  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9029e-19.\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.475299  [    0/  569]\n",
      "loss: 0.492892  [  128/  569]\n",
      "loss: 0.487218  [  256/  569]\n",
      "loss: 0.521831  [  384/  569]\n",
      "loss: 0.501327  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7126e-19.\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.517773  [    0/  569]\n",
      "loss: 0.485411  [  128/  569]\n",
      "loss: 0.478373  [  256/  569]\n",
      "loss: 0.508070  [  384/  569]\n",
      "loss: 0.483425  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5414e-19.\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.511808  [    0/  569]\n",
      "loss: 0.473098  [  128/  569]\n",
      "loss: 0.488150  [  256/  569]\n",
      "loss: 0.495731  [  384/  569]\n",
      "loss: 0.530109  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3872e-19.\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.515758  [    0/  569]\n",
      "loss: 0.468701  [  128/  569]\n",
      "loss: 0.513638  [  256/  569]\n",
      "loss: 0.512627  [  384/  569]\n",
      "loss: 0.529281  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2485e-19.\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.499748  [    0/  569]\n",
      "loss: 0.557613  [  128/  569]\n",
      "loss: 0.529072  [  256/  569]\n",
      "loss: 0.527791  [  384/  569]\n",
      "loss: 0.458899  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1236e-19.\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.533270  [    0/  569]\n",
      "loss: 0.472220  [  128/  569]\n",
      "loss: 0.479205  [  256/  569]\n",
      "loss: 0.444395  [  384/  569]\n",
      "loss: 0.514552  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0113e-19.\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.516685  [    0/  569]\n",
      "loss: 0.554515  [  128/  569]\n",
      "loss: 0.434338  [  256/  569]\n",
      "loss: 0.472692  [  384/  569]\n",
      "loss: 0.519022  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.1015e-20.\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.534517  [    0/  569]\n",
      "loss: 0.513247  [  128/  569]\n",
      "loss: 0.461484  [  256/  569]\n",
      "loss: 0.536112  [  384/  569]\n",
      "loss: 0.548686  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.1914e-20.\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.478216  [    0/  569]\n",
      "loss: 0.523756  [  128/  569]\n",
      "loss: 0.564060  [  256/  569]\n",
      "loss: 0.446688  [  384/  569]\n",
      "loss: 0.530982  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.3723e-20.\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.469598  [    0/  569]\n",
      "loss: 0.537662  [  128/  569]\n",
      "loss: 0.503702  [  256/  569]\n",
      "loss: 0.522839  [  384/  569]\n",
      "loss: 0.512939  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.6350e-20.\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.532408  [    0/  569]\n",
      "loss: 0.526634  [  128/  569]\n",
      "loss: 0.525370  [  256/  569]\n",
      "loss: 0.523490  [  384/  569]\n",
      "loss: 0.477069  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.9715e-20.\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.542046  [    0/  569]\n",
      "loss: 0.453661  [  128/  569]\n",
      "loss: 0.510324  [  256/  569]\n",
      "loss: 0.506205  [  384/  569]\n",
      "loss: 0.479537  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.3744e-20.\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.483215  [    0/  569]\n",
      "loss: 0.502708  [  128/  569]\n",
      "loss: 0.500110  [  256/  569]\n",
      "loss: 0.531200  [  384/  569]\n",
      "loss: 0.561202  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.8369e-20.\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.487175  [    0/  569]\n",
      "loss: 0.515624  [  128/  569]\n",
      "loss: 0.502531  [  256/  569]\n",
      "loss: 0.471187  [  384/  569]\n",
      "loss: 0.465977  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.3532e-20.\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.476679  [    0/  569]\n",
      "loss: 0.515561  [  128/  569]\n",
      "loss: 0.431474  [  256/  569]\n",
      "loss: 0.550852  [  384/  569]\n",
      "loss: 0.496471  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.9179e-20.\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.500786  [    0/  569]\n",
      "loss: 0.486592  [  128/  569]\n",
      "loss: 0.497312  [  256/  569]\n",
      "loss: 0.491133  [  384/  569]\n",
      "loss: 0.487452  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.5261e-20.\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.522686  [    0/  569]\n",
      "loss: 0.512448  [  128/  569]\n",
      "loss: 0.485403  [  256/  569]\n",
      "loss: 0.505471  [  384/  569]\n",
      "loss: 0.506121  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.1735e-20.\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.493074  [    0/  569]\n",
      "loss: 0.472345  [  128/  569]\n",
      "loss: 0.462059  [  256/  569]\n",
      "loss: 0.515147  [  384/  569]\n",
      "loss: 0.470020  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8562e-20.\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.516570  [    0/  569]\n",
      "loss: 0.517398  [  128/  569]\n",
      "loss: 0.495281  [  256/  569]\n",
      "loss: 0.472388  [  384/  569]\n",
      "loss: 0.457369  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5705e-20.\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.505010  [    0/  569]\n",
      "loss: 0.467109  [  128/  569]\n",
      "loss: 0.528293  [  256/  569]\n",
      "loss: 0.479259  [  384/  569]\n",
      "loss: 0.540585  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3135e-20.\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.526831  [    0/  569]\n",
      "loss: 0.493994  [  128/  569]\n",
      "loss: 0.509912  [  256/  569]\n",
      "loss: 0.464678  [  384/  569]\n",
      "loss: 0.555544  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0821e-20.\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.506474  [    0/  569]\n",
      "loss: 0.487177  [  128/  569]\n",
      "loss: 0.479827  [  256/  569]\n",
      "loss: 0.521209  [  384/  569]\n",
      "loss: 0.478728  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8739e-20.\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.544892  [    0/  569]\n",
      "loss: 0.500319  [  128/  569]\n",
      "loss: 0.500915  [  256/  569]\n",
      "loss: 0.530589  [  384/  569]\n",
      "loss: 0.442153  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6865e-20.\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.514991  [    0/  569]\n",
      "loss: 0.517142  [  128/  569]\n",
      "loss: 0.498389  [  256/  569]\n",
      "loss: 0.499383  [  384/  569]\n",
      "loss: 0.510829  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5179e-20.\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.428695  [    0/  569]\n",
      "loss: 0.494576  [  128/  569]\n",
      "loss: 0.506617  [  256/  569]\n",
      "loss: 0.483922  [  384/  569]\n",
      "loss: 0.487816  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3661e-20.\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.479378  [    0/  569]\n",
      "loss: 0.507136  [  128/  569]\n",
      "loss: 0.503743  [  256/  569]\n",
      "loss: 0.480937  [  384/  569]\n",
      "loss: 0.559787  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2295e-20.\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.520428  [    0/  569]\n",
      "loss: 0.449400  [  128/  569]\n",
      "loss: 0.515372  [  256/  569]\n",
      "loss: 0.495570  [  384/  569]\n",
      "loss: 0.470839  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1065e-20.\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.504055  [    0/  569]\n",
      "loss: 0.537372  [  128/  569]\n",
      "loss: 0.470410  [  256/  569]\n",
      "loss: 0.536968  [  384/  569]\n",
      "loss: 0.501250  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.9588e-21.\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.484668  [    0/  569]\n",
      "loss: 0.544138  [  128/  569]\n",
      "loss: 0.483758  [  256/  569]\n",
      "loss: 0.536324  [  384/  569]\n",
      "loss: 0.460755  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.9629e-21.\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.470977  [    0/  569]\n",
      "loss: 0.504582  [  128/  569]\n",
      "loss: 0.514504  [  256/  569]\n",
      "loss: 0.556116  [  384/  569]\n",
      "loss: 0.478790  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.0666e-21.\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.513455  [    0/  569]\n",
      "loss: 0.466069  [  128/  569]\n",
      "loss: 0.456933  [  256/  569]\n",
      "loss: 0.505108  [  384/  569]\n",
      "loss: 0.535827  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.2600e-21.\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.517873  [    0/  569]\n",
      "loss: 0.484205  [  128/  569]\n",
      "loss: 0.525635  [  256/  569]\n",
      "loss: 0.490261  [  384/  569]\n",
      "loss: 0.500439  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.5340e-21.\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.496415  [    0/  569]\n",
      "loss: 0.454259  [  128/  569]\n",
      "loss: 0.466640  [  256/  569]\n",
      "loss: 0.526195  [  384/  569]\n",
      "loss: 0.494410  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.8806e-21.\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.475119  [    0/  569]\n",
      "loss: 0.489124  [  128/  569]\n",
      "loss: 0.498839  [  256/  569]\n",
      "loss: 0.522749  [  384/  569]\n",
      "loss: 0.491615  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.2925e-21.\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.519899  [    0/  569]\n",
      "loss: 0.480074  [  128/  569]\n",
      "loss: 0.471170  [  256/  569]\n",
      "loss: 0.514466  [  384/  569]\n",
      "loss: 0.495123  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.7633e-21.\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.531248  [    0/  569]\n",
      "loss: 0.494010  [  128/  569]\n",
      "loss: 0.479134  [  256/  569]\n",
      "loss: 0.510710  [  384/  569]\n",
      "loss: 0.508334  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.2869e-21.\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.479694  [    0/  569]\n",
      "loss: 0.501408  [  128/  569]\n",
      "loss: 0.494087  [  256/  569]\n",
      "loss: 0.491843  [  384/  569]\n",
      "loss: 0.537534  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.8583e-21.\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.545712  [    0/  569]\n",
      "loss: 0.520335  [  128/  569]\n",
      "loss: 0.485680  [  256/  569]\n",
      "loss: 0.505344  [  384/  569]\n",
      "loss: 0.504373  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4724e-21.\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.509159  [    0/  569]\n",
      "loss: 0.500875  [  128/  569]\n",
      "loss: 0.542233  [  256/  569]\n",
      "loss: 0.466235  [  384/  569]\n",
      "loss: 0.504150  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.1252e-21.\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.495419  [    0/  569]\n",
      "loss: 0.493726  [  128/  569]\n",
      "loss: 0.511960  [  256/  569]\n",
      "loss: 0.503437  [  384/  569]\n",
      "loss: 0.510983  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8127e-21.\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.478527  [    0/  569]\n",
      "loss: 0.466901  [  128/  569]\n",
      "loss: 0.520776  [  256/  569]\n",
      "loss: 0.522185  [  384/  569]\n",
      "loss: 0.490270  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5314e-21.\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.516305  [    0/  569]\n",
      "loss: 0.500728  [  128/  569]\n",
      "loss: 0.484500  [  256/  569]\n",
      "loss: 0.496277  [  384/  569]\n",
      "loss: 0.488199  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2783e-21.\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.514969  [    0/  569]\n",
      "loss: 0.499905  [  128/  569]\n",
      "loss: 0.484213  [  256/  569]\n",
      "loss: 0.505823  [  384/  569]\n",
      "loss: 0.510219  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0504e-21.\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.509087  [    0/  569]\n",
      "loss: 0.479908  [  128/  569]\n",
      "loss: 0.520857  [  256/  569]\n",
      "loss: 0.456397  [  384/  569]\n",
      "loss: 0.509624  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8454e-21.\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.504769  [    0/  569]\n",
      "loss: 0.479115  [  128/  569]\n",
      "loss: 0.456718  [  256/  569]\n",
      "loss: 0.540549  [  384/  569]\n",
      "loss: 0.504655  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6609e-21.\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.470293  [    0/  569]\n",
      "loss: 0.497960  [  128/  569]\n",
      "loss: 0.490779  [  256/  569]\n",
      "loss: 0.489338  [  384/  569]\n",
      "loss: 0.555729  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4948e-21.\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.495669  [    0/  569]\n",
      "loss: 0.500437  [  128/  569]\n",
      "loss: 0.498741  [  256/  569]\n",
      "loss: 0.503615  [  384/  569]\n",
      "loss: 0.486913  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3453e-21.\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.433786  [    0/  569]\n",
      "loss: 0.521893  [  128/  569]\n",
      "loss: 0.503552  [  256/  569]\n",
      "loss: 0.511329  [  384/  569]\n",
      "loss: 0.482943  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2108e-21.\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.493676  [    0/  569]\n",
      "loss: 0.490466  [  128/  569]\n",
      "loss: 0.452216  [  256/  569]\n",
      "loss: 0.543122  [  384/  569]\n",
      "loss: 0.570381  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0897e-21.\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.502306  [    0/  569]\n",
      "loss: 0.478445  [  128/  569]\n",
      "loss: 0.527770  [  256/  569]\n",
      "loss: 0.502382  [  384/  569]\n",
      "loss: 0.453194  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.8072e-22.\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.512634  [    0/  569]\n",
      "loss: 0.512451  [  128/  569]\n",
      "loss: 0.488253  [  256/  569]\n",
      "loss: 0.520429  [  384/  569]\n",
      "loss: 0.488348  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.8264e-22.\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.535336  [    0/  569]\n",
      "loss: 0.496863  [  128/  569]\n",
      "loss: 0.476496  [  256/  569]\n",
      "loss: 0.461784  [  384/  569]\n",
      "loss: 0.490744  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.9438e-22.\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.484848  [    0/  569]\n",
      "loss: 0.488449  [  128/  569]\n",
      "loss: 0.521303  [  256/  569]\n",
      "loss: 0.484955  [  384/  569]\n",
      "loss: 0.475540  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.1494e-22.\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.490082  [    0/  569]\n",
      "loss: 0.486796  [  128/  569]\n",
      "loss: 0.560223  [  256/  569]\n",
      "loss: 0.515397  [  384/  569]\n",
      "loss: 0.427798  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.4345e-22.\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.517540  [    0/  569]\n",
      "loss: 0.433975  [  128/  569]\n",
      "loss: 0.490203  [  256/  569]\n",
      "loss: 0.477888  [  384/  569]\n",
      "loss: 0.526659  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.7910e-22.\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.527165  [    0/  569]\n",
      "loss: 0.519217  [  128/  569]\n",
      "loss: 0.474578  [  256/  569]\n",
      "loss: 0.501473  [  384/  569]\n",
      "loss: 0.505519  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.2119e-22.\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.478211  [    0/  569]\n",
      "loss: 0.471312  [  128/  569]\n",
      "loss: 0.513829  [  256/  569]\n",
      "loss: 0.535822  [  384/  569]\n",
      "loss: 0.477744  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.6907e-22.\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.474100  [    0/  569]\n",
      "loss: 0.526314  [  128/  569]\n",
      "loss: 0.509867  [  256/  569]\n",
      "loss: 0.501259  [  384/  569]\n",
      "loss: 0.518316  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.2217e-22.\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.499140  [    0/  569]\n",
      "loss: 0.515954  [  128/  569]\n",
      "loss: 0.460743  [  256/  569]\n",
      "loss: 0.518960  [  384/  569]\n",
      "loss: 0.521280  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.7995e-22.\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.500105  [    0/  569]\n",
      "loss: 0.478947  [  128/  569]\n",
      "loss: 0.454872  [  256/  569]\n",
      "loss: 0.503908  [  384/  569]\n",
      "loss: 0.451712  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4195e-22.\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.478462  [    0/  569]\n",
      "loss: 0.494720  [  128/  569]\n",
      "loss: 0.480775  [  256/  569]\n",
      "loss: 0.531720  [  384/  569]\n",
      "loss: 0.508515  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.0776e-22.\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.527482  [    0/  569]\n",
      "loss: 0.492726  [  128/  569]\n",
      "loss: 0.476126  [  256/  569]\n",
      "loss: 0.497297  [  384/  569]\n",
      "loss: 0.472821  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.7698e-22.\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.559984  [    0/  569]\n",
      "loss: 0.464701  [  128/  569]\n",
      "loss: 0.496287  [  256/  569]\n",
      "loss: 0.465524  [  384/  569]\n",
      "loss: 0.518260  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4928e-22.\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.494841  [    0/  569]\n",
      "loss: 0.478603  [  128/  569]\n",
      "loss: 0.502559  [  256/  569]\n",
      "loss: 0.511840  [  384/  569]\n",
      "loss: 0.498971  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2436e-22.\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.563663  [    0/  569]\n",
      "loss: 0.487929  [  128/  569]\n",
      "loss: 0.496023  [  256/  569]\n",
      "loss: 0.506064  [  384/  569]\n",
      "loss: 0.506149  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0192e-22.\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.450485  [    0/  569]\n",
      "loss: 0.550843  [  128/  569]\n",
      "loss: 0.511465  [  256/  569]\n",
      "loss: 0.491444  [  384/  569]\n",
      "loss: 0.539708  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8173e-22.\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.485930  [    0/  569]\n",
      "loss: 0.506888  [  128/  569]\n",
      "loss: 0.464953  [  256/  569]\n",
      "loss: 0.494580  [  384/  569]\n",
      "loss: 0.538199  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6356e-22.\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.526431  [    0/  569]\n",
      "loss: 0.532550  [  128/  569]\n",
      "loss: 0.542865  [  256/  569]\n",
      "loss: 0.453876  [  384/  569]\n",
      "loss: 0.492519  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4720e-22.\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.511312  [    0/  569]\n",
      "loss: 0.478656  [  128/  569]\n",
      "loss: 0.468087  [  256/  569]\n",
      "loss: 0.507409  [  384/  569]\n",
      "loss: 0.517625  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3248e-22.\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.486744  [    0/  569]\n",
      "loss: 0.485056  [  128/  569]\n",
      "loss: 0.488076  [  256/  569]\n",
      "loss: 0.523640  [  384/  569]\n",
      "loss: 0.525944  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1923e-22.\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.494586  [    0/  569]\n",
      "loss: 0.531113  [  128/  569]\n",
      "loss: 0.498560  [  256/  569]\n",
      "loss: 0.489189  [  384/  569]\n",
      "loss: 0.446608  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0731e-22.\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.471698  [    0/  569]\n",
      "loss: 0.516222  [  128/  569]\n",
      "loss: 0.537745  [  256/  569]\n",
      "loss: 0.498378  [  384/  569]\n",
      "loss: 0.525054  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.6578e-23.\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.555687  [    0/  569]\n",
      "loss: 0.429132  [  128/  569]\n",
      "loss: 0.566400  [  256/  569]\n",
      "loss: 0.502876  [  384/  569]\n",
      "loss: 0.522784  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.6920e-23.\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.520839  [    0/  569]\n",
      "loss: 0.510012  [  128/  569]\n",
      "loss: 0.473894  [  256/  569]\n",
      "loss: 0.522648  [  384/  569]\n",
      "loss: 0.470856  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.8228e-23.\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.471052  [    0/  569]\n",
      "loss: 0.547112  [  128/  569]\n",
      "loss: 0.487233  [  256/  569]\n",
      "loss: 0.474917  [  384/  569]\n",
      "loss: 0.476513  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.0405e-23.\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.468055  [    0/  569]\n",
      "loss: 0.534133  [  128/  569]\n",
      "loss: 0.485972  [  256/  569]\n",
      "loss: 0.453240  [  384/  569]\n",
      "loss: 0.489007  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.3365e-23.\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.452760  [    0/  569]\n",
      "loss: 0.498700  [  128/  569]\n",
      "loss: 0.500828  [  256/  569]\n",
      "loss: 0.559716  [  384/  569]\n",
      "loss: 0.485875  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.7028e-23.\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.515906  [    0/  569]\n",
      "loss: 0.486296  [  128/  569]\n",
      "loss: 0.536855  [  256/  569]\n",
      "loss: 0.459916  [  384/  569]\n",
      "loss: 0.536114  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.1326e-23.\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.514410  [    0/  569]\n",
      "loss: 0.482569  [  128/  569]\n",
      "loss: 0.492375  [  256/  569]\n",
      "loss: 0.510618  [  384/  569]\n",
      "loss: 0.474593  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.6193e-23.\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.429509  [    0/  569]\n",
      "loss: 0.525542  [  128/  569]\n",
      "loss: 0.453375  [  256/  569]\n",
      "loss: 0.489897  [  384/  569]\n",
      "loss: 0.531720  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.1574e-23.\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.424140  [    0/  569]\n",
      "loss: 0.575474  [  128/  569]\n",
      "loss: 0.457334  [  256/  569]\n",
      "loss: 0.497553  [  384/  569]\n",
      "loss: 0.595276  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.7416e-23.\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.474531  [    0/  569]\n",
      "loss: 0.484324  [  128/  569]\n",
      "loss: 0.458089  [  256/  569]\n",
      "loss: 0.517999  [  384/  569]\n",
      "loss: 0.547756  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.3675e-23.\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.492333  [    0/  569]\n",
      "loss: 0.478414  [  128/  569]\n",
      "loss: 0.560845  [  256/  569]\n",
      "loss: 0.448101  [  384/  569]\n",
      "loss: 0.498676  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.0307e-23.\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.416152  [    0/  569]\n",
      "loss: 0.487919  [  128/  569]\n",
      "loss: 0.514965  [  256/  569]\n",
      "loss: 0.539983  [  384/  569]\n",
      "loss: 0.512964  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.7276e-23.\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.470213  [    0/  569]\n",
      "loss: 0.477176  [  128/  569]\n",
      "loss: 0.462866  [  256/  569]\n",
      "loss: 0.481103  [  384/  569]\n",
      "loss: 0.506501  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4549e-23.\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.459235  [    0/  569]\n",
      "loss: 0.499773  [  128/  569]\n",
      "loss: 0.475257  [  256/  569]\n",
      "loss: 0.550981  [  384/  569]\n",
      "loss: 0.481124  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2094e-23.\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.506162  [    0/  569]\n",
      "loss: 0.490903  [  128/  569]\n",
      "loss: 0.502139  [  256/  569]\n",
      "loss: 0.557898  [  384/  569]\n",
      "loss: 0.453727  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9885e-23.\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.485948  [    0/  569]\n",
      "loss: 0.512296  [  128/  569]\n",
      "loss: 0.556708  [  256/  569]\n",
      "loss: 0.518853  [  384/  569]\n",
      "loss: 0.477145  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7896e-23.\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.474462  [    0/  569]\n",
      "loss: 0.530076  [  128/  569]\n",
      "loss: 0.475590  [  256/  569]\n",
      "loss: 0.507368  [  384/  569]\n",
      "loss: 0.507528  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6106e-23.\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.519749  [    0/  569]\n",
      "loss: 0.494288  [  128/  569]\n",
      "loss: 0.464257  [  256/  569]\n",
      "loss: 0.558670  [  384/  569]\n",
      "loss: 0.474518  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4496e-23.\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.512213  [    0/  569]\n",
      "loss: 0.508697  [  128/  569]\n",
      "loss: 0.532252  [  256/  569]\n",
      "loss: 0.482726  [  384/  569]\n",
      "loss: 0.472977  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3046e-23.\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.498485  [    0/  569]\n",
      "loss: 0.515893  [  128/  569]\n",
      "loss: 0.506125  [  256/  569]\n",
      "loss: 0.506355  [  384/  569]\n",
      "loss: 0.510250  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1742e-23.\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.441806  [    0/  569]\n",
      "loss: 0.518546  [  128/  569]\n",
      "loss: 0.519526  [  256/  569]\n",
      "loss: 0.533358  [  384/  569]\n",
      "loss: 0.493287  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0567e-23.\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.478525  [    0/  569]\n",
      "loss: 0.492268  [  128/  569]\n",
      "loss: 0.548611  [  256/  569]\n",
      "loss: 0.497264  [  384/  569]\n",
      "loss: 0.482161  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.5107e-24.\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.502584  [    0/  569]\n",
      "loss: 0.544426  [  128/  569]\n",
      "loss: 0.513713  [  256/  569]\n",
      "loss: 0.475308  [  384/  569]\n",
      "loss: 0.487732  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.5597e-24.\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.458787  [    0/  569]\n",
      "loss: 0.561086  [  128/  569]\n",
      "loss: 0.508346  [  256/  569]\n",
      "loss: 0.510941  [  384/  569]\n",
      "loss: 0.493690  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.7037e-24.\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.539348  [    0/  569]\n",
      "loss: 0.491368  [  128/  569]\n",
      "loss: 0.517726  [  256/  569]\n",
      "loss: 0.524087  [  384/  569]\n",
      "loss: 0.498614  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.9333e-24.\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.518375  [    0/  569]\n",
      "loss: 0.483287  [  128/  569]\n",
      "loss: 0.552595  [  256/  569]\n",
      "loss: 0.506815  [  384/  569]\n",
      "loss: 0.446521  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.2400e-24.\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.508624  [    0/  569]\n",
      "loss: 0.496404  [  128/  569]\n",
      "loss: 0.517770  [  256/  569]\n",
      "loss: 0.478354  [  384/  569]\n",
      "loss: 0.519331  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.6160e-24.\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.517351  [    0/  569]\n",
      "loss: 0.477437  [  128/  569]\n",
      "loss: 0.448857  [  256/  569]\n",
      "loss: 0.479209  [  384/  569]\n",
      "loss: 0.528366  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.0544e-24.\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.505708  [    0/  569]\n",
      "loss: 0.493419  [  128/  569]\n",
      "loss: 0.514917  [  256/  569]\n",
      "loss: 0.479355  [  384/  569]\n",
      "loss: 0.557414  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.5489e-24.\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.524135  [    0/  569]\n",
      "loss: 0.480154  [  128/  569]\n",
      "loss: 0.543357  [  256/  569]\n",
      "loss: 0.503218  [  384/  569]\n",
      "loss: 0.490205  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.0941e-24.\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.459384  [    0/  569]\n",
      "loss: 0.509412  [  128/  569]\n",
      "loss: 0.536591  [  256/  569]\n",
      "loss: 0.501625  [  384/  569]\n",
      "loss: 0.564946  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.6846e-24.\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.498999  [    0/  569]\n",
      "loss: 0.474967  [  128/  569]\n",
      "loss: 0.478125  [  256/  569]\n",
      "loss: 0.524552  [  384/  569]\n",
      "loss: 0.513170  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.3162e-24.\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.493419  [    0/  569]\n",
      "loss: 0.497954  [  128/  569]\n",
      "loss: 0.485419  [  256/  569]\n",
      "loss: 0.499516  [  384/  569]\n",
      "loss: 0.435909  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9846e-24.\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.490102  [    0/  569]\n",
      "loss: 0.503697  [  128/  569]\n",
      "loss: 0.439041  [  256/  569]\n",
      "loss: 0.481952  [  384/  569]\n",
      "loss: 0.520555  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6861e-24.\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.475824  [    0/  569]\n",
      "loss: 0.551099  [  128/  569]\n",
      "loss: 0.508012  [  256/  569]\n",
      "loss: 0.470406  [  384/  569]\n",
      "loss: 0.482663  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4175e-24.\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.447082  [    0/  569]\n",
      "loss: 0.532381  [  128/  569]\n",
      "loss: 0.548558  [  256/  569]\n",
      "loss: 0.412475  [  384/  569]\n",
      "loss: 0.470873  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1757e-24.\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.462924  [    0/  569]\n",
      "loss: 0.488736  [  128/  569]\n",
      "loss: 0.515994  [  256/  569]\n",
      "loss: 0.509957  [  384/  569]\n",
      "loss: 0.534538  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9582e-24.\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.515927  [    0/  569]\n",
      "loss: 0.504123  [  128/  569]\n",
      "loss: 0.537523  [  256/  569]\n",
      "loss: 0.486897  [  384/  569]\n",
      "loss: 0.475838  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7624e-24.\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.502078  [    0/  569]\n",
      "loss: 0.515348  [  128/  569]\n",
      "loss: 0.498816  [  256/  569]\n",
      "loss: 0.428783  [  384/  569]\n",
      "loss: 0.515256  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5861e-24.\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.459493  [    0/  569]\n",
      "loss: 0.477214  [  128/  569]\n",
      "loss: 0.483149  [  256/  569]\n",
      "loss: 0.448423  [  384/  569]\n",
      "loss: 0.554807  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4275e-24.\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 0.452941  [    0/  569]\n",
      "loss: 0.499690  [  128/  569]\n",
      "loss: 0.524975  [  256/  569]\n",
      "loss: 0.456530  [  384/  569]\n",
      "loss: 0.516801  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2848e-24.\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.532984  [    0/  569]\n",
      "loss: 0.471022  [  128/  569]\n",
      "loss: 0.486230  [  256/  569]\n",
      "loss: 0.506523  [  384/  569]\n",
      "loss: 0.487244  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1563e-24.\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.481569  [    0/  569]\n",
      "loss: 0.519436  [  128/  569]\n",
      "loss: 0.547090  [  256/  569]\n",
      "loss: 0.462972  [  384/  569]\n",
      "loss: 0.452830  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0407e-24.\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.496039  [    0/  569]\n",
      "loss: 0.536817  [  128/  569]\n",
      "loss: 0.465377  [  256/  569]\n",
      "loss: 0.523165  [  384/  569]\n",
      "loss: 0.484342  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.3659e-25.\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.538296  [    0/  569]\n",
      "loss: 0.513969  [  128/  569]\n",
      "loss: 0.462117  [  256/  569]\n",
      "loss: 0.531693  [  384/  569]\n",
      "loss: 0.493190  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.4293e-25.\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.486016  [    0/  569]\n",
      "loss: 0.530008  [  128/  569]\n",
      "loss: 0.501448  [  256/  569]\n",
      "loss: 0.497597  [  384/  569]\n",
      "loss: 0.464060  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.5864e-25.\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.513650  [    0/  569]\n",
      "loss: 0.464244  [  128/  569]\n",
      "loss: 0.551790  [  256/  569]\n",
      "loss: 0.554867  [  384/  569]\n",
      "loss: 0.512596  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.8277e-25.\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.492514  [    0/  569]\n",
      "loss: 0.462441  [  128/  569]\n",
      "loss: 0.484943  [  256/  569]\n",
      "loss: 0.544914  [  384/  569]\n",
      "loss: 0.494510  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.1450e-25.\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.529701  [    0/  569]\n",
      "loss: 0.528864  [  128/  569]\n",
      "loss: 0.497251  [  256/  569]\n",
      "loss: 0.487249  [  384/  569]\n",
      "loss: 0.450278  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.5305e-25.\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.475533  [    0/  569]\n",
      "loss: 0.551942  [  128/  569]\n",
      "loss: 0.491794  [  256/  569]\n",
      "loss: 0.489281  [  384/  569]\n",
      "loss: 0.511814  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.9774e-25.\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.499694  [    0/  569]\n",
      "loss: 0.446446  [  128/  569]\n",
      "loss: 0.492008  [  256/  569]\n",
      "loss: 0.508140  [  384/  569]\n",
      "loss: 0.558886  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.4797e-25.\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.475498  [    0/  569]\n",
      "loss: 0.474645  [  128/  569]\n",
      "loss: 0.511154  [  256/  569]\n",
      "loss: 0.496461  [  384/  569]\n",
      "loss: 0.487977  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.0317e-25.\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.512933  [    0/  569]\n",
      "loss: 0.500575  [  128/  569]\n",
      "loss: 0.456276  [  256/  569]\n",
      "loss: 0.521422  [  384/  569]\n",
      "loss: 0.476054  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.6285e-25.\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.493846  [    0/  569]\n",
      "loss: 0.462303  [  128/  569]\n",
      "loss: 0.487409  [  256/  569]\n",
      "loss: 0.459786  [  384/  569]\n",
      "loss: 0.449728  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.2657e-25.\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.474413  [    0/  569]\n",
      "loss: 0.497304  [  128/  569]\n",
      "loss: 0.546841  [  256/  569]\n",
      "loss: 0.444159  [  384/  569]\n",
      "loss: 0.535650  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.9391e-25.\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.508476  [    0/  569]\n",
      "loss: 0.528430  [  128/  569]\n",
      "loss: 0.510121  [  256/  569]\n",
      "loss: 0.440570  [  384/  569]\n",
      "loss: 0.524155  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6452e-25.\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.461180  [    0/  569]\n",
      "loss: 0.523969  [  128/  569]\n",
      "loss: 0.497367  [  256/  569]\n",
      "loss: 0.458558  [  384/  569]\n",
      "loss: 0.506034  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3807e-25.\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.461292  [    0/  569]\n",
      "loss: 0.498293  [  128/  569]\n",
      "loss: 0.507558  [  256/  569]\n",
      "loss: 0.513115  [  384/  569]\n",
      "loss: 0.480215  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1426e-25.\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.503392  [    0/  569]\n",
      "loss: 0.514914  [  128/  569]\n",
      "loss: 0.483725  [  256/  569]\n",
      "loss: 0.471685  [  384/  569]\n",
      "loss: 0.534684  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.9284e-25.\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.509349  [    0/  569]\n",
      "loss: 0.451835  [  128/  569]\n",
      "loss: 0.465509  [  256/  569]\n",
      "loss: 0.546310  [  384/  569]\n",
      "loss: 0.515129  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7355e-25.\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.505913  [    0/  569]\n",
      "loss: 0.526178  [  128/  569]\n",
      "loss: 0.456700  [  256/  569]\n",
      "loss: 0.474797  [  384/  569]\n",
      "loss: 0.518562  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5620e-25.\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.449047  [    0/  569]\n",
      "loss: 0.521467  [  128/  569]\n",
      "loss: 0.514042  [  256/  569]\n",
      "loss: 0.482386  [  384/  569]\n",
      "loss: 0.507389  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4058e-25.\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.500273  [    0/  569]\n",
      "loss: 0.484110  [  128/  569]\n",
      "loss: 0.502256  [  256/  569]\n",
      "loss: 0.514692  [  384/  569]\n",
      "loss: 0.479199  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2652e-25.\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.504436  [    0/  569]\n",
      "loss: 0.490498  [  128/  569]\n",
      "loss: 0.477953  [  256/  569]\n",
      "loss: 0.509961  [  384/  569]\n",
      "loss: 0.485299  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1387e-25.\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.474259  [    0/  569]\n",
      "loss: 0.516815  [  128/  569]\n",
      "loss: 0.537096  [  256/  569]\n",
      "loss: 0.492737  [  384/  569]\n",
      "loss: 0.462435  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0248e-25.\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.494705  [    0/  569]\n",
      "loss: 0.498952  [  128/  569]\n",
      "loss: 0.468721  [  256/  569]\n",
      "loss: 0.502789  [  384/  569]\n",
      "loss: 0.470795  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.2232e-26.\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.499054  [    0/  569]\n",
      "loss: 0.468341  [  128/  569]\n",
      "loss: 0.518405  [  256/  569]\n",
      "loss: 0.453860  [  384/  569]\n",
      "loss: 0.548894  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.3009e-26.\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.488915  [    0/  569]\n",
      "loss: 0.569403  [  128/  569]\n",
      "loss: 0.494858  [  256/  569]\n",
      "loss: 0.511052  [  384/  569]\n",
      "loss: 0.495576  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.4708e-26.\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.512868  [    0/  569]\n",
      "loss: 0.548955  [  128/  569]\n",
      "loss: 0.524361  [  256/  569]\n",
      "loss: 0.510816  [  384/  569]\n",
      "loss: 0.519821  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.7237e-26.\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.513978  [    0/  569]\n",
      "loss: 0.539287  [  128/  569]\n",
      "loss: 0.501027  [  256/  569]\n",
      "loss: 0.471740  [  384/  569]\n",
      "loss: 0.500292  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.0514e-26.\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.545886  [    0/  569]\n",
      "loss: 0.508715  [  128/  569]\n",
      "loss: 0.466204  [  256/  569]\n",
      "loss: 0.462842  [  384/  569]\n",
      "loss: 0.549429  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.4462e-26.\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.517710  [    0/  569]\n",
      "loss: 0.490962  [  128/  569]\n",
      "loss: 0.469948  [  256/  569]\n",
      "loss: 0.487472  [  384/  569]\n",
      "loss: 0.541133  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.9016e-26.\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.471184  [    0/  569]\n",
      "loss: 0.513980  [  128/  569]\n",
      "loss: 0.470333  [  256/  569]\n",
      "loss: 0.553785  [  384/  569]\n",
      "loss: 0.506541  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.4115e-26.\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.431965  [    0/  569]\n",
      "loss: 0.489665  [  128/  569]\n",
      "loss: 0.438085  [  256/  569]\n",
      "loss: 0.529665  [  384/  569]\n",
      "loss: 0.543586  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.9703e-26.\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.523468  [    0/  569]\n",
      "loss: 0.527441  [  128/  569]\n",
      "loss: 0.487875  [  256/  569]\n",
      "loss: 0.443150  [  384/  569]\n",
      "loss: 0.564546  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.5733e-26.\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.530846  [    0/  569]\n",
      "loss: 0.476496  [  128/  569]\n",
      "loss: 0.482528  [  256/  569]\n",
      "loss: 0.479060  [  384/  569]\n",
      "loss: 0.505991  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.2159e-26.\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 0.531029  [    0/  569]\n",
      "loss: 0.498207  [  128/  569]\n",
      "loss: 0.512523  [  256/  569]\n",
      "loss: 0.439349  [  384/  569]\n",
      "loss: 0.527252  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8944e-26.\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.489891  [    0/  569]\n",
      "loss: 0.520519  [  128/  569]\n",
      "loss: 0.497556  [  256/  569]\n",
      "loss: 0.484832  [  384/  569]\n",
      "loss: 0.497681  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.6049e-26.\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.482029  [    0/  569]\n",
      "loss: 0.467718  [  128/  569]\n",
      "loss: 0.509635  [  256/  569]\n",
      "loss: 0.464855  [  384/  569]\n",
      "loss: 0.497503  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3444e-26.\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.505371  [    0/  569]\n",
      "loss: 0.540038  [  128/  569]\n",
      "loss: 0.525448  [  256/  569]\n",
      "loss: 0.487916  [  384/  569]\n",
      "loss: 0.443455  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.1100e-26.\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.513792  [    0/  569]\n",
      "loss: 0.485733  [  128/  569]\n",
      "loss: 0.549081  [  256/  569]\n",
      "loss: 0.469430  [  384/  569]\n",
      "loss: 0.504012  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8990e-26.\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.512244  [    0/  569]\n",
      "loss: 0.510535  [  128/  569]\n",
      "loss: 0.488487  [  256/  569]\n",
      "loss: 0.533536  [  384/  569]\n",
      "loss: 0.525923  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.7091e-26.\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.539067  [    0/  569]\n",
      "loss: 0.533023  [  128/  569]\n",
      "loss: 0.527305  [  256/  569]\n",
      "loss: 0.513076  [  384/  569]\n",
      "loss: 0.490988  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5382e-26.\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.556015  [    0/  569]\n",
      "loss: 0.517212  [  128/  569]\n",
      "loss: 0.541417  [  256/  569]\n",
      "loss: 0.443823  [  384/  569]\n",
      "loss: 0.467806  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3844e-26.\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.515421  [    0/  569]\n",
      "loss: 0.502361  [  128/  569]\n",
      "loss: 0.519680  [  256/  569]\n",
      "loss: 0.467711  [  384/  569]\n",
      "loss: 0.555465  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2459e-26.\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.457457  [    0/  569]\n",
      "loss: 0.478069  [  128/  569]\n",
      "loss: 0.488577  [  256/  569]\n",
      "loss: 0.544820  [  384/  569]\n",
      "loss: 0.509434  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1213e-26.\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.495413  [    0/  569]\n",
      "loss: 0.539388  [  128/  569]\n",
      "loss: 0.534278  [  256/  569]\n",
      "loss: 0.478425  [  384/  569]\n",
      "loss: 0.453901  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0092e-26.\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.468951  [    0/  569]\n",
      "loss: 0.511930  [  128/  569]\n",
      "loss: 0.491332  [  256/  569]\n",
      "loss: 0.569309  [  384/  569]\n",
      "loss: 0.533481  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.0828e-27.\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.472684  [    0/  569]\n",
      "loss: 0.512952  [  128/  569]\n",
      "loss: 0.508904  [  256/  569]\n",
      "loss: 0.469594  [  384/  569]\n",
      "loss: 0.482277  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.1745e-27.\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "loss: 0.499910  [    0/  569]\n",
      "loss: 0.475561  [  128/  569]\n",
      "loss: 0.474832  [  256/  569]\n",
      "loss: 0.467712  [  384/  569]\n",
      "loss: 0.535356  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.3571e-27.\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "loss: 0.470870  [    0/  569]\n",
      "loss: 0.486112  [  128/  569]\n",
      "loss: 0.499927  [  256/  569]\n",
      "loss: 0.493123  [  384/  569]\n",
      "loss: 0.494332  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.6214e-27.\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "loss: 0.496644  [    0/  569]\n",
      "loss: 0.508695  [  128/  569]\n",
      "loss: 0.445800  [  256/  569]\n",
      "loss: 0.519554  [  384/  569]\n",
      "loss: 0.489612  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.9592e-27.\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "loss: 0.485437  [    0/  569]\n",
      "loss: 0.460998  [  128/  569]\n",
      "loss: 0.522070  [  256/  569]\n",
      "loss: 0.514210  [  384/  569]\n",
      "loss: 0.506670  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.3633e-27.\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "loss: 0.525845  [    0/  569]\n",
      "loss: 0.475712  [  128/  569]\n",
      "loss: 0.463069  [  256/  569]\n",
      "loss: 0.516820  [  384/  569]\n",
      "loss: 0.454922  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.8270e-27.\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "loss: 0.534874  [    0/  569]\n",
      "loss: 0.494721  [  128/  569]\n",
      "loss: 0.483692  [  256/  569]\n",
      "loss: 0.501669  [  384/  569]\n",
      "loss: 0.499969  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.3443e-27.\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "loss: 0.460003  [    0/  569]\n",
      "loss: 0.514557  [  128/  569]\n",
      "loss: 0.517698  [  256/  569]\n",
      "loss: 0.450619  [  384/  569]\n",
      "loss: 0.573615  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.9098e-27.\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "loss: 0.531261  [    0/  569]\n",
      "loss: 0.469898  [  128/  569]\n",
      "loss: 0.486435  [  256/  569]\n",
      "loss: 0.482264  [  384/  569]\n",
      "loss: 0.494373  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.5189e-27.\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "loss: 0.496643  [    0/  569]\n",
      "loss: 0.468135  [  128/  569]\n",
      "loss: 0.500534  [  256/  569]\n",
      "loss: 0.502931  [  384/  569]\n",
      "loss: 0.525369  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.1670e-27.\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "loss: 0.494194  [    0/  569]\n",
      "loss: 0.493256  [  128/  569]\n",
      "loss: 0.532735  [  256/  569]\n",
      "loss: 0.517815  [  384/  569]\n",
      "loss: 0.468866  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8503e-27.\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "loss: 0.484860  [    0/  569]\n",
      "loss: 0.413296  [  128/  569]\n",
      "loss: 0.481873  [  256/  569]\n",
      "loss: 0.533143  [  384/  569]\n",
      "loss: 0.503186  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5652e-27.\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 0.497570  [    0/  569]\n",
      "loss: 0.525892  [  128/  569]\n",
      "loss: 0.531519  [  256/  569]\n",
      "loss: 0.510522  [  384/  569]\n",
      "loss: 0.458730  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.3087e-27.\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "loss: 0.481623  [    0/  569]\n",
      "loss: 0.556368  [  128/  569]\n",
      "loss: 0.472969  [  256/  569]\n",
      "loss: 0.496088  [  384/  569]\n",
      "loss: 0.470459  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0779e-27.\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "loss: 0.500380  [    0/  569]\n",
      "loss: 0.485337  [  128/  569]\n",
      "loss: 0.536362  [  256/  569]\n",
      "loss: 0.504315  [  384/  569]\n",
      "loss: 0.547721  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8701e-27.\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "loss: 0.541002  [    0/  569]\n",
      "loss: 0.496609  [  128/  569]\n",
      "loss: 0.534611  [  256/  569]\n",
      "loss: 0.475465  [  384/  569]\n",
      "loss: 0.481765  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6831e-27.\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "loss: 0.484586  [    0/  569]\n",
      "loss: 0.487648  [  128/  569]\n",
      "loss: 0.487904  [  256/  569]\n",
      "loss: 0.536458  [  384/  569]\n",
      "loss: 0.476514  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.5148e-27.\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "loss: 0.490654  [    0/  569]\n",
      "loss: 0.491286  [  128/  569]\n",
      "loss: 0.531474  [  256/  569]\n",
      "loss: 0.550125  [  384/  569]\n",
      "loss: 0.468331  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3633e-27.\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "loss: 0.460303  [    0/  569]\n",
      "loss: 0.494848  [  128/  569]\n",
      "loss: 0.537716  [  256/  569]\n",
      "loss: 0.545115  [  384/  569]\n",
      "loss: 0.475648  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2269e-27.\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "loss: 0.479007  [    0/  569]\n",
      "loss: 0.495319  [  128/  569]\n",
      "loss: 0.486849  [  256/  569]\n",
      "loss: 0.513587  [  384/  569]\n",
      "loss: 0.456855  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.1043e-27.\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "loss: 0.502061  [    0/  569]\n",
      "loss: 0.529288  [  128/  569]\n",
      "loss: 0.482002  [  256/  569]\n",
      "loss: 0.458272  [  384/  569]\n",
      "loss: 0.528881  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.9383e-28.\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "loss: 0.478235  [    0/  569]\n",
      "loss: 0.507608  [  128/  569]\n",
      "loss: 0.482761  [  256/  569]\n",
      "loss: 0.502339  [  384/  569]\n",
      "loss: 0.493002  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.9445e-28.\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "loss: 0.518729  [    0/  569]\n",
      "loss: 0.495114  [  128/  569]\n",
      "loss: 0.504756  [  256/  569]\n",
      "loss: 0.544368  [  384/  569]\n",
      "loss: 0.473146  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.0500e-28.\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "loss: 0.536272  [    0/  569]\n",
      "loss: 0.512021  [  128/  569]\n",
      "loss: 0.503094  [  256/  569]\n",
      "loss: 0.508499  [  384/  569]\n",
      "loss: 0.537951  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.2450e-28.\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "loss: 0.477947  [    0/  569]\n",
      "loss: 0.482679  [  128/  569]\n",
      "loss: 0.486319  [  256/  569]\n",
      "loss: 0.465889  [  384/  569]\n",
      "loss: 0.549194  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.5205e-28.\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "loss: 0.515258  [    0/  569]\n",
      "loss: 0.555405  [  128/  569]\n",
      "loss: 0.498987  [  256/  569]\n",
      "loss: 0.496845  [  384/  569]\n",
      "loss: 0.508332  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.8685e-28.\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "loss: 0.517920  [    0/  569]\n",
      "loss: 0.499976  [  128/  569]\n",
      "loss: 0.529057  [  256/  569]\n",
      "loss: 0.535589  [  384/  569]\n",
      "loss: 0.428068  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.2816e-28.\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "loss: 0.432474  [    0/  569]\n",
      "loss: 0.494811  [  128/  569]\n",
      "loss: 0.548969  [  256/  569]\n",
      "loss: 0.509227  [  384/  569]\n",
      "loss: 0.512831  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.7535e-28.\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "loss: 0.507681  [    0/  569]\n",
      "loss: 0.537725  [  128/  569]\n",
      "loss: 0.476898  [  256/  569]\n",
      "loss: 0.517119  [  384/  569]\n",
      "loss: 0.491458  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.2781e-28.\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "loss: 0.489842  [    0/  569]\n",
      "loss: 0.493700  [  128/  569]\n",
      "loss: 0.556701  [  256/  569]\n",
      "loss: 0.515982  [  384/  569]\n",
      "loss: 0.480071  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.8503e-28.\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "loss: 0.490927  [    0/  569]\n",
      "loss: 0.518311  [  128/  569]\n",
      "loss: 0.562155  [  256/  569]\n",
      "loss: 0.465597  [  384/  569]\n",
      "loss: 0.506362  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4653e-28.\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "loss: 0.471406  [    0/  569]\n",
      "loss: 0.503147  [  128/  569]\n",
      "loss: 0.477851  [  256/  569]\n",
      "loss: 0.502523  [  384/  569]\n",
      "loss: 0.470327  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.1187e-28.\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "loss: 0.543942  [    0/  569]\n",
      "loss: 0.533111  [  128/  569]\n",
      "loss: 0.524405  [  256/  569]\n",
      "loss: 0.451248  [  384/  569]\n",
      "loss: 0.473449  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.8069e-28.\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "loss: 0.609487  [    0/  569]\n",
      "loss: 0.498878  [  128/  569]\n",
      "loss: 0.482657  [  256/  569]\n",
      "loss: 0.495136  [  384/  569]\n",
      "loss: 0.532928  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.5262e-28.\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "loss: 0.503880  [    0/  569]\n",
      "loss: 0.476599  [  128/  569]\n",
      "loss: 0.497184  [  256/  569]\n",
      "loss: 0.480517  [  384/  569]\n",
      "loss: 0.517791  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2736e-28.\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "loss: 0.511200  [    0/  569]\n",
      "loss: 0.475535  [  128/  569]\n",
      "loss: 0.514311  [  256/  569]\n",
      "loss: 0.521704  [  384/  569]\n",
      "loss: 0.537579  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0462e-28.\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "loss: 0.484939  [    0/  569]\n",
      "loss: 0.504001  [  128/  569]\n",
      "loss: 0.522397  [  256/  569]\n",
      "loss: 0.491126  [  384/  569]\n",
      "loss: 0.501186  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8416e-28.\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "loss: 0.515995  [    0/  569]\n",
      "loss: 0.472086  [  128/  569]\n",
      "loss: 0.543549  [  256/  569]\n",
      "loss: 0.522365  [  384/  569]\n",
      "loss: 0.424738  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6574e-28.\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "loss: 0.502646  [    0/  569]\n",
      "loss: 0.476505  [  128/  569]\n",
      "loss: 0.484508  [  256/  569]\n",
      "loss: 0.508479  [  384/  569]\n",
      "loss: 0.536700  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4917e-28.\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "loss: 0.471176  [    0/  569]\n",
      "loss: 0.506171  [  128/  569]\n",
      "loss: 0.516948  [  256/  569]\n",
      "loss: 0.553578  [  384/  569]\n",
      "loss: 0.490544  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3425e-28.\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "loss: 0.553705  [    0/  569]\n",
      "loss: 0.505664  [  128/  569]\n",
      "loss: 0.510903  [  256/  569]\n",
      "loss: 0.508369  [  384/  569]\n",
      "loss: 0.477496  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.2083e-28.\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "loss: 0.527702  [    0/  569]\n",
      "loss: 0.514493  [  128/  569]\n",
      "loss: 0.520192  [  256/  569]\n",
      "loss: 0.496273  [  384/  569]\n",
      "loss: 0.473911  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.0874e-28.\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "loss: 0.466623  [    0/  569]\n",
      "loss: 0.500428  [  128/  569]\n",
      "loss: 0.522762  [  256/  569]\n",
      "loss: 0.453883  [  384/  569]\n",
      "loss: 0.547288  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 9.7869e-29.\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "loss: 0.502707  [    0/  569]\n",
      "loss: 0.483893  [  128/  569]\n",
      "loss: 0.476134  [  256/  569]\n",
      "loss: 0.555464  [  384/  569]\n",
      "loss: 0.498475  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 8.8082e-29.\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "loss: 0.442513  [    0/  569]\n",
      "loss: 0.528687  [  128/  569]\n",
      "loss: 0.488908  [  256/  569]\n",
      "loss: 0.526449  [  384/  569]\n",
      "loss: 0.493392  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.9274e-29.\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "loss: 0.523135  [    0/  569]\n",
      "loss: 0.488507  [  128/  569]\n",
      "loss: 0.490656  [  256/  569]\n",
      "loss: 0.493986  [  384/  569]\n",
      "loss: 0.556717  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 7.1347e-29.\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "loss: 0.489832  [    0/  569]\n",
      "loss: 0.501418  [  128/  569]\n",
      "loss: 0.510942  [  256/  569]\n",
      "loss: 0.467150  [  384/  569]\n",
      "loss: 0.552077  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 6.4212e-29.\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "loss: 0.474846  [    0/  569]\n",
      "loss: 0.471410  [  128/  569]\n",
      "loss: 0.481412  [  256/  569]\n",
      "loss: 0.514565  [  384/  569]\n",
      "loss: 0.526428  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.7791e-29.\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "loss: 0.512897  [    0/  569]\n",
      "loss: 0.462304  [  128/  569]\n",
      "loss: 0.478082  [  256/  569]\n",
      "loss: 0.507407  [  384/  569]\n",
      "loss: 0.503472  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 5.2012e-29.\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "loss: 0.496103  [    0/  569]\n",
      "loss: 0.513467  [  128/  569]\n",
      "loss: 0.486159  [  256/  569]\n",
      "loss: 0.474098  [  384/  569]\n",
      "loss: 0.491854  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.6811e-29.\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "loss: 0.450028  [    0/  569]\n",
      "loss: 0.474072  [  128/  569]\n",
      "loss: 0.490847  [  256/  569]\n",
      "loss: 0.516104  [  384/  569]\n",
      "loss: 0.476546  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 4.2130e-29.\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "loss: 0.516725  [    0/  569]\n",
      "loss: 0.503519  [  128/  569]\n",
      "loss: 0.509693  [  256/  569]\n",
      "loss: 0.461359  [  384/  569]\n",
      "loss: 0.521956  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.7917e-29.\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "loss: 0.483638  [    0/  569]\n",
      "loss: 0.489695  [  128/  569]\n",
      "loss: 0.548877  [  256/  569]\n",
      "loss: 0.472113  [  384/  569]\n",
      "loss: 0.534413  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.4125e-29.\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "loss: 0.500698  [    0/  569]\n",
      "loss: 0.517322  [  128/  569]\n",
      "loss: 0.470798  [  256/  569]\n",
      "loss: 0.450396  [  384/  569]\n",
      "loss: 0.504177  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 3.0712e-29.\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "loss: 0.503080  [    0/  569]\n",
      "loss: 0.513307  [  128/  569]\n",
      "loss: 0.470145  [  256/  569]\n",
      "loss: 0.514417  [  384/  569]\n",
      "loss: 0.524198  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.7641e-29.\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "loss: 0.448289  [    0/  569]\n",
      "loss: 0.491210  [  128/  569]\n",
      "loss: 0.492861  [  256/  569]\n",
      "loss: 0.510181  [  384/  569]\n",
      "loss: 0.499207  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.4877e-29.\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "loss: 0.508103  [    0/  569]\n",
      "loss: 0.458330  [  128/  569]\n",
      "loss: 0.511292  [  256/  569]\n",
      "loss: 0.488555  [  384/  569]\n",
      "loss: 0.490480  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.2389e-29.\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "loss: 0.452422  [    0/  569]\n",
      "loss: 0.513290  [  128/  569]\n",
      "loss: 0.463311  [  256/  569]\n",
      "loss: 0.481249  [  384/  569]\n",
      "loss: 0.569888  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 2.0150e-29.\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "loss: 0.451879  [    0/  569]\n",
      "loss: 0.540437  [  128/  569]\n",
      "loss: 0.532826  [  256/  569]\n",
      "loss: 0.517790  [  384/  569]\n",
      "loss: 0.436462  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.8135e-29.\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "loss: 0.485764  [    0/  569]\n",
      "loss: 0.510492  [  128/  569]\n",
      "loss: 0.474761  [  256/  569]\n",
      "loss: 0.509443  [  384/  569]\n",
      "loss: 0.545364  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.6322e-29.\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "loss: 0.472336  [    0/  569]\n",
      "loss: 0.528974  [  128/  569]\n",
      "loss: 0.487097  [  256/  569]\n",
      "loss: 0.449284  [  384/  569]\n",
      "loss: 0.474207  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.4690e-29.\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "loss: 0.550409  [    0/  569]\n",
      "loss: 0.470691  [  128/  569]\n",
      "loss: 0.495245  [  256/  569]\n",
      "loss: 0.553022  [  384/  569]\n",
      "loss: 0.521943  [  456/  569]\n",
      "Adjusting learning rate of group 0 to 1.3221e-29.\n",
      "Done! lr = 1.322070819480824e-29\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dl, model, loss_fn, optimizer, scheduler1=schedulerOnPlateau, scheduler2=schedulerExponential)\n",
    "print(f\"Done! lr = {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso tampoco ha mejorado, al iguale que cuando solo hemos reducido exponencialmente el learning rate, puede que se haya reducido tan rápido que la velocidad de entrenamiento sea muy lenta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay que tener en cuenta que este es un ejemplo muy pequeño y nuestra red es muy pequeña también. Pero en casos en los que el problema sea más grande y usemos redes más grandes probablemente si se vean mejoras usando este método.\n",
    "\n",
    "De momento solo lo he mostrado como ejemplo, no quiero liar ahora entrenando modelos más grandes con muchos más datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2da05e9852e3725e6cd29dd2f3d6ebaa07dda6697715ddc2b5ea77aa5959f695"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
