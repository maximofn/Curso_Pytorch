{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch size finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro de los hiperparámetros importantes es el tamaño de cada lote o batch size, cuanto más grande sea mejor por dos motivos\n",
    " * El primero es que va a entrenar más rápido, porque en cada iteracción va a entrenar con una cantidad de datos mayor. Por lo que vamos a necesitar menos iteracciones\n",
    " * El segundo lo vamos a ver más adelante más ane profundidad, pero cuanto mayor sea el batch size, más estable va a ser el proceso de entrenamiento, por lo que la búsqueda del mínimo, al ser más estable, va a ser más rápida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo implementamos con el dataset de cancer al igual que en el tema enterior en el que buscamos el mejor learning rate posible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  type  \n",
       "0          0.4601                  0.11890     0  \n",
       "1          0.2750                  0.08902     0  \n",
       "2          0.3613                  0.08758     0  \n",
       "3          0.6638                  0.17300     0  \n",
       "4          0.2364                  0.07678     0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cancer_df = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n",
    "cancer_df['type'] = cancer['target']\n",
    "cancer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CancerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        cols = [col for col in dataframe.columns if col != 'target']\n",
    "        self.parameters = torch.from_numpy(dataframe[cols].values).type(torch.float32)\n",
    "        self.targets = torch.from_numpy(dataframe['type'].values).type(torch.float32)\n",
    "        self.targets = self.targets.reshape((len(self.targets), 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parameters)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        parameters = self.parameters[idx]\n",
    "        target = self.targets[idx]\n",
    "        return parameters, target\n",
    "\n",
    "ds = CancerDataset(cancer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso no se va a dividir el dataset en uno de entrenamiento y otro de validación, porque el dataset tiene tan pocos datos, que para poder hacer el ejemplo es necesario usar todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = ds\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset de cancer es tan pequeño y la red que hemos usado hasta ahora también es tan pequeña, que no nos valen para hacer un ejemplo de una batch size finder. Por lo que para este tema defino una red absurdamente grande para que ocupe mucha memoria de GPU y así poder llenarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CancerNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layers=[20000, 5000, 2000]):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_inputs, hidden_layers[0]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[0], hidden_layers[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[1], hidden_layers[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[1], hidden_layers[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[1], hidden_layers[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[1], hidden_layers[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[1], hidden_layers[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[1], hidden_layers[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[1], hidden_layers[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[1], hidden_layers[2]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[2], num_outputs),\n",
    "        )\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.network(x)\n",
    "        probs = self.activation(logits)\n",
    "        return logits, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos si hay GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función para poder ver la memoria total, libre y ocupada de la GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: total: [4096] MiB, free: [3733] MiB, used: [170] MiB\n"
     ]
    }
   ],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=memory.total --format=csv\"\n",
    "    memory_total_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_total_values = [int(x.split()[0]) for i, x in enumerate(memory_total_info)]\n",
    "\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "\n",
    "    command = \"nvidia-smi --query-gpu=memory.used --format=csv\"\n",
    "    memory_used_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_used_values = [int(x.split()[0]) for i, x in enumerate(memory_used_info)]\n",
    "    return memory_total_values, memory_free_values, memory_used_values\n",
    "\n",
    "total, free, used = get_gpu_memory()\n",
    "print(f\"GPU memory: total: {total} MiB, free: {free} MiB, used: {used} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que ahora que aun no hemos mandado el modelo a la GPU ni los datos casi no tenemos GPU ocupada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos la red y la mandamos a la GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model to cuda\n"
     ]
    }
   ],
   "source": [
    "model = CancerNeuralNetwork(31, 1)\n",
    "model.to(device)\n",
    "print(f\"model to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: total: [4096] MiB, free: [1802] MiB, used: [2101] MiB\n"
     ]
    }
   ],
   "source": [
    "total, free, used = get_gpu_memory()\n",
    "print(f\"GPU memory: total: {total} MiB, free: {free} MiB, used: {used} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la memoria de la GPU ha subido bastante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos las funciones de coste y el optimizador, usamos el learning rate que habíamos obtenido en el tema anterior. Aunque ahora puede que no sea el mejor porque no hemos separado el learning rate en entrenamiento y validación, y la red que usamos es más grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-2\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea la función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_prints = 4\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # X and y to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        logits, probs = model(X)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % (int(len(dataloader)/num_prints)+1) == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los posibles valores de learning rate. Esto se hace así porque en este caso el dataset es tan pequeño (569 datos) que no podemos usar un batch size de 1024 por ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[512, 256, 128, 64, 32, 16, 8, 4, 2, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_of_posible_batch_sizes(dataset):\n",
    "    batch_sizes = []\n",
    "    batch_size = 1\n",
    "    while batch_size < len(dataset):\n",
    "        batch_sizes.append(batch_size)\n",
    "        batch_size *= 2\n",
    "    batch_sizes.sort(reverse=True)\n",
    "    return batch_sizes\n",
    "\n",
    "BSs = list_of_posible_batch_sizes(train_ds)\n",
    "BSs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por fin creamos la función que busca el mejor batch size. Buscamos que sea lo mayor posible sin que se desborde la GPU y que sea un múltiplo de 2.\n",
    "\n",
    "Se comienza con el mayor valor posible de batch size y si se desborda la memoria de la GPU se prueba con el siguiente, hasta que no se desborde y por lo tanto ese es el valor más ótimo de batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 512\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.691913  [    0/  569]\n",
      "loss: 0.692149  [   57/  569]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Error: CUDA out of memory. Tried to allocate 382.00 MiB (GPU 0; 3.81 GiB total capacity; 2.22 GiB already allocated; 112.38 MiB free; 2.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "batch size: 256\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Error: CUDA out of memory. Tried to allocate 382.00 MiB (GPU 0; 3.81 GiB total capacity; 2.18 GiB already allocated; 112.38 MiB free; 2.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "batch size: 128\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Error: CUDA out of memory. Tried to allocate 382.00 MiB (GPU 0; 3.81 GiB total capacity; 2.16 GiB already allocated; 112.38 MiB free; 2.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "batch size: 64\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.687320  [    0/  569]\n",
      "loss: 0.680578  [  192/  569]\n",
      "loss: 0.685234  [  384/  569]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.675825  [    0/  569]\n",
      "loss: 0.664338  [  192/  569]\n",
      "loss: 0.659287  [  384/  569]\n",
      "Done!, bacth size is 64\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "for BS_train in BSs:\n",
    "    print(f\"batch size: {BS_train}\")\n",
    "    train_dl = DataLoader(train_ds, batch_size=BS_train, shuffle=True)\n",
    "    epochs = 2\n",
    "    out_of_memory = False\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        try:\n",
    "            train_loop(train_dl, model, loss_fn, optimizer)\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "            out_of_memory = True\n",
    "            break\n",
    "    if out_of_memory == False:\n",
    "        break\n",
    "    print()\n",
    "print(f\"Done!, bacth size is {BS_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BS_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos el mayor valor de batch size posible para nuestro problema, por lo que pasamos a entrenar la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.661637  [    0/  569]\n",
      "loss: 0.650951  [  192/  569]\n",
      "loss: 0.652181  [  384/  569]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.617266  [    0/  569]\n",
      "loss: 0.641129  [  192/  569]\n",
      "loss: 0.645686  [  384/  569]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.632995  [    0/  569]\n",
      "loss: 0.632342  [  192/  569]\n",
      "loss: 0.601855  [  384/  569]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.651026  [    0/  569]\n",
      "loss: 0.629167  [  192/  569]\n",
      "loss: 0.570165  [  384/  569]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.637134  [    0/  569]\n",
      "loss: 0.672079  [  192/  569]\n",
      "loss: 0.600126  [  384/  569]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.599684  [    0/  569]\n",
      "loss: 0.603658  [  192/  569]\n",
      "loss: 0.650290  [  384/  569]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.571652  [    0/  569]\n",
      "loss: 0.573309  [  192/  569]\n",
      "loss: 0.601443  [  384/  569]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.672219  [    0/  569]\n",
      "loss: 0.527147  [  192/  569]\n",
      "loss: 0.546311  [  384/  569]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.628805  [    0/  569]\n",
      "loss: 0.538602  [  192/  569]\n",
      "loss: 0.634535  [  384/  569]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.523270  [    0/  569]\n",
      "loss: 0.608928  [  192/  569]\n",
      "loss: 0.568259  [  384/  569]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.568634  [    0/  569]\n",
      "loss: 0.592499  [  192/  569]\n",
      "loss: 0.481340  [  384/  569]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.645425  [    0/  569]\n",
      "loss: 0.597001  [  192/  569]\n",
      "loss: 0.599947  [  384/  569]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.585564  [    0/  569]\n",
      "loss: 0.535049  [  192/  569]\n",
      "loss: 0.517885  [  384/  569]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.659532  [    0/  569]\n",
      "loss: 0.592803  [  192/  569]\n",
      "loss: 0.588073  [  384/  569]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(train_ds, batch_size=BS_train, shuffle=True)\n",
    "epochs = 14\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    try:\n",
    "        train_loop(train_dl, model, loss_fn, optimizer)\n",
    "    except Exception as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            position = str(e).index('CUDA out of memory')\n",
    "            print(f\"\\t{str(e)[position:]}\")\n",
    "            out_of_memory = True\n",
    "            break\n",
    "        else:\n",
    "            out_of_memory = False\n",
    "        break\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vemos ahora el estado de la memoria de la GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: total: [4096] MiB, free: [110] MiB, used: [3793] MiB\n"
     ]
    }
   ],
   "source": [
    "total, free, used = get_gpu_memory()\n",
    "print(f\"GPU memory: total: {total} MiB, free: {free} MiB, used: {used} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que la memoria de la GPU, está casi llena"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cursoPytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5bc1ee3c77952ec27629ca4df39b2d196e4e16f06fca6055137af5fbf570f5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
