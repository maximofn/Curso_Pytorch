{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora cada vez que hemos entrenado una red hemos usado como optimizador `torch.optim.SGD`, es decir, el descenso del gradiente estocástico (stochastic gradient descent SGD). Pero cuando vimos la teoría de cómo funciona una red neuronal solo hablamos del descenso del gradiente. Vamos a ver en qué se diferencian ambos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto, dividimos el dataset de datos en batches para meterle a la red el mayor número de datos a la vez. Y hemos visto que cuanto mayor es el tamaño de cada batch mejor, la red obtiene mejores resultados. En un caso ideal, lo mejor es meterle a la red todos los datos del dataset, pero esto normalmente no es posible, por eso se divide en batches, porque normalmente hay muchos más datos que capacidad tiene la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver la diferencia de entrenar una red metiendo todos los datos en la memoria de la GPU, metiendo batches y metiendo cada vez solo un dato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a verlo con el ejemplo del dataset Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  type  \n",
       "0          0.4601                  0.11890     0  \n",
       "1          0.2750                  0.08902     0  \n",
       "2          0.3613                  0.08758     0  \n",
       "3          0.6638                  0.17300     0  \n",
       "4          0.2364                  0.07678     0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "cancer = datasets.load_breast_cancer()\n",
    "cancer_df = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n",
    "cancer_df['type'] = cancer['target']\n",
    "cancer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CancerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        cols = [col for col in dataframe.columns if col != 'target']\n",
    "        self.parameters = torch.from_numpy(dataframe[cols].values).type(torch.float32)\n",
    "        self.targets = torch.from_numpy(dataframe['type'].values).type(torch.float32)\n",
    "        self.targets = self.targets.reshape((len(self.targets), 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parameters)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        parameters = self.parameters[idx]\n",
    "        target = self.targets[idx]\n",
    "        return parameters, target\n",
    "\n",
    "ds = CancerDataset(cancer_df)\n",
    "train_ds, valid_ds = torch.utils.data.random_split(ds, [int(0.8*len(ds)), len(ds) - int(0.8*len(ds))], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CancerNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layers=[100, 50, 20]):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_inputs, hidden_layers[0]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[0], hidden_layers[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[1], hidden_layers[2]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[2], num_outputs),\n",
    "        )\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.network(x)\n",
    "        probs = self.activation(logits)\n",
    "        return logits, probs\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BS_train = len(train_ds)\n",
    "train_dl = DataLoader(train_ds, batch_size=BS_train, shuffle=True)\n",
    "\n",
    "model = CancerNeuralNetwork(num_inputs=31, num_outputs=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, epochs, num_prints=4):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    iteration = []\n",
    "    iter = 0\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # X and y to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Compute prediction and loss\n",
    "            logits, probs = model(X)\n",
    "            loss = loss_fn(probs, y)\n",
    "\n",
    "            # Add to lists\n",
    "            losses.append(loss.item())\n",
    "            iteration.append(iter)\n",
    "            iter += 1\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % (int(len(dataloader)/num_prints)+1) == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return losses, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.965165  [    0/  455]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.248273  [    0/  455]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.324464  [    0/  455]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.790592  [    0/  455]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.740439  [    0/  455]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.714093  [    0/  455]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.385249  [    0/  455]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.659133  [    0/  455]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.063710  [    0/  455]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.701113  [    0/  455]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.131021  [    0/  455]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.565366  [    0/  455]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.763951  [    0/  455]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.632293  [    0/  455]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.887139  [    0/  455]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.562328  [    0/  455]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.710925  [    0/  455]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.565726  [    0/  455]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.703913  [    0/  455]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.547287  [    0/  455]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.650676  [    0/  455]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.528221  [    0/  455]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.606172  [    0/  455]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.511143  [    0/  455]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.569538  [    0/  455]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.496019  [    0/  455]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.540182  [    0/  455]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.481586  [    0/  455]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.517584  [    0/  455]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.473310  [    0/  455]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.500297  [    0/  455]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.460008  [    0/  455]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.482111  [    0/  455]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.452919  [    0/  455]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.469822  [    0/  455]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.443857  [    0/  455]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.456405  [    0/  455]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.434500  [    0/  455]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.445295  [    0/  455]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.428816  [    0/  455]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.437791  [    0/  455]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.423122  [    0/  455]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.430623  [    0/  455]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.417695  [    0/  455]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.424129  [    0/  455]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.412661  [    0/  455]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.418435  [    0/  455]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.408138  [    0/  455]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.413496  [    0/  455]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.403990  [    0/  455]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.408996  [    0/  455]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.400150  [    0/  455]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.404993  [    0/  455]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.396668  [    0/  455]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.401277  [    0/  455]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.393333  [    0/  455]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.397784  [    0/  455]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.390190  [    0/  455]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.394500  [    0/  455]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.387217  [    0/  455]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.391420  [    0/  455]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.384389  [    0/  455]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.388604  [    0/  455]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.381773  [    0/  455]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.385988  [    0/  455]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.379281  [    0/  455]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.383496  [    0/  455]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.376863  [    0/  455]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.381103  [    0/  455]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.374597  [    0/  455]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.378834  [    0/  455]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.372404  [    0/  455]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.376676  [    0/  455]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.370349  [    0/  455]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.374666  [    0/  455]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.368412  [    0/  455]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.372787  [    0/  455]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.366556  [    0/  455]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.371014  [    0/  455]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.364825  [    0/  455]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.369381  [    0/  455]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.363149  [    0/  455]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.367781  [    0/  455]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.361559  [    0/  455]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.366307  [    0/  455]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.359954  [    0/  455]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.364802  [    0/  455]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.358506  [    0/  455]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.363439  [    0/  455]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.357024  [    0/  455]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.362236  [    0/  455]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.355700  [    0/  455]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.361089  [    0/  455]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.354804  [    0/  455]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.360655  [    0/  455]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.355292  [    0/  455]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.362299  [    0/  455]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.357716  [    0/  455]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.366261  [    0/  455]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.360950  [    0/  455]\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses, iteractions = train_loop(train_dl, model, loss_fn, optimizer, epochs=100)\n",
    "\n",
    "# Plot loss vs. iteration\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(iteractions, losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2da05e9852e3725e6cd29dd2f3d6ebaa07dda6697715ddc2b5ea77aa5959f695"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
