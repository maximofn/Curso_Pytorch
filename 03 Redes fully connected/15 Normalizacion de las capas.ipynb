{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización de las capas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con una filosofía similar a la normalización de los datos tenemos la normalización de las capas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos cómo se acyualiza un patámetro al realizar el entrenamiento. Si usamos `SGD`, la actualización de los parámetros se realiza de la siguente manera\n",
    "\n",
    "$$\\omega = \\omega - \\alpha\\frac{\\partial{loss}}{\\partial{\\omega}}$$\n",
    "\n",
    "Esto puede producir dos casos\n",
    " * Que al ir restanto valores a $\\omega$, este se vaya reduciendo tanto, hasta que computacionalmente sea incapaz de representarse y pase a ser 0. Es decir, los ordenadores son capaces de representar números pequeños hasta un cierto valor, por debajo de dicho valor, el número pasa a ser 0. Esto es equivalente a destruir una conexión entre neuronas\n",
    " * En el caso de que el gradiente sea negativo, lo que estamos haciendo es hacer $\\omega$ cada vez más grande. En este caso puede pasar lo contrario, que el valor de $\\omega$ aumente tanto que no sea representable por el computador\n",
    "\n",
    "Ambos casos suponen un problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para solucinar esto, al igual que antes, se calcula la media y la desviación estandar de la salida de una de las capas y se normaliza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización de las capas en Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo se hace esto? Aunque hay varias funciones vamos a ver las `BatchNormXd`, donde `X` puede ser 1, 2 o 3\n",
    "\n",
    "```python\n",
    "class WineNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layers=[100, 50, 20]):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_inputs, hidden_layers[0]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[0], hidden_layers[1]),\n",
    "            torch.nn.BatchNorm1d(hidden_layers[1]),                 // Normalization\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[1], hidden_layers[2]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layers[2], num_outputs),\n",
    "        )\n",
    "        self.activation = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.network(x)\n",
    "        probs = self.activation(logits)\n",
    "        return logits, probs\n",
    "```\n",
    "\n",
    "Como se puede ver se ha metido una capa de normalización después de la segunda capa lineal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
