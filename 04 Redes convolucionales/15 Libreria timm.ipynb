{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librería timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque Pytorch ofrece una gran cantidad de redes preentrenadas para hacer transfern learning, en la mayoría de aplicaciones se usa la librería [timm](https://rwightman.github.io/pytorch-image-models/) porque ofrece el *state of the art* (SOTA) de redes ya preentrenadas. Es decir, en cuanto se crea una nueva arquitectura de red, al poco tiempo la tienes implementada en esta librería\n",
    "\n",
    "El objetivo de este curso es hacer una introducción al deep learning, por lo que se escapa de su alcance explicar la librería. Pero es muy sencilla de utilizar y está documentada.\n",
    "\n",
    "Pero me parece importante mencionarla, para que si en un futuro quieres presentarte a una competición o hacer alguna aplicación es bueno que sepas que existe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cual es el mejor modelo para hacer transfer learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el artículo [Which image models are best?](https://www.kaggle.com/code/jhoward/which-image-models-are-best/notebook), [Jeremy Howard](https://www.kaggle.com/jhoward) hace un estudio de cuales son los modelos más rápidos y con mejor rendimiento para Imagenet. Pero aunque es interesante, no es lo que más nos interesa, porque nosotros no vamos a usar Imagenet, sino que queremos entrenar una red par nuestro problema, por lo que en el artículo [The best vision models for fine-tuning](https://www.kaggle.com/code/jhoward/the-best-vision-models-for-fine-tuning) hace precísamente esto. Realiza un estudio de los modelos más rápidos, con mejor rendimiénto y menor uso de memoria de GPU para el transfer learning, para dos tidos de datasets, unos muy similar a Imagenet, y otro que no se parece tanto"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
