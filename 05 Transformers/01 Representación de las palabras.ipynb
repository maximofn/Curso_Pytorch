{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representación de las palabras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto en el tema de redes convolucionales, las imágenes se representan como la cantidad de rojo, verde y azul de cada pixel. Esa cantidad es un número que varía de entre 0 y 255. Es decir, para representar una imagen necesitamos hacerlo mediante números.\n",
    "\n",
    "Con el lenguaje pasa igual, para poder procesarlo y realizar predicciones o generar texto, necesitamos poder representarlo mediante números. Vamos a ver varias formas de representar el lenguaje mediante números: `encoding ordinal`, `one-hot encoding` y `word embedding`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding ordinal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la manera más básica de representar numéricamente un lenguaje, y consiste en asignar un número a cada palabra, por ejemplo, podemos decir que gato lo representaremos con un 1, perro con un 2, mesa con un 3, ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque el encoding ordinal nos resuelve el problema tiene varios problemas\n",
    "\n",
    " * En el ejemplo que hemos dado se puede asumir que mesa (3) equivale a gato (1) + perro (2), lo cual no tiene nada que ver. En el lenguaje existen relaciones entre las palabras, por ejemplo perro y perra tienen mucha relación, por lo que es necesario un sistema de codificación mejor\n",
    " * Importancia de las palabras. Las palabras van a entrar a redes neuronales, que como hemos visto se componene de capas, con unos pesos, mediante las cuales se realizan unas operaciones matemáticas. Por lo que puede llegar a pasar que la red le de más importancia a las palabras con un número mayor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a esto, se pensó en una alternativa, el one-hot encodding. Aquí lo que se hace es crear vectores de tamaño N, donde cada palabra corresponderá a un vector de todo ceros, menos un uno en una posición determinada. En el ejemplo de antes gato correspondería al vector `[1 0 0 ... 0]`, perro al vector `[0 1 0 ... 0]`, mesa al vector `[0 0 1 ... 0]`, ...\n",
    "\n",
    "Haciendo esta codificación arreglamos los dos problemas que hemos contados del encoding ordinal y además añadimos una ventaja, como las redes neuronales realizan operaciones matriciales, si a una matriz le multiplicas por un vector de todo ceros, menos un uno en una posición, lo que estás haciendo es que el resultado de la operación es obtener la columna o la fila correspondiente a la posición del 1\n",
    "\n",
    "![one-hot encodding matrix multiplication](Imagenes/one_hot_encoding_matrix_multiplication.png)\n",
    "\n",
    "Esto es muy útil si en redes neuronales quieres una fila o una columna de una matriz, porque al no obtener la fila o la columna haciendo slicing, es decir, `matriz[i]`, sino que se obtiene mediante una multiplicación, esta operación es deribable y por tanto se podría añadir al algoritmo del descenso del gradiente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El one-hot encoding está muy bien, pero crea un nuevo problema, y es que en un lenguaje de un millón de palabras, necesitaríamos que cada palabra se codificase con vectores de 999.999 ceros y 1 solo uno. Esto a la hora de hacer operaciones matriciales no es muy eficiente y hace que las redes y datos ocupen mucha memoria"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para solucionar esto se empezó a usar el hot encoding a secas o también llamado word embedding, que consiste en seguir representando cada palabra en vectores, pero ahora cada vector tendrá un tamaño fijo, de momento digamos N, y en el que cada uno de sus componentes puede tener cualquier valor.\n",
    "\n",
    "Esto resuelve el problema de la memoria y la ineficiencia en las operaciones matriciales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además crea una nueva característica, ya que al representar las palabras en vectores de N dimensiones, en realidad lo que estamos haciendo es representar las palabras en un espacio N dimensional. Si esto te suena a muy complicado no te preocupes, que ahora te lo cuento de otra manera , ya verás que sencillo es y cómo lo vas a entender\n",
    "\n",
    "Supongamos que en vez de ser un espacio de N dimensiones, tenemos un espacio de 2 dimensiones, alto y ancho, pues podríamos representar las palabras de esta manera\n",
    "\n",
    "![word embedding 2 dimmension](Imagenes/word_embedding_2_dimmension.png)\n",
    "\n",
    "Como puedes ver todas las palabras que tienen semejanza están juntas. PUes ahora supón que en vez de 2 dimensiones tenemos 3, alto, ancho y profundo, podríamos representar las palabras de esta manera\n",
    "\n",
    "![word embedding 3 dimmension](Imagenes/word_embedding_3_dimmension.png)\n",
    "\n",
    "Ahora las palabras siguen estando juntas por semejanza, pero tenemos una dimensión más para poder hacer más grupos.\n",
    "\n",
    "Como en el lenguaje tenemos muchisimas palabras no nos vale con 3 dimensiones, por lo que tenemos que hacerlo con muchas más. El modelo más grande de word embeding de Bert es de 1024 dimensiones, mientras que el modelo más grande de word embeding de GPT3 es de 4096 dimensiones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta posibilidad de poder representar las palabras en grupos por semejanza también nos da otra ventaja, y es la relación entre palabras. Si al vector que representa la palabra `rey` le restas el vector que representa la palabra `hombre` y le sumas el vector que representa la palabra `mujer` obtienes un vector muy parecido al que representa la palabra `reina`.\n",
    "\n",
    "Como hemos dicho, en el lenguaje existe una gran relación entre las palabras, lo cual va a ser muy importante para entender frases y además es uno de los mecanismos más importantes de los transformers, el de atención, de hecho el paper de los transformers se llama `Attention is all you need` (`Atención es todo lo que necesitas`). Así que gracias a esta forma de representar las palabras podemos prestar atención a esta relación entre palabras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como se crean los word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos explicado cómo se representan muy bien las palabras gracias a los word embedding, hemos mostrado como las palabras con semejanzas están representadas juntas y hemos hablado de los word embeddings de Bert y GPT3. Pero no hemos explicado cómo se crean, es decir, cómo se decide qué valor tiene cada item del vector para cada palabra. Obviamente eso no lo hace ninguna persona\n",
    "\n",
    "Cuando se entrenan los grandes modelos, también se entrenan las capas de embedding, ya que pertenecen al modelo, por lo que durante el entrenamiento de la red, todos estos vectores que representan las palabras del lenguaje van cogiendo forma y se van uniendo en grupos (cluesters) por significado semántico. Por lo que si en el entrenamiento se ha introducido texto muy variado, este word embeding estará mejor hecho que si el texto no es variado. Por ejemplo, si el dataset solo contiene textos económicos, contables, etc. es muy probable que en el word embedding, la palabra `banco` no tenga nada de relación con la palabra `silla`\n",
    "\n",
    "Por lo que para un uso general del modelo, es necesario que el dataset de entrenamiento sea lo más variado posible. Pero si el uso va a ser muy específico de ese tema, no es necesario que el dataset sea tan variado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar el embedding de Bert para ver cómo representa las palabras, para ello vamos a usar [huggingface](https://huggingface.co) que se hizo muy popular gracias a su librería [transformers](https://huggingface.co/docs/transformers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el modelo Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora su tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el token de la palabra `hola`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codifica \"hola\" en tokens y agrega los tokens especiales [CLS] y [SEP]\n",
    "input_ids = tokenizer.encode(\"hola\", add_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más adelante entenderás esto, pero el modelo Bert solo contiene la parte de codificación del transformer, por lo que para obtener el embedding de la palabra `hola` lo que hacemos es pasarla por el modelo. Primero convertimos su token a un vector y luego hacemos inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte los IDs a tensores y pasa por el modelo para obtener los embeddings\n",
    "input_ids_tensor = torch.tensor([input_ids])\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos el embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " torch.Size([768]),\n",
       " tensor([-2.5825e-01, -4.1451e-01,  1.3310e+00,  5.0903e-02, -1.2352e-01,\n",
       "         -2.5993e-01, -7.3110e-01, -1.4308e-01,  7.0695e-01,  6.4720e-02,\n",
       "         -4.4573e-01,  2.7530e-01, -4.5984e-01,  9.7510e-01,  3.5397e-01,\n",
       "          2.4433e-01,  4.8471e-01,  7.0356e-01, -2.9036e-01,  1.8888e-02,\n",
       "          7.4378e-01,  5.2929e-01, -7.6937e-01, -3.6225e-01, -2.5520e-01,\n",
       "         -1.0521e+00, -1.0024e+00, -9.4964e-01, -4.0125e-01, -2.5991e-01,\n",
       "         -1.7926e-02,  1.9220e-01,  6.0427e-01,  1.9149e-01, -5.8110e-01,\n",
       "          9.2513e-01, -1.6392e-01, -3.8327e-01,  8.4580e-02, -6.9295e-01,\n",
       "         -7.2972e-02, -3.5562e-01, -3.9924e-01,  1.2729e-01,  7.2401e-01,\n",
       "         -7.1348e-01,  1.1409e-01, -9.0983e-02,  2.5221e-01, -6.5058e-01,\n",
       "          3.4572e-01, -2.4137e-01,  1.7462e-01,  3.0785e-01, -1.0352e-01,\n",
       "          8.9522e-02,  6.7650e-01, -1.9520e-01, -4.4381e-01,  7.4259e-01,\n",
       "          3.4907e-02, -2.0905e-01, -2.2943e-01, -2.7157e-01, -5.2984e-02,\n",
       "          4.5041e-01, -3.4271e-02,  2.5017e-01,  1.6648e-01,  5.6548e-01,\n",
       "          5.2772e-01,  2.0025e-01,  2.7856e-01, -6.8803e-02, -1.1460e-01,\n",
       "          5.4109e-01,  4.2133e-01, -4.0985e-02, -4.5428e-01, -2.2600e-02,\n",
       "          2.9526e-01,  3.9938e-01, -6.6892e-01,  3.9901e-01, -2.9518e-02,\n",
       "          2.5004e-01,  1.2302e-01, -7.1713e-01,  9.8863e-02, -3.0180e-01,\n",
       "          2.8751e-01, -7.4658e-02, -2.2376e-01,  1.3947e-01, -8.8486e-02,\n",
       "          5.6340e-01,  4.8550e-01, -1.6552e-01, -3.4574e-01, -3.4350e-01,\n",
       "         -2.6928e-03,  6.8230e-03,  3.0957e-01, -5.4541e-01,  7.4116e-01,\n",
       "          5.2804e-01, -4.6520e-01,  4.1979e-01, -3.9053e-02,  3.0877e-01,\n",
       "          2.1298e-01,  8.8229e-03, -6.5908e-02, -1.8597e-01, -5.7949e-02,\n",
       "         -2.1972e-01,  5.0315e-02, -3.4142e-01,  3.4964e-01, -5.2466e-01,\n",
       "          4.5714e-01, -9.7181e-01,  2.2560e-01, -6.4043e-01,  6.1906e-01,\n",
       "         -7.0823e-01,  5.2540e-01,  1.4335e-01,  1.5776e-01, -2.4592e-01,\n",
       "         -8.4858e-01,  6.3599e-01, -1.3715e-01,  3.7194e-01,  8.3884e-01,\n",
       "          7.8762e-02, -9.1356e-02,  6.0297e-01,  5.2168e-01, -1.2847e-01,\n",
       "          4.4937e-01, -5.0882e-02, -4.3448e-01, -5.1748e-01,  6.1088e-02,\n",
       "          2.0309e-01, -4.6948e-02, -2.4922e-01, -1.0234e-01, -2.2967e-01,\n",
       "          4.9019e-01, -3.4253e-01,  1.1985e-01,  8.0837e-01,  1.1348e-01,\n",
       "          1.1909e-01, -5.8680e-01,  1.8841e-01, -2.0953e-01,  8.5059e-02,\n",
       "         -7.6992e-02,  6.1587e-01,  7.3014e-02,  1.1745e+00,  3.3159e-01,\n",
       "          4.4779e-01, -1.7294e-01, -8.8308e-01, -4.2922e-01,  4.0261e-01,\n",
       "          4.8596e-01,  7.5293e-01,  5.1704e-01, -1.0969e+00,  2.5361e-01,\n",
       "         -6.6033e-01, -7.8854e-01,  2.5082e-01, -9.9266e-02, -3.4348e-01,\n",
       "         -5.5196e-02,  6.3647e-01, -3.6725e-01, -4.7996e-01,  8.0175e-01,\n",
       "          7.1838e-01,  8.3973e-02,  6.6396e-01, -1.4945e-01, -3.2832e-02,\n",
       "          5.7759e-01, -3.9102e-01, -2.7092e-01, -2.0582e-02,  5.3514e-01,\n",
       "          1.0083e+00,  7.1328e-01,  4.7898e-02, -3.3864e-01,  3.0087e-02,\n",
       "         -3.3216e-01,  4.7014e-01,  1.9827e-01, -1.9741e-02,  1.8255e-01,\n",
       "          7.3555e-01,  5.5707e-01, -5.4740e-01, -5.3417e-01, -2.1649e-01,\n",
       "         -4.8752e-01,  2.8207e-02,  1.4778e-01,  2.4485e-01,  2.0865e-01,\n",
       "          1.9512e-01,  6.0489e-02,  5.8498e-02,  3.2981e-01,  3.8871e-01,\n",
       "         -1.3441e-01, -2.0938e-01,  1.2635e-01, -2.9667e-01, -2.4350e-01,\n",
       "         -8.3465e-01, -3.6154e-01,  7.6929e-02,  1.0488e+00,  2.8111e-01,\n",
       "          4.7788e-01, -2.2853e-01,  1.1423e-01,  1.2028e+00, -2.0393e-01,\n",
       "         -5.5550e-01,  2.7389e-02, -4.0413e-01, -9.0180e-02,  9.1589e-02,\n",
       "          1.8545e-01, -2.3418e-01,  2.0616e-01,  2.7090e-01,  7.5456e-01,\n",
       "          1.6548e-01, -1.8258e-01,  3.1321e-01,  6.5229e-01, -1.1259e+00,\n",
       "         -6.2501e-01,  5.0933e-01,  5.8251e-01, -4.7929e-01, -1.7829e-01,\n",
       "         -1.0265e-01, -4.9357e-01, -1.3672e-01,  4.9565e-01, -9.0260e-02,\n",
       "          2.9207e-01, -1.4003e-01,  8.8573e-02,  4.0531e-01,  6.4640e-02,\n",
       "         -4.4815e-01,  1.4784e-01,  3.9559e-01, -3.3175e-01,  1.5630e-01,\n",
       "         -2.8196e-01, -6.3732e-01, -3.4184e-01,  4.9398e-01, -6.2488e-02,\n",
       "          1.2971e-01, -1.2725e-01,  4.1088e-01,  1.5616e-01,  5.2418e-01,\n",
       "          3.6864e-02,  2.9553e-01, -2.1984e-02,  3.2889e-01,  2.5950e-01,\n",
       "          3.7110e-01, -5.9618e-01, -2.4965e-02,  7.0409e-01, -3.4485e-01,\n",
       "         -3.9417e-01,  4.6341e-02,  4.6692e-01, -5.9813e-01, -3.9973e-01,\n",
       "         -6.5458e-01, -3.7964e-01,  9.2486e-02,  1.5430e-01,  3.0686e-01,\n",
       "         -4.8729e-01, -6.8723e-01, -2.6829e-01,  1.0837e-01, -3.4077e-01,\n",
       "          5.2579e-01,  1.8083e-01,  3.0885e-01,  4.3724e-01,  2.9607e-01,\n",
       "         -1.1229e+00,  1.3425e-01, -2.4213e-01,  1.0098e-01, -1.0453e+00,\n",
       "         -7.2261e-02,  1.0527e-01, -3.2078e-01,  5.1410e-02,  4.5599e-01,\n",
       "         -3.9070e-01, -5.6791e-02, -3.2807e-01,  2.4743e-02,  6.1843e-03,\n",
       "         -1.9241e-02, -8.7904e-02, -2.8295e-01, -1.1292e-01,  4.4434e-01,\n",
       "         -5.5298e-01,  1.6701e+00,  2.4631e-01, -6.0632e-01, -3.8101e-01,\n",
       "         -1.6728e-01, -4.1187e-01,  3.7242e-01, -4.7237e-01, -3.7068e-01,\n",
       "         -2.2244e-01,  3.0735e-02,  3.0767e-01, -1.7766e-01,  3.3356e-01,\n",
       "         -5.3968e-01, -1.2398e-01,  1.1270e+00,  2.3696e-01, -3.9356e-01,\n",
       "          4.0454e-01,  7.7514e-01,  1.3773e-01,  3.5332e-01, -4.9657e-02,\n",
       "         -1.3908e-01, -5.1421e-01,  5.7348e-01,  3.9149e-01,  4.2659e-01,\n",
       "         -4.4654e-01,  5.7964e-01, -4.1247e-01, -3.0203e-01,  1.5923e-01,\n",
       "          1.7008e-01,  1.5853e-01, -6.5512e-01,  4.0509e-01,  1.5434e-01,\n",
       "          2.7988e-01,  1.1538e+00,  7.1329e-02, -5.5685e-01, -5.0432e-01,\n",
       "         -3.2112e-01,  3.7954e-01,  7.0191e-01,  8.1919e-01,  5.3862e-01,\n",
       "          1.9385e-01, -2.8539e-01, -3.8453e-01,  3.7991e-01,  2.7743e-03,\n",
       "         -3.6790e-01, -2.1858e-01, -2.5162e-01, -2.8518e-01, -1.9961e-01,\n",
       "          6.0209e-02, -3.8936e-01, -5.2719e-01,  2.7635e-01,  2.2088e-01,\n",
       "         -1.9439e-01, -5.4365e-01,  2.4449e-01, -5.5246e-01,  5.8949e-02,\n",
       "          2.3092e-01, -2.4247e-01, -4.0883e-01,  4.1931e-01,  3.2460e-02,\n",
       "         -3.8152e-02,  7.7652e-01,  1.1433e-01,  4.4207e-01,  8.9557e-02,\n",
       "         -4.9538e-02, -3.8856e-01,  3.0611e-01,  6.7259e-03, -1.0294e-01,\n",
       "          2.0491e-01, -8.3063e-01,  1.0400e-01,  7.5385e-02,  1.9079e-01,\n",
       "          7.7134e-01,  7.8497e-03, -4.0084e-01, -1.0995e+00, -2.0998e-01,\n",
       "         -2.1493e-01,  3.6324e-01, -2.6929e-01,  4.4935e-01,  7.9469e-02,\n",
       "         -2.0338e-01,  7.5015e-04,  2.0809e-01, -5.3361e-01, -8.5694e-02,\n",
       "          1.5571e-01,  1.9137e-01,  2.9598e-01, -7.9180e-01, -8.5024e-02,\n",
       "          4.5093e-01,  2.8476e-01, -2.3477e-02,  3.1353e-01, -3.8972e-01,\n",
       "         -5.8812e-01, -1.4405e-01,  5.2667e-01,  2.1114e-01,  6.9160e-01,\n",
       "          1.1040e-01, -6.6212e-01, -2.4167e-01, -3.1208e-01, -6.8946e-01,\n",
       "          9.0065e-02, -3.2550e-01,  1.0865e-01, -6.9225e-02,  7.9415e-01,\n",
       "          3.5719e-01, -4.7709e-02,  5.8496e-01,  3.2404e-01,  3.2710e-01,\n",
       "         -5.3465e-01, -6.5190e-01, -4.4160e-01, -2.7959e-01,  3.3781e-01,\n",
       "         -4.3085e-01, -9.5965e-02,  4.4950e-01,  2.6725e-01, -5.1892e-01,\n",
       "          9.1187e-01,  7.7019e-01,  2.8171e-01,  1.6482e-01, -3.9435e-01,\n",
       "          2.2941e-01, -1.6950e-01,  1.0807e+00, -2.6920e-01, -6.3293e-02,\n",
       "          6.3354e-02,  1.3716e-01,  3.5841e-01,  1.8765e-02,  6.3029e-01,\n",
       "          1.2520e+00, -4.0800e-01,  1.3768e-01, -1.7872e-01, -4.2460e-01,\n",
       "          1.8329e-03,  6.6827e-01, -4.8916e-01,  3.5838e-02, -1.7510e-01,\n",
       "          3.0494e-01,  2.5800e-01, -1.4203e-02, -1.9342e-01,  7.4762e-02,\n",
       "         -1.2156e+00, -3.8086e-01,  7.5571e-01, -7.4717e-01, -4.8127e-01,\n",
       "          4.7156e-01, -1.0115e+00,  7.2005e-01,  1.4904e-01,  1.0712e-01,\n",
       "         -1.0700e+00, -6.3591e-01, -1.7679e-01, -2.3520e-01,  2.1834e-01,\n",
       "         -7.1132e-01,  5.6419e-01, -5.5347e-01, -4.2149e-01, -8.5917e-01,\n",
       "         -9.2111e-01, -5.0382e-01, -2.6803e-01, -3.8510e-01, -6.3019e-02,\n",
       "         -2.7504e-01, -4.8096e-01,  4.1766e-02,  2.4598e-01, -7.8069e-02,\n",
       "         -5.1462e-04, -1.5674e-01,  8.2449e-02, -3.9807e-01, -5.5167e-01,\n",
       "         -7.2173e-01, -3.6543e-01, -8.6475e-02,  3.0335e-01, -7.3499e-02,\n",
       "          6.9072e-01,  3.4314e-02, -3.1909e-01,  1.1327e+00, -2.1054e-01,\n",
       "          4.3408e-01,  2.2886e-01,  2.7666e-01, -2.2957e-01,  5.4332e-01,\n",
       "          9.0909e-02, -1.8733e-01, -2.5756e-02, -5.2694e-01,  3.1615e-01,\n",
       "         -4.4205e-02, -3.9433e-01,  3.6853e-01, -3.6912e-01, -1.9299e-01,\n",
       "         -2.0933e-02,  3.0260e-01,  7.7690e-01,  3.1010e-01, -3.1683e-02,\n",
       "          2.5678e-01,  8.2799e-02, -3.1035e-01, -3.6144e-01,  8.8207e-03,\n",
       "         -3.5352e-01,  6.5795e-02, -3.1118e-01,  4.2112e-01,  6.9235e-01,\n",
       "          7.7507e-01, -4.4271e-01, -1.9444e-01, -1.7643e-03,  4.8237e-01,\n",
       "          2.7745e-01,  8.1946e-01,  7.8944e-01,  6.6033e-02,  2.5566e-01,\n",
       "          4.6346e-01,  4.3867e-01,  1.1042e-01,  5.2630e-01,  2.5568e-01,\n",
       "          5.4265e-01,  7.5593e-03,  1.6307e-01,  7.5912e-01,  1.4914e-01,\n",
       "          1.8400e-01, -4.5468e-01,  6.3936e-01, -2.3667e-01, -5.9620e-01,\n",
       "         -6.5020e-02,  6.5176e-01,  3.4994e-01, -6.3388e-01,  1.3962e-01,\n",
       "         -5.0519e-02, -1.5701e-01,  2.0605e-01, -1.3078e-01,  4.2799e-01,\n",
       "         -2.5557e-01, -5.1880e-01, -3.8730e-01,  8.6814e-02, -1.3519e-01,\n",
       "          3.3113e-01, -3.6915e-01,  7.0775e-01, -6.7905e-01, -1.6133e-01,\n",
       "         -1.6250e-01, -2.0065e-01, -1.6880e-01,  5.6510e-02, -3.4621e-01,\n",
       "         -8.2647e-01,  5.9519e-01,  2.0442e-01, -1.2010e-01, -1.6275e-01,\n",
       "         -1.6276e-01, -8.3049e-02, -1.7422e-01, -9.2428e-01,  1.7473e-01,\n",
       "          8.9459e-02, -2.3648e-01, -1.7369e-01, -5.7046e-01,  3.9763e-01,\n",
       "          6.4014e-01, -7.6717e-01,  7.1451e-01, -2.0290e-01, -3.2099e-01,\n",
       "          1.2280e+00, -3.2537e-01,  2.4774e-01, -8.4226e-01,  8.5112e-02,\n",
       "         -2.5876e-01, -3.0974e-01, -1.0391e+00,  2.4101e-01, -3.2001e-01,\n",
       "          1.7402e-01,  1.9405e-02, -9.3991e-02, -3.7987e-01, -6.4105e-01,\n",
       "          1.0521e-01, -2.7520e-01,  1.9141e-01, -2.1314e-02, -7.6020e-01,\n",
       "          6.1420e-01,  2.7288e-01,  3.3042e-01,  4.6296e-01, -4.5412e-01,\n",
       "         -1.2968e-01, -2.1748e-01,  9.2562e-01,  5.8016e-01, -7.8493e-02,\n",
       "          5.9380e-01, -3.0175e-01,  2.8850e-01, -5.6617e-02,  3.3344e-01,\n",
       "         -5.3179e-01, -2.7053e-01,  1.1324e-01,  5.1270e-01,  1.2839e-02,\n",
       "          3.0462e-01,  1.1437e-01, -3.9658e-01,  9.4069e-02,  6.9815e-02,\n",
       "          2.8108e-01,  3.1108e-01,  5.5833e-01,  2.6595e-01,  4.1064e-01,\n",
       "         -1.8158e-01, -2.9689e-01, -1.5877e-01,  8.5765e-02, -8.8096e-02,\n",
       "          3.4590e-01,  7.7852e-01, -1.4876e-01, -1.3339e-01,  4.7070e-01,\n",
       "          1.4376e-01, -2.3130e-01,  1.8085e-01, -3.7666e-01,  1.3460e-02,\n",
       "         -4.9131e-01,  1.7173e-01,  4.2091e-01, -3.2985e-01,  1.2880e+00,\n",
       "         -6.1066e-02, -1.0905e+00, -5.2591e-01,  7.6820e-03, -1.8256e-01,\n",
       "         -1.0568e-01,  3.8647e-02, -2.5952e-01, -4.3041e-01,  8.7920e-02,\n",
       "         -3.8376e-01, -1.2933e+00, -3.9699e-01, -1.6890e-01, -4.1329e-01,\n",
       "         -5.5927e-01,  2.4547e-01,  4.8992e-01, -4.3188e-01, -5.2326e-01,\n",
       "         -4.4401e-01, -1.3911e-01,  4.7853e-01, -5.0210e-01, -6.0040e-01,\n",
       "         -6.8643e-01, -2.3887e-01, -3.0083e-01, -9.8818e-02, -2.9556e-01,\n",
       "          1.8756e-01,  1.2806e-01, -2.3050e-01, -6.3612e-01, -3.8646e-01,\n",
       "          5.3126e-01,  6.0666e-01, -5.4809e-01, -1.5052e-02,  3.7715e-01,\n",
       "         -7.1793e-01, -8.1560e-01,  3.3117e-01,  1.2017e-02, -6.8313e-01,\n",
       "         -9.8275e-02, -3.5623e-01,  7.5204e-01]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El primer elemento de la salida del modelo son los embeddings para cada token.\n",
    "# Tomamos el segundo token (índice 1) ya que el primer token es [CLS]\n",
    "hola_embedding = outputs[0][0][1]\n",
    "\n",
    "type(hola_embedding), hola_embedding.shape, hola_embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos obtenemos un tensor de tamaño 768, esto quiere decir que el modelo Bert que hemos usado tiene un word embedding de tamaño 768, es decir, cada palabra estará representada por un vector de 768 valores distintos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similitud coseno entre 'rey - hombre + mujer' y 'reina' es: 0.728291928768158\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# Carga el pre-trained BERT\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Lista de palabras\n",
    "palabras = [\"rey\", \"reina\", \"hombre\", \"mujer\"]\n",
    "\n",
    "# Diccionario para guardar los embeddings\n",
    "embeddings = {}\n",
    "\n",
    "for palabra in palabras:\n",
    "    # Codifica la palabra en tokens y agrega los tokens especiales [CLS] y [SEP]\n",
    "    input_ids = tokenizer.encode(palabra, add_special_tokens=True)\n",
    "\n",
    "    # Convierte los IDs a tensores y pasa por el modelo para obtener los embeddings\n",
    "    input_ids_tensor = torch.tensor([input_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids_tensor)\n",
    "\n",
    "    # El primer elemento de la salida del modelo son los embeddings para cada token.\n",
    "    # Tomamos el segundo token (índice 1) ya que el primer token es [CLS]\n",
    "    embeddings[palabra] = outputs[0][0][1]\n",
    "\n",
    "# Realiza la operación rey - hombre + mujer\n",
    "resultado = embeddings[\"rey\"] - embeddings[\"hombre\"] + embeddings[\"mujer\"]\n",
    "\n",
    "# Calcula la similitud coseno entre el resultado y la palabra \"reina\"\n",
    "similitud = cosine_similarity(resultado.unsqueeze(0), embeddings[\"reina\"].unsqueeze(0))\n",
    "\n",
    "print(f\"La similitud coseno entre 'rey - hombre + mujer' y 'reina' es: {similitud.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
