{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La arquitectura transformer fue presentada en 2017 en el paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762). En el paper resolvieron la tarea de traducción mediante únicamente mecanismos de atención. Sin embargo más adelante se vio la potencia de esta arquitectura que ahora está presente no solo en problemas de traducción, sino en todos los problemas relacionados con el lenguaje e incluso mediante los visual transformers se han conseguido mejores resultados en tareas de visión que con redes convolucionales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la arquitectura del modelo transformer\n",
    "\n",
    "![transformer architecture model](Imagenes/transformer_architecture_model.png)\n",
    "\n",
    "Iremos viendo cada uno de los bloques por separado para entenderlos y así más adelante volver a ver la arquitectura y poder entender qué hace el modelo transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que en la arquitectura hay tres bloques naranjas `Multi-Head Attention`. Como ya hemos dicho, la arquitectura transformer se centró en solo usar mecanismos de atención para resolver el problema de traducción de textos, pero viendo su arquitectura podemos ver cómo de importante son esos mecanismos en el funcionamiento del transformer"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
