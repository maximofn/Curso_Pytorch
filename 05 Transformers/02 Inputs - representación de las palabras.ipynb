{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs - representación de las palabras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que tenemos que ver es cómo le introducimos una frase al transformer\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_inputs.png\" alt=\"Encoder inputs\" style=\"width:425px;height:626px;\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representación de las palabras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto en el tema de redes convolucionales, las imágenes se representan como la cantidad de rojo, verde y azul de cada pixel. Esa cantidad es un número que varía de entre 0 y 255. Es decir, para representar una imagen necesitamos hacerlo mediante números.\n",
    "\n",
    "Con el lenguaje pasa igual, para poder procesarlo y realizar predicciones o generar texto, necesitamos poder representarlo mediante números. Para ello se utilizan los `token`s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para explicar qué son los `token`s, primero veámoslo con un ejemplo práctico, vamos a usar el tokenizador de BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT es un modelo grande de lenguaje creado por Google. Para usar su tokenizador vamos a usar la librería de huggingface"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La empresa [huggingface](https://huggingface.co) se hizo muy popular gracias a su librería [transformers](https://huggingface.co/docs/transformers), ya que hizo que fuese muy sencillo usarlos gracias a su API. Ahora no solo podemos usar transformers, sino casi cualquier modelo, ya que se autodefinen como el GitHub del deep learning, así que casi todos los nuevos modelos se publican también en huggingface, donde los puedes usar con su librería, ver su documentación e incluso probar online gracias a sus `spaces`\n",
    "\n",
    "En este curso no vamos a explicar hugginface, pero es bueno que la conozcas, ya que es un muy buen recurso"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el modelo BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguramente obtendrás un warning como yo, ya que estamos importando los pesos de `bert-base-multilingual-cased` que fue entrenado con más capas de las que aparecen en el modelo `BertModel`. Si fuésemos a usar el modelo para lo que fue entrenado y dio como resultado los pesos `bert-base-multilingual-cased` deberíamos importar las capas extra, pero como no es nuestro caso podemos ignorar el warning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cargamos su tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los `token`s de la palabra `hello`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 61694, 10133, 102]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"hello\", add_special_tokens=True)\n",
    "input_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a analizar los `ids` o `token`s.\n",
    "\n",
    "Por cómo está implementado BERT siempre espera que cada frase empieze con el token `[CLS]` (`101`) y que termine con el token `[SEP]` (`102`), por lo que el tokenizador de BERT los añade automáticamente\n",
    "\n",
    "Por otro lado los tokens `61694` y `10133` son los tokens generados por dividir la palabra `hello` en dos subpalabras, que son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'hell', '##o', '[SEP]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT ha dividido la palabra `hello` en `hell` y `##o`.\n",
    "\n",
    "Cuando se crea el vocabulario que se quiere que tenga nuestro modelo, se hacen descomposiciones de palabras en otras más sencillas, a estas se les llaman `token`s\n",
    "\n",
    "De modo que no habrá un token `perro` y otro `perra` sino que habrá un token `perr`, otro `##o` y otro `##a` (las dobles almohadillas indican que son el final de una palabra).\n",
    "\n",
    "Puedes pensar, pero `perro` y `perra` son dos tokens y `perr`, `##o` y `##a` son tres tokens, y es verdad, pero ahora podemos añadir el token `herman` y juntarlo con los tokens ya creados `##o` y `##a`, y añadir el token `cuñad` y volver a juntarlo con los tokens ya creados `##o` y `##a`. De esta manera hemos pasado de tener 6 tokens `perro`, `perra`, `hermano`, `hermana`, `cuñado` y `cuñada` a solo 5 con `perr`, `herman`, `cuñad`, `##o` y `##a`. Hemos ahorrado un token, y a medida que añadamos más palabras terminadas en `o` o `a` seguiremos reduciendo el número de tokens.\n",
    "\n",
    "Esta técnica y otras de dividir las palabras en otras más sencillas se llama tokenización, lo que produce un vocabulario más pequeño y así poder usar modelos más pequeños"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
