{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el lenguaje el orden de las palabras es muy importante, ya que `Juan quiere a María` no significa lo mismo que `María quiere a Juan`, en las dos frases hay una persona que quiere a otra, pero por cambiar solo dos palabras de sitio en cada frase es distinta la persona que quiere a la otra. O en las frases `estoy seguro de que no debemos hacerlo` y `no estoy seguro de debamos hacerlo` la posición de la palabra `no` cambia bastante el significado de la frase. Por lo que es importante tener algún mecanismo para poder determinar el orden de cada palabra en la frase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de los transformers, para problemas de lenguaje se usaban modelos recurrentes, es decir, modelos a los que les entraba un token y generaban una salida, a continuación les entraba otro token y generaban una segunda salida. De esta manera no hacía falta saber la posición de los tokens, porque entraban al modelo de uno en uno. Mientras que a los transformer les entra la frase entera y mediante los mecanismos de atención se hace la magia\n",
    "\n",
    "Estos modelos recurrentes tenían el problema de que si la frase era muy larga, cuando iban por los tokens finales se habían olvidado de los primeros tokens, por lo que los resultados que se obtenían no eran muy buenos\n",
    "\n",
    "Con los transformers, al introducir la frase entera al modelo este problema se soluciona, pero se añade el problema de no saber el orden de los tokens, por eso se tuvo que crear el mecanismo de `positional encoding`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una primera intuición de cómo solucionar esto sería poner índices a los tokens en función de su orden, es decir, el primer token tendría el índice 0, el segundo el índice 1, y así sucesivamente. Sin embargo esto tiene el problema de que para que el modelo sea derivable y así poder hacer la operación del descenso del gradiente no podemos poner el índice a secas y con un `if` o un `for` ir buscando tokens. Por lo que ese índice se tendría que sumar, multiplicar, o cualquier otra operación, al vector de embedding de los tokens. Por lo que a medida que avanzamos en la frase, el índice será mayor, lo que supondrá números mayores y que con las multiplicaciones de los pesos puede hacer que se le de más importancia a los últimos tokens de la frase que a los primeros\n",
    "\n",
    "Otro efecto negativo de esto es que como hemos explicado, los embedding corresponden a la posición de los tokens en un espacio vectorial, en el que los tokens que tienen un significado parecido suelen estar colocados juntos. Por lo que si se modifican de esta manera (sumando, restando, multiplicando, ...), algunos pueden llegar a modificarse tanto que se muevan a otra zona de otros tokens con otro significado. Por lo que todo lo que ganamos con el word embedding lo perdemos con el positional encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto necesitamos algo que nos permita saber la posición y además al añadírselo al vector de embedding no altere el resultado de la inferencia o del entrenamiento de la red\n",
    "\n",
    "Que es lo que se hace en la capa de positional encoding del transformer\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_encoder_positional_embedding.png\" alt=\"Encoder positional encoding\" style=\"width:425px;height:626px;\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pocicion mediante seno y coseno"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La solución que proponen en el paper es determinar la posición mediante un seno y un coseno\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/positional_encoding.png\" alt=\"positional encoding\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como sabemos, el valor del seno y del coseno va de entre -1 y 1, por lo que al añadirlos al vector de embedding solo va a modificar cada valor del vector entre -1 y 1. Por tanto, los tokens no se van a ver muy desplazados dentro del espacio vectorial. Cada valor del vector de un token cambiará entre -1 y 1, lo que hará que se mueva el token dentro de ese espacio vectorial, pero el desplazamiento será tan pequeño que seguirá dentro de la zona de tokens con el mismo significado\n",
    "\n",
    "En la siguiente figura podemos ver como el token `batery` está en una zona del espacio vectorial y se ve media circunferencia de puntos de las zonas a las que podría desplazarse\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/positional_encoding_embeding_displacement.png\" alt=\"word embedding displacement\">\n",
    "</div>\n",
    "\n",
    "\n",
    "En realidad en el ejemplo de la imagen, si imaginamos un círculo alrededor de la imagen, todo ese área es la zona a la que se podría desplazar el token tras el positional encoding\n",
    "\n",
    "En el siguiente espacio vectorial cada token se movería en un espacio correspondiente a una esfera de radio 1 alrededor de el. Podemos ver que los ejes van desde -1000 hasta 1000, más o menos, en cada eje. Por lo que un desplazamiento de 1 en cualquiera de cada uno de los ejes no significaría apenas un desplazamiento \n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/word_embedding_3_dimmension.png\" alt=\"word embedding 3 dimmension\" style=\"width:662px;height:467px;\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvamos a la fórmula\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/positional_encoding.png\" alt=\"positional encoding\">\n",
    "</div>\n",
    "\n",
    "Lo que se está diciendo con estas fórmulas es que se va a crear una matriz de positional encoding que va a tener tantas filas como la frase más larga que pueda entrar al transformer y tantas columnas como las dimensiones de nuestro word embedding\n",
    "\n",
    "Esto lo explicaremos más adelante, pero para entrenar a los transformers no les puedes introducir primero una frase de 10 tokens, luego otra de 20 tokens, etc. Porque eso corresponde a matrices.\n",
    "Como hemos visto en los entrenamientos de redes neuronales se hacen batches de entradas y salidas, se meten a la red y se hace el descenso del gradiente. Por lo que si una frase tiene 10 tokens, otra 20 tokens y así, no se pueden hacer batches. Así que lo que se hace es definir una longitud máxima de frase, de manera que las frases que tengan una longitud menor se rellenan con embeddings que no corresponden a nada.\n",
    "Por esto la matriz de positional encoding tendrá tantas filas como se defina esta longitud\n",
    "\n",
    "Por otro lado hemos dicho que nuestro word embedding es un espacio vectorial de tantas dimensiones que queramos. Esto quiere decir que para cada token se creará un vector con tantas posiciones o de una longitud como la dimensión que definamos para el word embedding. Si defnimos un word embedding de 512 eso quiere decir que cada token corresponderá a un vector de tamaño 512.\n",
    "Por eso, el número de columnas de nuestro positional encoding será igual a la dimensión de nuestro word embedding\n",
    "\n",
    "Teniendo claro qué corresponde a cada dimensión de nuestro positional encoding, lo que nos dice la fórmula, es que para cada posición (para cada fila) las posiciones pares (0, 2, 4, ...) tendrán un valor determinado por un seno, y que cada posición impar (1, 3, 5, ...) tendrá un valor determinado con un coseno\n",
    "\n",
    "El resultado del seno o del coseno depende de la posición del token en la frase, de la posición del vector de embedding y del tamaño del word embedding\n",
    "\n",
    "Por lo que podemos ver, se van a crear vectores con valores entre -1 y 1 de manera que su valor dependerá de la posición de l token en la frase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos podido crear la matriz de positional encoding, lo que haremos será sumarla a la matriz de word embedding, sabiendo que esto desplazará cada token dentro del espacio vectorial, pero será un desplazamiento muy pequeño y no alterará prácticamente su significado semántico"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué los investigadores elgieron una combinación de seno y coseno, en vez de solo senos o solo cosenos? Supongo que probarían varias opciones y se quedaron con esta que es la que mejor les funcionó"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación del positional encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear directamente la clase de positional encoding que hace todo lo que hemos explicado y alguna cosa más que ahora explicaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_sequence_len, embedding_model_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq_len: length of input sequence\n",
    "            embed_model_dim: demension of embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_model_dim\n",
    "\n",
    "        # create constant 'positional_encoding' matrix with values dependant on pos and i\n",
    "        positional_encoding = torch.zeros(max_sequence_len, self.embedding_dim)\n",
    "        for pos in range(max_sequence_len):\n",
    "            for i in range(0, self.embedding_dim, 2):\n",
    "                positional_encoding[pos, i]     = math.sin(pos / (10000 ** ((2 *     i) / self.embedding_dim)))\n",
    "                positional_encoding[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i+1)) / self.embedding_dim)))\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "        # Esta última línea es equivalente a hacer self.positional_encoding = positional_encoding\n",
    "        # sin embargo al hacerlo así PyTorch no lo considerará parte del estado del modelo.\n",
    "        # Hay varias implicaciones de esto:\n",
    "        #  * Movimiento de dispositivos: Cuando se mueve el modelo a la GPU con model.to(device), \n",
    "        #    PyTorch automáticamente moverá todos los parámetros y buffers registrados al dispositivo especificado.\n",
    "        #    Sin embargo, no moverá los tensores que no sean parámetros o buffers registrados. Por lo tanto, \n",
    "        #    si se asigna positional_encoding a self.positional_encoding directamente, habría que moverlo \n",
    "        #    manualmente a la GPU.\n",
    "        #  * Serialización: Cuando se guarda el modelo con torch.save(model.state_dict(), PATH), PyTorch guardará \n",
    "        #    todos los parámetros y buffers registrados del modelo. Pero no guardará tensores que no son parámetros \n",
    "        #    o buffers registrados. Por lo tanto, si se asigna positional_encoding a self.positional_encoding \n",
    "        #    directamente, no se guardará cuando se guarde el estado del modelo.\n",
    "        #  * Modo de evaluación: Algunos métodos, como model.eval(), cambian el comportamiento de ciertas capas del \n",
    "        #    modelo (como Dropout o BatchNorm) dependiendo de si el modelo está en modo de entrenamiento o evaluación.\n",
    "        #    Para que estas capas funcionen correctamente, PyTorch necesita conocer su estado actual, que se almacena\n",
    "        #    en sus parámetros y buffers registrados. Si positional_encoding no está registrado como un buffer, \n",
    "        #    entonces no será afectado por el cambio de modo.\n",
    "        # En resumen, si no se usa register_buffer para positional_encoding, se tendrían que manejar estas cosas \n",
    "        # manualmente, lo cual podría ser propenso a errores.\n",
    "        # La principal ventaja de usar register_buffer() es que PyTorch se encargará de estas cosas por nosotros.\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            x: output\n",
    "        \"\"\"\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.embedding_dim)\n",
    "        \n",
    "        # add encoding matrix to embedding (x)\n",
    "        sequence_len = x.size(1)\n",
    "        # x = x + torch.autograd.Variable(self.positional_encoding[:,:sequence_len], requires_grad=False)\n",
    "        x = x + self.positional_encoding[:,:sequence_len]\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viendo la clase se puede ver que al inicializar la clase se crea la matriz de positional encoding con la fórmula que propone el paper.\n",
    "\n",
    "Como se explica en los comentarios del código se hace `self.register_buffer('positional_encoding', positional_encoding)` y no `self.positional_encoding = positional_encoding` para que Pytorch tenga en cuenta esa matriz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viendo el método `forward` vemos que lo que primero se hace es multiplicar la matriz de embedding por la raiz cuadrada de la dimensión de nuestro word embedding, esto se hace porque si los valores de la matriz de embedding son pequeños, la perturbación de la matriz de positional encoding va a ser significativa. De modo que se hace a la matriz de embeding mayor para que la perturbación no afecte mucho.\n",
    "\n",
    "Es decir, si volvemos a esta figura\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/word_embedding_3_dimmension.png\" alt=\"word embedding 3 dimmension\" style=\"width:331px;height:233px;\">\n",
    "</div>\n",
    "\n",
    "Supongamos que los ejes de nuestro word embedding van desde -5 hasta 5 (en vez de -1000 a 1000). Al hacer esta multiplicación estamos haciendo que los números de los vectores sean más grandes, es decir, los vectores tendrán números más grandes, pero su posición en el espacio vectorial no cambiará, serán vectores en la misma posición del espacio vectorial, pero todos los vectores más largos. De esta manera al variar sus valores entre -1 y 1 cambiará menos su posición relativa en el espacio vectorial\n",
    "\n",
    "Como veremos más adelante en los mecanismos de atención, el valor absoluto de las matrices no afecta a la hora de calcular la atención"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último se suma el embedding resultante con la matriz de positional encoding. No se suma la matriz de positional encoding entera, sino que se le suma la matriz de positional encoding con tantas filas como tenga `x`.\n",
    "\n",
    "Como hemos dicho para el entrenamiento se necesita definir una longitud máxima de las frases, de manera que las frases que tengan menos tokens se rellenarán con tokens vacíos o sin significado. Pero eso es en el entrenamiento, si hemos definido una longitud máxima de 10 tokens, pero al encoder le metemos 4 tokens, no tiene sentido sumarle la matriz correspondiente a 10 tokens, sino la de 4 tokens. Por eso solo se le suma la matriz con tantas filas (`[:,:sequence_len]`) como tokens tenga `x`.\n",
    "\n",
    "Por último se puede ver que se ha dejado comentada la línea `x = x + torch.autograd.Variable(self.positional_encoding[:,:sequence_len], requires_grad=False)` y que lo que se hace es `x = x + self.positional_encoding[:,:sequence_len]`. La forma comentada es como se tenía que programar en Pytorch hasta la versión 0.4.0, pero a partir de ahí se puede hacer como está en la línea sin comentar. Mi suposición es que de las primeras personas que implementaron un transformer en Pytorch programaban de esa forma, por lo que el resto lo copió sin planteárselo. Por lo que lo dejo comentado porque en internet hay muchos ejemplos con la línea comentada, para que sepas por qué es."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a inicializar un objeto de la clase `PositionalEncoding` y ver su matriz de positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  9.9995e-01,  1.0000e-04,  1.0000e+00],\n",
       "         [ 9.0930e-01,  9.9980e-01,  2.0000e-04,  1.0000e+00],\n",
       "         [ 1.4112e-01,  9.9955e-01,  3.0000e-04,  1.0000e+00],\n",
       "         [-7.5680e-01,  9.9920e-01,  4.0000e-04,  1.0000e+00],\n",
       "         [-9.5892e-01,  9.9875e-01,  5.0000e-04,  1.0000e+00],\n",
       "         [-2.7942e-01,  9.9820e-01,  6.0000e-04,  1.0000e+00],\n",
       "         [ 6.5699e-01,  9.9755e-01,  7.0000e-04,  1.0000e+00],\n",
       "         [ 9.8936e-01,  9.9680e-01,  8.0000e-04,  1.0000e+00],\n",
       "         [ 4.1212e-01,  9.9595e-01,  9.0000e-04,  1.0000e+00]]),\n",
       " torch.Size([10, 4]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 4\n",
    "max_sequence_len = 10\n",
    "positional_encoding = PositionalEncoding(max_sequence_len=max_sequence_len, embedding_model_dim=embedding_dim)\n",
    "positional_encoding.positional_encoding.squeeze(0), positional_encoding.positional_encoding.squeeze(0).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos hemos definido que la máxima longitud de las frases sea 10, la matriz tiene 10 filas, y como hemos definido la longitud el word embedding en 4, la matriz tiene 4 columnas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si creamos una frase de 10 o menos tokens y obtenemos sus embeddings, lo que hará el transformer será sumarle la matriz de positional encoding, por lo que obtendremos una nueva matriz que corresponderá a un nuevo embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver unos pocos ejemplos y luego contamos más cosas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto la capa de embedding nos da unos vectores de los tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of vocabulary\n",
    "            embed_dim: dimension of embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            out: embedding vector\n",
    "        \"\"\"\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.3518,  1.9700,  0.3385, -1.4901],\n",
       "         [-0.2379, -0.3390,  0.3265,  1.5819],\n",
       "         [-0.4283, -1.7501, -0.7696,  1.7848],\n",
       "         [ 0.5877, -0.1465,  0.6528, -0.2571],\n",
       "         [ 0.1525,  0.7716, -0.9592,  0.6064]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 100\n",
    "embedding = Embedding(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "input = torch.LongTensor([[0, 1, 80, 32, 6]])\n",
    "word_embeding = embedding(input)\n",
    "word_embeding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos obtenido los embeddings de los tokens 0, 1, 80, 32 y 6\n",
    "\n",
    "Como vemos, tenemos cinco vectores que no nos dicen nada, no sabemos si puestos en ese orden corresponden a cuatro tokens que crean una frase con lógica o son cuatro tokens, uno detrás del otro, sin ningún sentido."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ahora a obtener los embeddings después de pasar por la capa de positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embeding:\n",
      "[[ 2.351843    1.9699655   0.338458   -1.4900707 ]\n",
      " [-0.23786469 -0.33895883  0.32652166  1.5818858 ]\n",
      " [-0.42827424 -1.7501339  -0.7696138   1.7848046 ]\n",
      " [ 0.5877228  -0.1465103   0.6527555  -0.2571328 ]\n",
      " [ 0.15253736  0.7715723  -0.9592322   0.60638547]]\n",
      "\n",
      "word_encoding:\n",
      "[[ 4.703686    4.939931    0.676916   -1.9801414 ]\n",
      " [ 0.36574158  0.32203233  0.65314335  4.1637716 ]\n",
      " [ 0.05274892 -2.5004678  -1.5390276   4.569609  ]\n",
      " [ 1.3165655   0.70652944  1.305811    0.4857344 ]\n",
      " [-0.45172778  2.5423446  -1.9180645   2.212771  ]]\n"
     ]
    }
   ],
   "source": [
    "word_encoding = positional_encoding(word_embeding)\n",
    "print(f\"word_embeding:\\n{word_embeding.squeeze(0).detach().numpy()}\")\n",
    "print(f\"\\nword_encoding:\\n{word_encoding.squeeze(0).detach().numpy()}\")   # Se añade detach() para que no se muestre el gradiente y se convierte a numpy para que se muestre como array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que se ha modificado la matriz, veamos en cuanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.351843    2.9699655   0.338458   -0.4900707 ]\n",
      " [ 0.6036063   0.6609912   0.32662168  2.5818858 ]\n",
      " [ 0.48102316 -0.7503339  -0.76941377  2.7848046 ]\n",
      " [ 0.72884274  0.85303974  0.65305555  0.74286723]\n",
      " [-0.60426515  1.7707722  -0.95883226  1.6063855 ]]\n"
     ]
    }
   ],
   "source": [
    "print((word_encoding - word_embeding).squeeze(0).detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que hay algún valor mayor que 1 o menor que -1, pero eso es porque internamente el embedding se ha multiplicado por la raiz cuadrada de la dimensión del word emebedding, si vemos la diferencia de la matriz de embedding después de esa operación y después de sumarle la matriz de positional encoding ya no pasará esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      " [ 8.41470957e-01  9.99949992e-01  1.00016594e-04  1.00000000e+00]\n",
      " [ 9.09297407e-01  9.99799967e-01  2.00033188e-04  1.00000000e+00]\n",
      " [ 1.41119957e-01  9.99550045e-01  3.00049782e-04  1.00000000e+00]\n",
      " [-7.56802499e-01  9.99199986e-01  3.99947166e-04  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print((word_encoding - (word_embeding * math.sqrt(embedding_dim))).squeeze(0).detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya no hay ningún valor mayor que 1 o menor que -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto que los valores de la matriz de positional encoding dependen de la posición del token (`pos`), del índice del vector de embedding (`i`) y de la dimensión del word embedding (`d_model`), así que vamos a ver dos matrices de positional encoding don diferentes valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matriz de positional encoding con max len 10 y embedding dim 4:\n",
      " [[ 0.0000000e+00  1.0000000e+00  0.0000000e+00  1.0000000e+00]\n",
      " [ 8.4147096e-01  9.9994999e-01  9.9999997e-05  1.0000000e+00]\n",
      " [ 9.0929741e-01  9.9980003e-01  1.9999999e-04  1.0000000e+00]\n",
      " [ 1.4112000e-01  9.9955004e-01  2.9999999e-04  1.0000000e+00]\n",
      " [-7.5680250e-01  9.9920011e-01  3.9999999e-04  1.0000000e+00]\n",
      " [-9.5892429e-01  9.9875027e-01  4.9999997e-04  1.0000000e+00]\n",
      " [-2.7941549e-01  9.9820054e-01  5.9999997e-04  1.0000000e+00]\n",
      " [ 6.5698659e-01  9.9755102e-01  6.9999992e-04  1.0000000e+00]\n",
      " [ 9.8935825e-01  9.9680173e-01  7.9999992e-04  1.0000000e+00]\n",
      " [ 4.1211849e-01  9.9595273e-01  8.9999987e-04  1.0000000e+00]]\n",
      "\n",
      "matriz de positional encoding con max len 5 y embedding dim 4:\n",
      " [[ 0.0000000e+00  1.0000000e+00  0.0000000e+00  1.0000000e+00]\n",
      " [ 8.4147096e-01  9.9994999e-01  9.9999997e-05  1.0000000e+00]\n",
      " [ 9.0929741e-01  9.9980003e-01  1.9999999e-04  1.0000000e+00]\n",
      " [ 1.4112000e-01  9.9955004e-01  2.9999999e-04  1.0000000e+00]\n",
      " [-7.5680250e-01  9.9920011e-01  3.9999999e-04  1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "max_sequence_len = 10\n",
    "embedding_model_dim = 4\n",
    "positional_encoding = PositionalEncoding(max_sequence_len=max_sequence_len, embedding_model_dim=embedding_model_dim)\n",
    "print(f\"matriz de positional encoding con max len {max_sequence_len} y embedding dim {embedding_model_dim}:\\n {positional_encoding.positional_encoding.squeeze(0).detach().numpy()}\")\n",
    "\n",
    "max_sequence_len = 5\n",
    "embedding_model_dim = 4\n",
    "positional_encoding = PositionalEncoding(max_sequence_len=max_sequence_len, embedding_model_dim=embedding_model_dim)\n",
    "print(f\"\\nmatriz de positional encoding con max len {max_sequence_len} y embedding dim {embedding_model_dim}:\\n {positional_encoding.positional_encoding.squeeze(0).detach().numpy()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambio de la matriz de positional encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando cambia solo el támaño máximo de frase los primeros vectores se mantienen iguales, ya que tanto `pos` como `i` como `d_model` en ellos son iguales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matriz de positional encoding con max len 5 y embedding dim 4:\n",
      " [[ 0.0000000e+00  1.0000000e+00  0.0000000e+00  1.0000000e+00]\n",
      " [ 8.4147096e-01  9.9994999e-01  9.9999997e-05  1.0000000e+00]\n",
      " [ 9.0929741e-01  9.9980003e-01  1.9999999e-04  1.0000000e+00]\n",
      " [ 1.4112000e-01  9.9955004e-01  2.9999999e-04  1.0000000e+00]\n",
      " [-7.5680250e-01  9.9920011e-01  3.9999999e-04  1.0000000e+00]]\n",
      "\n",
      "matriz de positional encoding con max len 5 y embedding dim 6:\n",
      " [[ 0.0000000e+00  1.0000000e+00  0.0000000e+00  1.0000000e+00\n",
      "   0.0000000e+00  1.0000000e+00]\n",
      " [ 8.4147096e-01  9.9892300e-01  2.1544329e-03  1.0000000e+00\n",
      "   4.6415889e-06  1.0000000e+00]\n",
      " [ 9.0929741e-01  9.9569422e-01  4.3088561e-03  1.0000000e+00\n",
      "   9.2831779e-06  1.0000000e+00]\n",
      " [ 1.4112000e-01  9.9032068e-01  6.4632590e-03  9.9999994e-01\n",
      "   1.3924767e-05  1.0000000e+00]\n",
      " [-7.5680250e-01  9.8281395e-01  8.6176321e-03  9.9999994e-01\n",
      "   1.8566356e-05  1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "max_sequence_len = 5\n",
    "embedding_model_dim = 4\n",
    "positional_encoding = PositionalEncoding(max_sequence_len=max_sequence_len, embedding_model_dim=embedding_model_dim)\n",
    "print(f\"matriz de positional encoding con max len {max_sequence_len} y embedding dim {embedding_model_dim}:\\n {positional_encoding.positional_encoding.squeeze(0).detach().numpy()}\")\n",
    "\n",
    "max_sequence_len = 5\n",
    "embedding_model_dim = 6\n",
    "positional_encoding = PositionalEncoding(max_sequence_len=max_sequence_len, embedding_model_dim=embedding_model_dim)\n",
    "print(f\"\\nmatriz de positional encoding con max len {max_sequence_len} y embedding dim {embedding_model_dim}:\\n {positional_encoding.positional_encoding.squeeze(0).detach().numpy()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero cuando cambia el valor de la dimensión del word embedding, solo se menatiene igual el primer vector ya que `d_model` es distinto. El primer vector es igual en ambos casos porque `i = 0`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambio de un token en función de su posición"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear 5 frases distintas, en las que un token se va a mover por todas las posibles posiciones, vamos a ver como su embedding va a ser distinto después de pasar por la capa de positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding del token 21 en la posición 0: [ 0.08386657  0.47746223 -0.68419075 -1.1009847 ]\n",
      "Embedding del token 21 en la posición 1: [ 0.08386657  0.47746223 -0.68419075 -1.1009847 ]\n",
      "Embedding del token 21 en la posición 2: [ 0.08386657  0.47746223 -0.68419075 -1.1009847 ]\n",
      "Embedding del token 21 en la posición 3: [ 0.08386657  0.47746223 -0.68419075 -1.1009847 ]\n",
      "Embedding del token 21 en la posición 4: [ 0.08386657  0.47746223 -0.68419075 -1.1009847 ]\n",
      "\n",
      "Embedding del token 21 en la posición 0 tras positional encoding: [ 0.16773313  1.9549245  -1.3683815  -1.2019694 ]\n",
      "Embedding del token 21 en la posición 1 tras positional encoding: [ 1.0092041  1.9548745 -1.3682815 -1.2019694]\n",
      "Embedding del token 21 en la posición 2 tras positional encoding: [ 1.0770305  1.9547246 -1.3681815 -1.2019694]\n",
      "Embedding del token 21 en la posición 3 tras positional encoding: [ 0.30885315  1.9544744  -1.3680815  -1.2019694 ]\n",
      "Embedding del token 21 en la posición 4 tras positional encoding: [-0.58906937  1.9541246  -1.3679816  -1.2019694 ]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 100\n",
    "embedding_dim = 4\n",
    "max_sequence_len = 5\n",
    "token = 21\n",
    "\n",
    "input1 = torch.LongTensor([[token, 1, 80, 32, 6]])\n",
    "input2 = torch.LongTensor([[0, token, 80, 32, 6]])\n",
    "input3 = torch.LongTensor([[0, 1, token, 32, 6]])\n",
    "input4 = torch.LongTensor([[0, 1, 80, token, 6]])\n",
    "input5 = torch.LongTensor([[0, 1, 80, 32, token]])\n",
    "\n",
    "embedding = Embedding(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "positional_encoding = PositionalEncoding(max_sequence_len=max_sequence_len, embedding_model_dim=embedding_dim)\n",
    "\n",
    "word_embeding1 = embedding(input1)\n",
    "word_embeding2 = embedding(input2)\n",
    "word_embeding3 = embedding(input3)\n",
    "word_embeding4 = embedding(input4)\n",
    "word_embeding5 = embedding(input5)\n",
    "\n",
    "word_encoding1 = positional_encoding(word_embeding1)\n",
    "word_encoding2 = positional_encoding(word_embeding2)\n",
    "word_encoding3 = positional_encoding(word_embeding3)\n",
    "word_encoding4 = positional_encoding(word_embeding4)\n",
    "word_encoding5 = positional_encoding(word_embeding5)\n",
    "\n",
    "print(f\"Embedding del token {token} en la posición 0: {word_embeding1.squeeze(0).detach().numpy()[0]}\")\n",
    "print(f\"Embedding del token {token} en la posición 1: {word_embeding2.squeeze(0).detach().numpy()[1]}\")\n",
    "print(f\"Embedding del token {token} en la posición 2: {word_embeding3.squeeze(0).detach().numpy()[2]}\")\n",
    "print(f\"Embedding del token {token} en la posición 3: {word_embeding4.squeeze(0).detach().numpy()[3]}\")\n",
    "print(f\"Embedding del token {token} en la posición 4: {word_embeding5.squeeze(0).detach().numpy()[4]}\")\n",
    "print()\n",
    "print(f\"Embedding del token {token} en la posición 0 tras positional encoding: {word_encoding1.squeeze(0).detach().numpy()[0]}\")\n",
    "print(f\"Embedding del token {token} en la posición 1 tras positional encoding: {word_encoding2.squeeze(0).detach().numpy()[1]}\")\n",
    "print(f\"Embedding del token {token} en la posición 2 tras positional encoding: {word_encoding3.squeeze(0).detach().numpy()[2]}\")\n",
    "print(f\"Embedding del token {token} en la posición 3 tras positional encoding: {word_encoding4.squeeze(0).detach().numpy()[3]}\")\n",
    "print(f\"Embedding del token {token} en la posición 4 tras positional encoding: {word_encoding5.squeeze(0).detach().numpy()[4]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puee ver el vector de embedding del token 21 es el mismo siempre, pero después de pasar por la capa de positional encoding ya no es igual"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo sabe el transformer la posición de un token tras pasar por la capa de positional encoding?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realmente el transformer, después de la capa de positional encoding, no sabe la posición de cada token. Lo que hace es que en la capa de atención, los tokens que están más cercanos unos de las otros tienen una puntuación mayor que si esos mismos tokens están alejados. Veamos un ejemplo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a contruir una frase con el mismo token, vamos a obtener sus embeddings, vamos a pasarlo por la capa de positional encoding y vamos a ver la similitud de ese token consigo mismo pero en diferentes posiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud entre el token 21 consigo mismo entre las posiciones 0 y 0 es: 1000.0001192092896\n",
      "Similitud entre el token 21 consigo mismo entre las posiciones 0 y 1 es: 999.988317489624\n",
      "Similitud entre el token 21 consigo mismo entre las posiciones 0 y 2 es: 999.9597668647766\n",
      "Similitud entre el token 21 consigo mismo entre las posiciones 0 y 3 es: 999.9275207519531\n",
      "Similitud entre el token 21 consigo mismo entre las posiciones 0 y 4 es: 999.9014735221863\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "vocab_size = 100\n",
    "embedding_dim = 512\n",
    "max_sequence_len = 5\n",
    "token = 21\n",
    "\n",
    "input = torch.LongTensor([[token, token, token, token, token]])\n",
    "\n",
    "embedding = Embedding(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "positional_encoding = PositionalEncoding(max_sequence_len=max_sequence_len, embedding_model_dim=embedding_dim)\n",
    "\n",
    "word_embeding = embedding(input)\n",
    "word_encoding = positional_encoding(word_embeding)\n",
    "\n",
    "similarity_token_position0_position0 = cosine_similarity(word_encoding[:,0,:], word_encoding[:,0,:], dim=1)\n",
    "similarity_token_position0_position1 = cosine_similarity(word_encoding[:,0,:], word_encoding[:,1,:], dim=1)\n",
    "similarity_token_position0_position2 = cosine_similarity(word_encoding[:,0,:], word_encoding[:,2,:], dim=1)\n",
    "similarity_token_position0_position3 = cosine_similarity(word_encoding[:,0,:], word_encoding[:,3,:], dim=1)\n",
    "similarity_token_position0_position4 = cosine_similarity(word_encoding[:,0,:], word_encoding[:,4,:], dim=1)\n",
    "\n",
    "print(f\"Similitud entre el token {token} consigo mismo entre las posiciones 0 y 0 es: {similarity_token_position0_position0.squeeze(0).detach().numpy()*1000}\")\n",
    "print(f\"Similitud entre el token {token} consigo mismo entre las posiciones 0 y 1 es: {similarity_token_position0_position1.squeeze(0).detach().numpy()*1000}\")\n",
    "print(f\"Similitud entre el token {token} consigo mismo entre las posiciones 0 y 2 es: {similarity_token_position0_position2.squeeze(0).detach().numpy()*1000}\")\n",
    "print(f\"Similitud entre el token {token} consigo mismo entre las posiciones 0 y 3 es: {similarity_token_position0_position3.squeeze(0).detach().numpy()*1000}\")\n",
    "print(f\"Similitud entre el token {token} consigo mismo entre las posiciones 0 y 4 es: {similarity_token_position0_position4.squeeze(0).detach().numpy()*1000}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver la similitud entre un mismo token es menor a medida que se separa en una frase. Por lo que gracias al mecanismo de positional encoding, cuando calculemos la relación entre tokens mediante el mecanismo de atención por un lado obtendremos la similitud semantica entre tokens y por otro lado obtendremos cómo de cerca o lejos están en una frase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
