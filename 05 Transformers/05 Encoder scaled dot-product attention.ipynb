{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder scaled dot-product attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos contado el bloque de atención está compuesto por el bloque `Scaled Dot-Prooduct Attention`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/multi-head_attention.png\" alt=\"Multi-Head Attention\" style=\"width:501px;height:623px;\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De modo que vamos a ver cómo es su arquitectura\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention.png\" alt=\"Scaled_Dot-Product_Attention\">\n",
    "</div>\n",
    "\n",
    "Esta arquitectura se define también con la siguiente fórmula\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así en frío puede parecer complicado, pero vamos a ir explicando todo poco a poco para entender cómo funciona este mecanismo de atención, que constituye la parte principal de la arquitectura Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud entre vectores (MatMul)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma que tenemos de calcular la similitud entre dos vectores consiste en calcular el producto escalar entre ambos, pero esto por qué se hace? El producto escalar entre dos vectores U y V se puede descomponer entre el producto de sus normas multiplicado por el coseno del ángulo que los separa\n",
    "\n",
    "$$\\mathbf{U} \\cdot \\mathbf{V} = |\\mathbf{U}| \\cdot |\\mathbf{V}| \\cos(\\theta)$$\n",
    "\n",
    "Por lo que valga lo que valga el producto de sus normas, si el ángulo que los separa es de 0º, su coseno valdrá 1, lo que quiere decir que los dos vectores son similares. Si el ángulo que los separa vale 180º, su coseno valdrá -1, lo que quiere decir que los dos vectores son opuestos o antagónicos. Pero si el ángulo que los separa es de 90º o 270º, su coseno valdrá 0, por lo que los vectores no tendrán ninguna similitud. En el siguiente gif se puede ver una muestra de este comportamiento\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/dot-product-unity.gif\" alt=\"dot product\">\n",
    "</div>\n",
    "\n",
    "Volviendo al producto escalar, en el siguiente gif podemos ver cómo a medida que cambia el ángulo entre los vectores, el producto entre ambos cambia entre el producto de sus normas, cero o el valor negativo del producto de sus normas\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/dot-product.gif\" alt=\"dot product\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya sabemos qué hace el bloque `MatMul` de `Scaled Dot-Product Attention`, ahora vamos a explicar el bloque entero y cómo consigue calcular la atención entre distintos tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a volver a ver la arquitectura y la fórmula del `Scaled dot-product attention`\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention.png\" alt=\"Scaled_Dot-Product_Attention\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "Como vemos tenemos que hacer unas operaciones con `Q`, `K` y `V`. Estos nombres vienen de `query`, `key` y `value` de las bases de datos y hace una especie de similitud de lo que haces en una base de datos para obtener información, pero no vamos a hacer la explicación centrándonos en el paradigma de las bases de datos. Así que solo quédate que tienen esos nombres por esa razón\n",
    "\n",
    "Para saber qué es `Q`, `K` y `V` volvamos a ver la arquitectura general del transformer\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_encoder_multi_head_attention.png\" alt=\"Multi-Head Attention\" style=\"width:425px;height:626px;\">\n",
    "</div>\n",
    "\n",
    "Como vemos la salida del `positional encoding`, se triplica y entra al módulo de `Multi-Head Attention`, por lo que tanto `Q`, `K` y `V` son el conjunto de vectores (es decir, la matriz) correspondientes a la frase que esté entrando al transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a recordar el camino de una frase cuando entra a un transformer\n",
    "\n",
    " 1. Primero entra al tokenizador y cada palabra se divide en sub palabras más sencillas que corresponden a un ID\n",
    " 2. Después entran al `Input embedding` donde son convertidas a un espacio vectorial, por lo que cada token se convertirá en un vector\n",
    " 3. A continuación a cada vector se le sumará otro para dar información de la posición del token en la frase\n",
    " 4. La matriz resultante se triplica y entra al `Multi-Head attention`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Transformers/Slide1.jpg\" alt=\"Multi-Head Attention\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto, tanto `Q`, como `K`, como `V` van a ser la matriz resultante del `input embedding` más el `positional encoding`. A partir de ahora, a esta matriz que se triplica la vamos a llamar `X`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen, la frase de entrada al transformer, se convierte en una matriz, donde cada fila corresponde a uno de los tokens de la frase, esa matriz se triplica y entra al módulo de `Multi-Head Attention`. Por lo que en este caso, esa matriz va a ser `Q`, `K` y `V` y para simplificar la vamos a llamar `X`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MatMul"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que hacemos es la operación `MatMul` de `Q` con `V`, pero que en realidad corresponde a la matriz `X` consigo misma\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_first_MatMul.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "Pero como vemos en la fórmula, para poder realizar la operación `K` tiene que estar transpuesta, veamos por qué\n",
    "\n",
    "La matriz `X` se compone del conjunto de vectores de embeddings de la frase\n",
    "\n",
    "$$X = \\begin{pmatrix}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots\\\\\n",
    "v_m \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Donde `m` es el número de tokens de la frase\n",
    "\n",
    "Cada vector va a tener tantos elementos como las dimensiones de nuestro embedding, supongamos que es `n`, por tanto\n",
    "\n",
    "$$X = \\begin{pmatrix}\n",
    "v_{1,1} & v_{1,2} & \\cdots & v_{1,n} \\\\\n",
    "v_{2,1} & v_{2,2} & \\cdots & v_{2,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{m,1} & v_{m,2} & \\cdots & v_{m,n} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Así que para poder multiplicar `X` consigo misma necesitamos que esté transpuesta\n",
    "\n",
    "$$X \\cdot X^T = \\begin{pmatrix}\n",
    "v_{1,1} & v_{1,2} & \\cdots & v_{1,n} \\\\\n",
    "v_{2,1} & v_{2,2} & \\cdots & v_{2,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{m,1} & v_{m,2} & \\cdots & v_{m,n} \\\\\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "v_{1,1} & v_{1,2} & \\cdots & v_{1,m} \\\\\n",
    "v_{2,1} & v_{2,2} & \\cdots & v_{2,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{n,1} & v_{n,2} & \\cdots & v_{n,m} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Para que así la multiplicación sea una multiplicación de matrices de dimensiones $\\left(m \\times n\\right) \\cdot \\left(n \\times m\\right)$ que dará como resultado una matriz de tamaño $\\left(m \\times m\\right)$ donde `m` era el número de tokens de la frase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos explicado por qué se tiene que multiplicar la matriz por ella misma transpuesta vamos a ver qué supone esto, si desarrollasemos la multiplicación matricial de antes llegaríamos a esto\n",
    "\n",
    "$$X \\cdot X^T = \\begin{pmatrix}\n",
    "v_1 \\cdot v_1 & v_1 \\cdot v_2 & \\cdots & v_1 \\cdot v_m \\\\\n",
    "v_2 \\cdot v_1 & v_2 \\cdot v_2 & \\cdots & v_2 \\cdot v_m \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_m \\cdot v_1 & v_m \\cdot v_2 & \\cdots & v_m \\cdot v_m \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Es decir, tendríamos una matriz con los productos escalares de los vectores de cada token de la frase, y como hemos explicado antes, el producto escalar entre dos vectores nos da la similitud entre estos. Por lo que hemos conseguido una matriz que nos da la similitud de cada token con el resto de tokens de la frase. Obviamente en la diagonal vamos a tener que la similitud va a ser el máximo, ya que se calcula la similitud de un token consigo mismo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver un ejemplo con dos frases en la que se le da un hueso a un perro\n",
    "\n",
    " 1. `I gave the dog a bone because it was hungry`\n",
    " 2. `I gave the dog a bone because it was old`\n",
    "\n",
    "En el primera frase el `it` se refiere al perro que tiene hambre, mientras que en la segunda el `it` se refiere al hueso que está viejo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I gave the dog a bone because it was hungry\"\n",
    "sentence2 = \"I gave the dog a bone because it was old\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens 1: ['I', 'gave', 'the', 'dog', 'a', 'bone', 'because', 'it', 'was', 'hung', '##ry']\n",
      "Tokens 2: ['I', 'gave', 'the', 'dog', 'a', 'bone', 'because', 'it', 'was', 'old']\n"
     ]
    }
   ],
   "source": [
    "tokens1 = tokenizer.tokenize(sentence1)\n",
    "tokens2 = tokenizer.tokenize(sentence2)\n",
    "\n",
    "print(f\"Tokens 1: {tokens1}\")\n",
    "print(f\"Tokens 2: {tokens2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens ID 1: [146, 15362, 10105, 17835, 169, 57254, 12373, 10271, 10134, 68971, 10908]\n",
      "Tokens ID 2: [146, 15362, 10105, 17835, 169, 57254, 12373, 10271, 10134, 12898]\n"
     ]
    }
   ],
   "source": [
    "tokens_id_1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
    "tokens_id_2 = tokenizer.convert_tokens_to_ids(tokens2)\n",
    "\n",
    "print(f\"Tokens ID 1: {tokens_id_1}\")\n",
    "print(f\"Tokens ID 2: {tokens_id_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_1 = model(torch.tensor([tokens_id_1]))\n",
    "word_embedding_2 = model(torch.tensor([tokens_id_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_embedding_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atención entre tokens con y sin el efecto del positional encoding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
