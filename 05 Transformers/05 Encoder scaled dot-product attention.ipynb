{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder scaled dot-product attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos contado el bloque de atención está compuesto por el bloque `Scaled Dot-Prooduct Attention`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/multi-head_attention.png\" alt=\"Multi-Head Attention\" style=\"width:501px;height:623px;\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De modo que vamos a ver cómo es su arquitectura\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention.png\" alt=\"Scaled_Dot-Product_Attention\">\n",
    "</div>\n",
    "\n",
    "Esta arquitectura se define también con la siguiente fórmula\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así en frío puede parecer complicado, pero vamos a ir explicando todo poco a poco para entender cómo funciona este mecanismo de atención, que constituye la parte principal de la arquitectura Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud entre vectores (MatMul)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma que tenemos de calcular la similitud entre dos vectores consiste en calcular el producto escalar entre ambos, pero esto por qué se hace? El producto escalar entre dos vectores U y V se puede descomponer entre el producto de sus normas multiplicado por el coseno del ángulo que los separa\n",
    "\n",
    "$$\\mathbf{U} \\cdot \\mathbf{V} = |\\mathbf{U}| \\cdot |\\mathbf{V}| \\cos(\\theta)$$\n",
    "\n",
    "Por lo que valga lo que valga el producto de sus normas, si el ángulo que los separa es de 0º, su coseno valdrá 1, lo que quiere decir que los dos vectores son similares. Si el ángulo que los separa vale 180º, su coseno valdrá -1, lo que quiere decir que los dos vectores son opuestos o antagónicos. Pero si el ángulo que los separa es de 90º o 270º, su coseno valdrá 0, por lo que los vectores no tendrán ninguna similitud. En el siguiente gif se puede ver una muestra de este comportamiento\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/dot-product-unity.gif\" alt=\"dot product\">\n",
    "</div>\n",
    "\n",
    "Volviendo al producto escalar, en el siguiente gif podemos ver cómo a medida que cambia el ángulo entre los vectores, el producto entre ambos cambia entre el producto de sus normas, cero o el valor negativo del producto de sus normas\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/dot-product.gif\" alt=\"dot product\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya sabemos qué hace el bloque `MatMul` de `Scaled Dot-Product Attention`, ahora vamos a explicar el bloque entero y cómo consigue calcular la atención entre distintos tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atención entre tokens con y sin el efecto del positional encoding"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
