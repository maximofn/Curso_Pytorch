{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder multi-head attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya hemos vísto cómo funciona el `Scaled Dot-Product Attention`, por lo que ya podemos abordar el `Multi-Head Attention` del encoder\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_encoder_multi_head_attention.png\" alt=\"Multi-Head Attention\" style=\"width:425px;height:626px;\">\n",
    "  <img src=\"Imagenes/multi-head_attention.png\" alt=\"Multi-Head Attention\" style=\"width:501px;height:623px;\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viendo la arquitenctura, podemos ver que `K`, `Q` y `V` pasan por un bloque `Linear` y además vemo que tanto los bloques `Linear` y el bloque `Scaled Dot-Product Attention` se repite varias veces, vamos a ver por qué se hace esto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar hay que explicar qué es el bloque `Linear` que se ve en la arquitectura, pues es simplemente una capa fully connected de las que explicamos al principio del todo, y ahora veremos por qué"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyecciones de las entradas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/multi-head_attention_proyection.png\" alt=\"Proyection Multi-Head Attention\" style=\"width:501px;height:623px;\">\n",
    "</div>\n",
    "\n",
    "Como hemos explicado, en la capa de `Input Embedding` se convierte el token a un vector de `n` dimensiones. De manera que por ejemplo puede que una dimensión determine si una palabra es un objeto inanimado o un ser vivo, otra podría capturar si es un objeto tangible o un concepto abstracto, otra podría capturar si la palabra tiene connotaciones positivas o negativas, mientras que otra podría capturar si una palabra es un sustantivo, verbo, adjetivo, etc., otra dimensión podría capturar si la palabra está en singular o plural y otra dimensión podría capturar el tiempo verbal (presente, pasado, futuro). Las tres primeras dimensiones capturan información de la semántica de la palabra, mientras que las tres últimas capturan información de la sintaxis\n",
    "\n",
    "Por lo que a la hora de introducir las matrices `K`, `Q` y `V` al `Scaled Dot-Product Attention` a lo mejor es mejor que entren juntas las tres primeras dimensiones y por otro lado las tres últimas\n",
    "\n",
    "Sin embargo, no sabemos exactamente cómo el input embedding construye las dimensiones. En nuestro caso de momento estamos cogiendo el `input embedding` ya entrenado de BERT, por lo que podríamos hacer un estudio, pero son 768 dimensiones, por lo que sería mucho trabajo hacer el estudio de cada dimensión y luego buscar relaciones entre las dimensiones. Pero es que además, en la realidad, el `input embedding` no está entrenado, va cambiando sus pesos durante el entrenamiento, por lo que ni podríamos hacer ese estudio.\n",
    "\n",
    "Por lo que aquí entran las capas `Linear` que se ven en la arquitectura, nosotros decidimos en cuantos grupos vamos a dividir el embedding, esto lo determina la `h` que se ve a la derecha del `Scaled Dot-Product Attention`. Y esas capas `Linear`, simplemente serán matrices que se quedarán con unas dimensiones u otras, incluso con parte de unas o de otras, puede ser una mezcla. Pero lo importante, es que nosotros no decidimos qué dimeniosnes van con otras, sino que se hace automáticamente durante el entrenamiento del Transformer, de manera que se obtenga el mejor resultado posible"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/multi-head_attention_concat.png\" alt=\"Concat Multi-Head Attention\" style=\"width:501px;height:623px;\">\n",
    "</div>\n",
    "\n",
    "Si en las proyecciones se divide el embedding en pequeñas dimensiones, ahora habrá que juntar toda esa información nuevamente, por eso se hace una concatenación de todas las mtrices resultantes del `Scaled Dot-Product Attencion` en la capa de `Concat`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/multi-head_attention_linear.png\" alt=\"Linear Multi-Head Attention\" style=\"width:501px;height:623px;\">\n",
    "</div>\n",
    "\n",
    "Por último se pasa todo por un bloque `Linear`, es decir, por una red fully connected. Esto es porque al concatenar las salidas en el paso anterior se obtiene una matriz mucho más grande. Supongamos una frase con 12 tokens y en la que se han obtenido los embeddings con el `input embedding` de BERT, que tiene una dimensión de 768. Por lo que deberíamos tener una matriz de 12x768. Si hacemos 6 proyecciones, es decir, si ponemos a la `h` que sale a la derecha del `Scaled Dot-Product Attention` un valor de 6, vamos a tener 6 matrices de 12x768. Si las concatenamos vamos a tener una matriz de 72x768, pero a la salida del `Multi-Head Attention` deberíamos tener una matriz de 12x768. Por lo que pasamos esta matriz por una red fully connected para a la salida volver a tener una matriz de 12x768"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a implementar la clase `Multi-Head Attention` con Pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero volvemos a poner el código de la clase `Scaled Dot-Product Attention` que hemos hecho en el notebook anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "    \n",
    "    def forward(self, key, query, value):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            key: key vector\n",
    "            query: query vector\n",
    "            value: value vector\n",
    "        \n",
    "        Returns:\n",
    "            output vector from scaled dot product attention\n",
    "        \"\"\"\n",
    "        # MatMul\n",
    "        key_trasposed = key.transpose(-1,-2)\n",
    "        product = torch.matmul(query, key_trasposed)\n",
    "        # scale\n",
    "        product = product / math.sqrt(self.dim_embedding)\n",
    "        # softmax\n",
    "        attention_matrix = torch.nn.functional.softmax(product, dim=-1)\n",
    "        # MatMul\n",
    "        output = torch.matmul(attention_matrix, value)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora creamos la clase del `Multi-Head Attention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.dim_proyection = dim_embedding // heads\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.proyection_Q = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.proyection_K = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.proyection_V = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.attention = nn.Linear(dim_embedding, dim_embedding)\n",
    "\n",
    "        self.scaled_dot_product_attention = ScaledDotProductAttention(self.dim_proyection)\n",
    "    \n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        proyection_Q = self.proyection_Q(Q).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_K = self.proyection_K(K).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_V = self.proyection_V(V).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "        proyection_Q = proyection_Q.transpose(1,2)\n",
    "        proyection_K = proyection_K.transpose(1,2)\n",
    "        proyection_V = proyection_V.transpose(1,2)\n",
    "\n",
    "        # calculate attention using function we will define next\n",
    "        outputs = []\n",
    "        for i in range(self.heads):\n",
    "            output = self.scaled_dot_product_attention(proyection_Q[:, i, :, :], proyection_K[:, i, :, :], proyection_V[:, i, :, :])\n",
    "            outputs.append(output)\n",
    "        scaled_dot_product_attention = torch.stack(outputs, dim=2)  # stacking along the head dimension\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scaled_dot_product_attention.transpose(1,2).contiguous().view(batch_size, -1, self.dim_embedding)\n",
    "        \n",
    "        output = self.attention(concat)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a obtener el embedding de una frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "def extract_embeddings(input_sentence, model_name='bert-base-uncased'):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "    \n",
    "    input_ids = tokenizer.encode(input_sentence, add_special_tokens=True)\n",
    "    input_ids_tensor = torch.tensor([input_ids])\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids_tensor)\n",
    "        \n",
    "    token_embeddings = outputs[0][0]\n",
    "    \n",
    "    # Los embeddings posicionales están en la segunda capa de los embeddings de la arquitectura BERT\n",
    "    positional_encodings = model.embeddings.position_embeddings.weight[:len(input_ids), :].detach()\n",
    "    \n",
    "    embeddings_with_positional_encoding = token_embeddings + positional_encodings\n",
    "    \n",
    "    return tokens, input_ids, token_embeddings, positional_encodings, embeddings_with_positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"I gave the dog a bone because it was hungry\"\n",
    "tokens1, input_ids1, token_embeddings1, positional_encodings1, embeddings_with_positional_encoding1 = extract_embeddings(sentence1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos un objeto de la clase `Multi-Head Attention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_embedding = embeddings_with_positional_encoding1.shape[-1]\n",
    "heads = 8\n",
    "multi_head_attention = MultiHeadAttention(heads=heads, dim_embedding=dim_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = multi_head_attention(embeddings_with_positional_encoding1, embeddings_with_positional_encoding1, embeddings_with_positional_encoding1)\n",
    "attention.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la salida obtenemos una matriz de 12x1x768 en vez de 12x768 por la línea `concat = scaled_dot_product_attention.transpose(1,2).contiguous().view(batch_size, -1, self.dim_embedding)`, para obtener 12x768 debería ser `concat = scaled_dot_product_attention.transpose(1,2).contiguous().view(batch_size, self.dim_embedding)`, es decir, quitar el `-1`. Sin embargo lo hemos dejado así porque luego nos será útil para el entrenamiento, para poder meter un dataloader con varias sentencias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
