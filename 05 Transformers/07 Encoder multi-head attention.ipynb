{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder multi-head attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya hemos vísto cómo funciona el `Scaled Dot-Product Attention`, por lo que ya podemos abordar el `Multi-Head Attention` del encoder\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_encoder_multi_head_attention.png\" alt=\"Multi-Head Attention\" style=\"width:425px;height:626px;\">\n",
    "  <img src=\"Imagenes/multi-head_attention.png\" alt=\"Multi-Head Attention\" style=\"width:501px;height:623px;\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viendo la arquitectura, podemos ver que `K`, `Q` y `V` pasan por un bloque `Linear` y además vemos que tanto los bloques `Linear` y el bloque `Scaled Dot-Product Attention` se repite varias veces, vamos a ver por qué se hace esto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar hay que explicar qué es el bloque `Linear` que se ve en la arquitectura, pues es simplemente una capa fully connected de las que explicamos al principio del todo, y ahora veremos por qué"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyecciones de las entradas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/multi-head_attention_proyection.png\" alt=\"Proyection Multi-Head Attention\" style=\"width:501px;height:623px;\">\n",
    "</div>\n",
    "\n",
    "Como hemos explicado, en la capa de `Input Embedding` se convierte el token a un vector de `n` dimensiones. De manera que por ejemplo puede que una dimensión determine si una palabra es un objeto inanimado o un ser vivo, otra podría capturar si es un objeto tangible o un concepto abstracto, otra podría capturar si la palabra tiene connotaciones positivas o negativas, mientras que otra podría capturar si una palabra es un sustantivo, verbo, adjetivo, etc., otra dimensión podría capturar si la palabra está en singular o plural y otra dimensión podría capturar el tiempo verbal (presente, pasado, futuro). Las tres primeras dimensiones capturan información de la semántica de la palabra, mientras que las tres últimas capturan información de la sintaxis\n",
    "\n",
    "Por lo que a la hora de introducir las matrices `K`, `Q` y `V` al `Scaled Dot-Product Attention` a lo mejor es mejor que entren juntas las tres primeras dimensiones y por otro lado las tres últimas\n",
    "\n",
    "Sin embargo, no sabemos exactamente cómo el input embedding construye las dimensiones. En nuestro caso de momento estamos cogiendo el `input embedding` ya entrenado de BERT, por lo que podríamos hacer un estudio, pero son 768 dimensiones, así que sería mucho trabajo hacer el estudio de cada dimensión y luego buscar relaciones entre las dimensiones. Pero es que además, en la realidad, el `input embedding` no está entrenado, va cambiando sus pesos durante el entrenamiento, por lo que ni podríamos hacer ese estudio.\n",
    "\n",
    "Por lo que aquí entran las capas `Linear` que se ven en la arquitectura, nosotros decidimos en cuantos grupos vamos a dividir el embedding, esto lo determina la `h` que se ve a la derecha del `Scaled Dot-Product Attention`. Y esas capas `Linear`, simplemente serán matrices que se quedarán con unas dimensiones u otras, incluso con parte de unas o de otras, puede ser una mezcla. Pero lo importante, es que nosotros no decidimos qué dimensiones van con otras, sino que se hace automáticamente durante el entrenamiento del Transformer, de manera que se obtenga el mejor resultado posible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se han dividido los embeddings en `h` grupos, se pasa por el `Scaled Dot-Product Attention` cada uno de los grupos, y se obtiene un resultado para cada uno de los grupos. Por lo que se obtienen `h` resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/multi-head_attention_concat.png\" alt=\"Concat Multi-Head Attention\" style=\"width:501px;height:623px;\">\n",
    "</div>\n",
    "\n",
    "Si en las proyecciones se divide el embedding en pequeñas dimensiones, ahora habrá que juntar toda esa información nuevamente, por eso se hace una concatenación de todas las matrices resultantes del `Scaled Dot-Product Attencion` en la capa de `Concat`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/multi-head_attention_linear.png\" alt=\"Linear Multi-Head Attention\" style=\"width:501px;height:623px;\">\n",
    "</div>\n",
    "\n",
    "Por último se pasa todo por un bloque `Linear`, es decir, por una red fully connected.\n",
    "\n",
    "Esto es porque los resultados de los `h` grupos se han concatenado uno detrás de otro, pero esa organización de los embeddings resultantes no tiene por qué ser la menjor, por lo que nuevamente se vuelve a pasar por una red fully connected para que se organice de la mejor manera posible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a implementar la clase `Multi-Head Attention` con Pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero volvemos a poner el código de la clase `Scaled Dot-Product Attention` que hemos hecho en el notebook anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "    \n",
    "    def forward(self, key, query, value):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            key: key vector\n",
    "            query: query vector\n",
    "            value: value vector\n",
    "        \n",
    "        Returns:\n",
    "            output vector from scaled dot product attention\n",
    "        \"\"\"\n",
    "        # MatMul\n",
    "        key_trasposed = key.transpose(-1,-2)\n",
    "        product = torch.matmul(query, key_trasposed)\n",
    "        # scale\n",
    "        scale = product / torch.sqrt(torch.tensor(self.dim_embedding))\n",
    "        # softmax\n",
    "        attention_matrix = torch.softmax(scale, dim=-1)\n",
    "        # MatMul\n",
    "        output = torch.matmul(attention_matrix, value)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora creamos la clase del `Multi-Head Attention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            heads: number of heads\n",
    "            dim_embedding: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.dim_proyection = dim_embedding // heads\n",
    "        self.heads = heads\n",
    "\n",
    "        # Aquí lo ideal sería crear tantas capas lineales como cabezas h haya, es decir, algo así:\n",
    "        #   self.proyection_Q = [nn.Linear(dim_embedding, self.dim_proyection) for _ in range(heads)]\n",
    "        #   self.proyection_K = [nn.Linear(dim_embedding, self.dim_proyection) for _ in range(heads)]\n",
    "        #   self.proyection_V = [nn.Linear(dim_embedding, self.dim_proyection) for _ in range(heads)]\n",
    "        # Aquí hemos creado h capas lineales a las que le entra la matriz de embedding (dim_embedding) y sale una matriz de dimensión dim_proyection\n",
    "        # Sin embargo computacionalmente es lo mismo que hacer una única capa linear que le entre la matriz de embedding y salga una matriz de dimensión dim_embedding\n",
    "        # porque internamente, la capa lineal va a hacer las combinaciones de dimensiones necesarias para que se junten las dimensiones del embedding que hagan\n",
    "        # el entrenamiento más óptimo\n",
    "        self.proyection_Q = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.proyection_K = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.proyection_V = nn.Linear(dim_embedding, dim_embedding)\n",
    "\n",
    "        self.scaled_dot_product_attention = ScaledDotProductAttention(self.dim_proyection)\n",
    "        self.attention = nn.Linear(dim_embedding, dim_embedding)\n",
    "    \n",
    "    def forward(self, Q, K, V):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q: query vector\n",
    "            K: key vector\n",
    "            V: value vector\n",
    "\n",
    "        Returns:\n",
    "            output vector from multi-head attention\n",
    "        \"\"\"\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        proyection_Q = self.proyection_Q(Q).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_K = self.proyection_K(K).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_V = self.proyection_V(V).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "        proyection_Q = proyection_Q.transpose(1,2)\n",
    "        proyection_K = proyection_K.transpose(1,2)\n",
    "        proyection_V = proyection_V.transpose(1,2)\n",
    "\n",
    "        # calculate attention\n",
    "        scaled_dot_product_attention = self.scaled_dot_product_attention(proyection_Q, proyection_K, proyection_V)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scaled_dot_product_attention.transpose(1,2).contiguous().view(batch_size, -1, self.dim_embedding)\n",
    "\n",
    "        # Final linear        \n",
    "        output = self.attention(concat)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a obtener el embedding de una frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "def extract_embeddings(input_sentences, model_name='bert-base-uncased'):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "    \n",
    "    # tokenización de lote\n",
    "    inputs = tokenizer(input_sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    token_embeddings = outputs[0]\n",
    "    \n",
    "    # Los embeddings posicionales están en la segunda capa de los embeddings de la arquitectura BERT\n",
    "    positional_encodings = model.embeddings.position_embeddings.weight[:token_embeddings.shape[1], :].detach().unsqueeze(0).repeat(token_embeddings.shape[0], 1, 1)\n",
    "\n",
    "    embeddings_with_positional_encoding = token_embeddings + positional_encodings\n",
    "\n",
    "    # convierte las IDs de los tokens a tokens\n",
    "    tokens = [tokenizer.convert_ids_to_tokens(input_id) for input_id in inputs['input_ids']]\n",
    "\n",
    "    return tokens, inputs['input_ids'], token_embeddings, positional_encodings, embeddings_with_positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I gave the dog a bone because it was hungry\"\n",
    "tokens1, input_ids1, token_embeddings1, positional_encodings1, embeddings_with_positional_encoding1 = extract_embeddings(sentence1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos un objeto de la clase `Multi-Head Attention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_embedding = embeddings_with_positional_encoding1.shape[-1]\n",
    "heads = 8\n",
    "multi_head_attention = MultiHeadAttention(heads=heads, dim_embedding=dim_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = multi_head_attention(embeddings_with_positional_encoding1, embeddings_with_positional_encoding1, embeddings_with_positional_encoding1)\n",
    "attention.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
