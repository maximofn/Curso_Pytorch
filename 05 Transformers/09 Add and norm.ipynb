{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add and norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_add_and_norm2.png\" alt=\"Add and norm\" style=\"width:425px;height:626px;\">\n",
    "</div>\n",
    "\n",
    "Por último antes de terminar el encoder falta pasar por la última capa de `Add & Norm`, como ya la hemos explicado antes no vamos a contar nada nuevo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya hemos hecho la clase antes, por lo que ahora vamos a ver la salida después del último `Add & Norm` del encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a coger el embbeding preentrenado de BERT, las implementaciones anteriores que hemos hecho antes y vamos a ver qué obtenemos a la salida de nuestra clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "    \n",
    "    def forward(self, key, query, value):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            key: key vector\n",
    "            query: query vector\n",
    "            value: value vector\n",
    "        \n",
    "        Returns:\n",
    "            output vector from scaled dot product attention\n",
    "        \"\"\"\n",
    "        # MatMul\n",
    "        key_trasposed = key.transpose(-1,-2)\n",
    "        product = torch.matmul(query, key_trasposed)\n",
    "        # scale\n",
    "        product = product / math.sqrt(self.dim_embedding)\n",
    "        # softmax\n",
    "        attention_matrix = torch.nn.functional.softmax(product, dim=-1)\n",
    "        # MatMul\n",
    "        output = torch.matmul(attention_matrix, value)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.dim_proyection = dim_embedding // heads\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.proyection_Q = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.proyection_K = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.proyection_V = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.attention = nn.Linear(dim_embedding, dim_embedding)\n",
    "\n",
    "        self.scaled_dot_product_attention = ScaledDotProductAttention(self.dim_proyection)\n",
    "    \n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        proyection_Q = self.proyection_Q(Q).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_K = self.proyection_K(K).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_V = self.proyection_V(V).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "        proyection_Q = proyection_Q.transpose(1,2)\n",
    "        proyection_K = proyection_K.transpose(1,2)\n",
    "        proyection_V = proyection_V.transpose(1,2)\n",
    "\n",
    "        # calculate attention using function we will define next\n",
    "        outputs = []\n",
    "        for i in range(self.heads):\n",
    "            output = self.scaled_dot_product_attention(proyection_Q[:, i, :, :], proyection_K[:, i, :, :], proyection_V[:, i, :, :])\n",
    "            outputs.append(output)\n",
    "        scaled_dot_product_attention = torch.stack(outputs, dim=2)  # stacking along the head dimension\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scaled_dot_product_attention.transpose(1,2).contiguous().view(batch_size, self.dim_embedding)\n",
    "        \n",
    "        output = self.attention(concat)\n",
    "    \n",
    "        return output\n",
    "\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding (int): Embedding dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.normalization = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            sublayer (torch.Tensor): Sublayer tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return self.normalization(torch.add(x, sublayer))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_embedding, increment=4):\n",
    "        super().__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim_embedding, dim_embedding*increment),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_embedding*increment, dim_embedding)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch_size, seq_len, dim_embedding)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch_size, seq_len, dim_embedding)\n",
    "        \"\"\"\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "\n",
    "def extract_embeddings(input_sentence, model_name='bert-base-uncased'):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "    \n",
    "    input_ids = tokenizer.encode(input_sentence, add_special_tokens=True)\n",
    "    input_ids_tensor = torch.tensor([input_ids])\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids_tensor)\n",
    "        \n",
    "    token_embeddings = outputs[0][0]\n",
    "    \n",
    "    # Los embeddings posicionales están en la segunda capa de los embeddings de la arquitectura BERT\n",
    "    positional_encodings = model.embeddings.position_embeddings.weight[:len(input_ids), :].detach()\n",
    "    \n",
    "    embeddings_with_positional_encoding = token_embeddings + positional_encodings\n",
    "    \n",
    "    return tokens, input_ids, token_embeddings, positional_encodings, embeddings_with_positional_encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos el resultado del input embedding más el positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"I gave the dog a bone because it was hungry\"\n",
    "tokens1, input_ids1, token_embeddings1, positional_encodings1, embeddings_with_positional_encoding1 = extract_embeddings(sentence1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos un objeto de la clase `Multi-Head Attention` y obtenemos su salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_embedding = embeddings_with_positional_encoding1.shape[-1]\n",
    "heads = 8\n",
    "multi_head_attention = MultiHeadAttention(heads=heads, dim_embedding=dim_embedding)\n",
    "attention = multi_head_attention(embeddings_with_positional_encoding1, embeddings_with_positional_encoding1, embeddings_with_positional_encoding1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora instanciamos un objeto de la clase `Add & Norm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_and_norm1 = AddAndNorm(dim_embedding=dim_embedding)\n",
    "add_and_norm_output1 = add_and_norm1(embeddings_with_positional_encoding1, attention)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos un objeto de la clase `Feed Forward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward = FeedForward(dim_embedding=dim_embedding)\n",
    "feed_forward_output = feed_forward(add_and_norm_output1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por último instanciamos un objeto del segundo `Add & Norm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_and_norm2 = AddAndNorm(dim_embedding=dim_embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metemos la salida del anterior `Feed Forward` al último `Add & Norm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_and_norm_output2 = add_and_norm2(add_and_norm_output1, feed_forward_output)\n",
    "add_and_norm_output2.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
