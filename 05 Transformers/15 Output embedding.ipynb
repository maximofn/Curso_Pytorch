{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que hicimos en la parte del encoder, hay que representar cada token de entrada en un espacio vectorial, por lo que se pasa el token por un bloque de `Output Embedding`.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_output_embedding.png\" alt=\"Output embedding\" style=\"width:425px;height:626px;\">\n",
    "</div>\n",
    "\n",
    "Al hacerlo mediante `word embedding` vimos que teníamos la ventaja de que cada token se representaba en una zona del espacio vectorial con tokens que tenían un significado parecido\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/word_embedding_2_dimmension.png\" alt=\"word embedding 2 dimmension\" style=\"width:662px;height:467px;\">\n",
    "  <img src=\"Imagenes/word_embedding_3_dimmension.png\" alt=\"word embedding 3 dimmension\" style=\"width:662px;height:467px;\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar el `Output Embedding` reutilizamos la clase que creamos en el `Input Embedding`. La clase será la misma, pero a la hora de crear el transformer, crearemos dos objetos distintos de la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of vocabulary\n",
    "            embed_dim: dimension of embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            out: embedding vector\n",
    "        \"\"\"\n",
    "        return self.embedding(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
