{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder masked multi-head attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a la parte principal del transformer, el segundo módulo de atención, pero este tiene la diferencia de que es `Masked`.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_decoder_masked_multi_head_attention.png\" alt=\"Multi-Head Attention\" style=\"width:425px;height:626px;\">\n",
    "</div>\n",
    "\n",
    "¿Recuerdas cuando explicamos el `Scaled Dot-Product Attention` no explicamos la parte de la máscara porque no se aplicaba? Pues ahora sí, por lo que ahora lo explicaremos\n",
    "\n",
    "Con la ventaja de que ya entendemos el resto de operaciones y ahora solo hay que añadir un pequeño bloque más"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que como hicimos antes, vamos a volver a explicar el `Scaled Dot-Product Attention` ya que es la parte principal del bloque de atención\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/multi-head_attention.png\" alt=\"Multi-Head Attention\" style=\"width:501px;height:623px;\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
