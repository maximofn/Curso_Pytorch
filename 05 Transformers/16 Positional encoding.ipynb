{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya vimos, en el lenguaje es muy importante la posición de las palabras en la frase, ya que `Juan quiere a María` no significa lo mismo que `María quiere a Juan`.\n",
    "\n",
    "Pero como el transformer veía la secuencia entera y creaba las relaciones mediante los mecanismos de atencíon, no tenía manera de saber la posición de cada token en la frase.\n",
    "\n",
    "Por lo que mediante la adición de un número pequeño (entre -1 y 1) podíamos dar la información de posición en la frase a esos mecanismos de atención\n",
    "\n",
    "Por tanto, en el decoder es necesario también un `Positional Encoding`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_decoder_positional_embedding.png\" alt=\"Decoder positional encoding\" style=\"width:425px;height:626px;\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta información se da medainte las ecuaciones\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/positional_encoding.png\" alt=\"positional encoding\">\n",
    "</div>\n",
    "\n",
    "Que modifican el valor de los embeddings entre -1 y 1, lo cual no cambiaba mucho la posición del embedding dentro del espacio vectorial\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/word_embedding_3_dimmension.png\" alt=\"word embedding 3 dimmension\" style=\"width:662px;height:467px;\">\n",
    "</div>\n",
    "\n",
    "Porque como se puede ver en la figura anterior, los embeddings tienen valores muy grandes, por lo que esta pequeña modificación, no los perturbará mucho"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí también reusamos la clase que creamos en el encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_sequence_len, embedding_model_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq_len: length of input sequence\n",
    "            embed_model_dim: demension of embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_model_dim\n",
    "\n",
    "        # create constant 'positional_encoding' matrix with values dependant on pos and i\n",
    "        positional_encoding = torch.zeros(max_sequence_len, self.embedding_dim)\n",
    "        for pos in range(max_sequence_len):\n",
    "            for i in range(0, self.embedding_dim, 2):\n",
    "                positional_encoding[pos, i]     = math.sin(pos / (10000 ** ((2 *     i) / self.embedding_dim)))\n",
    "                positional_encoding[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i+1)) / self.embedding_dim)))\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "        # Esta última línea es equivalente a hacer self.positional_encoding = positional_encoding\n",
    "        # sin embargo al hacerlo así PyTorch no lo considerará parte del estado del modelo.\n",
    "        # Hay varias implicaciones de esto:\n",
    "        #  * Movimiento de dispositivos: Cuando se mueve el modelo a la GPU con model.to(device), \n",
    "        #    PyTorch automáticamente moverá todos los parámetros y buffers registrados al dispositivo especificado.\n",
    "        #    Sin embargo, no moverá los tensores que no sean parámetros o buffers registrados. Por lo tanto, \n",
    "        #    si se asigna positional_encoding a self.positional_encoding directamente, habría que moverlo \n",
    "        #    manualmente a la GPU.\n",
    "        #  * Serialización: Cuando se guarda el modelo con torch.save(model.state_dict(), PATH), PyTorch guardará \n",
    "        #    todos los parámetros y buffers registrados del modelo. Pero no guardará tensores que no son parámetros \n",
    "        #    o buffers registrados. Por lo tanto, si se asigna positional_encoding a self.positional_encoding \n",
    "        #    directamente, no se guardará cuando se guarde el estado del modelo.\n",
    "        #  * Modo de evaluación: Algunos métodos, como model.eval(), cambian el comportamiento de ciertas capas del \n",
    "        #    modelo (como Dropout o BatchNorm) dependiendo de si el modelo está en modo de entrenamiento o evaluación.\n",
    "        #    Para que estas capas funcionen correctamente, PyTorch necesita conocer su estado actual, que se almacena\n",
    "        #    en sus parámetros y buffers registrados. Si positional_encoding no está registrado como un buffer, \n",
    "        #    entonces no será afectado por el cambio de modo.\n",
    "        # En resumen, si no se usa register_buffer para positional_encoding, se tendrían que manejar estas cosas \n",
    "        # manualmente, lo cual podría ser propenso a errores.\n",
    "        # La principal ventaja de usar register_buffer() es que PyTorch se encargará de estas cosas por nosotros.\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            x: output\n",
    "        \"\"\"\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.embedding_dim)\n",
    "        \n",
    "        # add encoding matrix to embedding (x)\n",
    "        sequence_len = x.size(1)\n",
    "        # x = x + torch.autograd.Variable(self.positional_encoding[:,:sequence_len], requires_grad=False)\n",
    "        x = x + self.positional_encoding[:,:sequence_len]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 100, 512]), output shape: torch.Size([1, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 512\n",
    "max_sequence_len = 100\n",
    "positional_encoding = PositionalEncoding(max_sequence_len, embedding_dim)\n",
    "x = torch.randn(1, max_sequence_len, embedding_dim)\n",
    "output = positional_encoding(x)\n",
    "print(f\"input shape: {x.shape}, output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos recibe una matriz con un tamaño dado y la devuelve del mismo tamaño, solo que la ha modificado un poco"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
