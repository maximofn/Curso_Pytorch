{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder masked scaled dot-product attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a volver a recordar la arquitectura y la fórmula del `Scaled Dot-Product Attention`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention.png\" alt=\"Scaled_Dot-Product_Attention\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "Ya explicamos todo menos la parte de la máscara, por lo que vamos a hacer un pequeño recordatorio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MatMul"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en el encoder, en el decoder tanto `Q`, como `K` como `V` son la misma matriz\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_decoder_masked_multi_head_attention.png\" alt=\"Multi-Head Attention\" style=\"width:425px;height:626px;\">\n",
    "</div>\n",
    "\n",
    "Así que en el primer `MatMul` en realidad se va a hacer una multiplicación de la matriz de entrada que llamaremos `X` consigo misma traspuesta\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_first_MatMul.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dijimos que la matriz `X` se compone del conjunto de vectores de embeddings de los tokens de la frase\n",
    "\n",
    "$$X = \\begin{pmatrix}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots\\\\\n",
    "v_m \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Donde `m` es el número de tokens de la frase\n",
    "\n",
    "Cada vector va a tener tantos elementos como las dimensiones de nuestro embedding, supongamos que es `n`, por tanto\n",
    "\n",
    "$$X = \\begin{pmatrix}\n",
    "v_{1,1} & v_{1,2} & \\cdots & v_{1,n} \\\\\n",
    "v_{2,1} & v_{2,2} & \\cdots & v_{2,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{m,1} & v_{m,2} & \\cdots & v_{m,n} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Así que la multiplicación de `X` consigo misma transpuesta es\n",
    "\n",
    "$$X \\cdot X^T = \\begin{pmatrix}\n",
    "v_1 \\cdot v_1 & v_1 \\cdot v_2 & \\cdots & v_1 \\cdot v_m \\\\\n",
    "v_2 \\cdot v_1 & v_2 \\cdot v_2 & \\cdots & v_2 \\cdot v_m \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_m \\cdot v_1 & v_m \\cdot v_2 & \\cdots & v_m \\cdot v_m \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Transformer - matmul.png\" alt=\"MatMul\">\n",
    "</div>\n",
    "\n",
    "La multiplicación sera una multiplicación de matrices de dimensiones $\\left(m \\times n\\right) \\cdot \\left(n \\times m\\right)$ que dará como resultado una matriz de tamaño $\\left(m \\times m\\right)$ donde `m` era el número de tokens de la frase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se dividía entre la raiz de la dimensión de embeddings porque es un tipo de normalización llamada `norma L2`.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_scale.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "Así que nos queda\n",
    "\n",
    "$$\n",
    "\\text{Scale} = \\frac{1}{\\sqrt{d_k}} \\cdot \\left( X \\cdot X^T \\right) = \\frac{1}{\\sqrt{d_k}} \\cdot \\begin{pmatrix}\n",
    "v_1 \\cdot v_1 & v_1 \\cdot v_2 & \\cdots & v_1 \\cdot v_m \\\\\n",
    "v_2 \\cdot v_1 & v_2 \\cdot v_2 & \\cdots & v_2 \\cdot v_m \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_m \\cdot v_1 & v_m \\cdot v_2 & \\cdots & v_m \\cdot v_m \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no aplicamos la máscara tendriamos que hacer la `Softmax`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_softmax.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "Por lo que nos quedaría una matriz así\n",
    "\n",
    "$$\n",
    "\\text{Softmax} = \\text{softmax}\\left( \\frac{1}{\\sqrt{d_k}} \\cdot \\left( X \\cdot X^T \\right) \\right) = \\text{softmax}\\left( \\frac{1}{\\sqrt{d_k}} \\cdot \\begin{pmatrix}\n",
    "v_1 \\cdot v_1 & v_1 \\cdot v_2 & \\cdots & v_1 \\cdot v_m \\\\\n",
    "v_2 \\cdot v_1 & v_2 \\cdot v_2 & \\cdots & v_2 \\cdot v_m \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_m \\cdot v_1 & v_m \\cdot v_2 & \\cdots & v_m \\cdot v_m \\\\\n",
    "\\end{pmatrix} \\right)\n",
    "$$\n",
    "\n",
    "La cual podemos simplemente suponer como porcentajes de atención\n",
    "\n",
    "$$\n",
    "\\text{Softmax} = \\begin{pmatrix}\n",
    "p_{1,1} & p_{1,2} & \\cdots & p_{1,m} \\\\\n",
    "p_{2,1} & p_{2,2} & \\cdots & p_{2,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "p_{m,1} & p_{m,2} & \\cdots & p_{m,m} \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MatMul"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al aplicar el último `MatMul`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_second_MatMul.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "En realidad lo que tenemos es la siguiente matriz\n",
    "\n",
    "$$\n",
    "\\text{Matmul} = \\begin{pmatrix}\n",
    "p_{1,1} & p_{1,2} & \\cdots & p_{1,m} \\\\\n",
    "p_{2,1} & p_{2,2} & \\cdots & p_{2,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "p_{m,1} & p_{m,2} & \\cdots & p_{m,m} \\\\\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots\\\\\n",
    "v_m \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "p_{1,1}v_1 + p_{1,2}v_2 + \\cdots + p_{1,m}v_m \\\\\n",
    "p_{2,1}v_1 + p_{2,2}v_2 + \\cdots + p_{2,m}v_m \\\\\n",
    "\\vdots \\\\\n",
    "p_{m,1}v_1 + p_{m,2}v_2 + \\cdots + p_{m,m}v_m \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a recordar cómo era la matriz de entrada\n",
    "\n",
    "$$X = \\begin{pmatrix}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots\\\\\n",
    "v_m \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "v_{1,1} & v_{1,2} & \\cdots & v_{1,n} \\\\\n",
    "v_{2,1} & v_{2,2} & \\cdots & v_{2,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{m,1} & v_{m,2} & \\cdots & v_{m,n} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Es decir, la primera fila correspondía al embedding del primer token, la segunda fila al embedding del segundo token y así sucesivamente hasta la última fila que corresponde al embedding del último token."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que si volvemos a ver el resultado de `Scaled Dot-Product Attention` vemos que la matriz\n",
    "\n",
    "$$\n",
    "\\text{Matmul} = \\begin{pmatrix}\n",
    "p_{1,1} & p_{1,2} & \\cdots & p_{1,m} \\\\\n",
    "p_{2,1} & p_{2,2} & \\cdots & p_{2,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "p_{m,1} & p_{m,2} & \\cdots & p_{m,m} \\\\\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots\\\\\n",
    "v_m \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "p_{1,1}v_1 + p_{1,2}v_2 + \\cdots + p_{1,m}v_m \\\\\n",
    "p_{2,1}v_1 + p_{2,2}v_2 + \\cdots + p_{2,m}v_m \\\\\n",
    "\\vdots \\\\\n",
    "p_{m,1}v_1 + p_{m,2}v_2 + \\cdots + p_{m,m}v_m \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Representa que\n",
    "\n",
    " * La primera fila (que representaría el primer token) se corresponde a la suma de probabilidades de atención del primer token con el resto de tokens por los embeddings del resto de tokens\n",
    " * La segunda fila (que representaría al segundo token) se corresponde a la suma de probabilidades de atención del segundo token con el resto de tokens por los embeddings del resto de tokens\n",
    " * Así sucesivamente, hasta la última fila (que representaría al último token) que se corresponde a la suma de probabilidades del último token con el resto de tokens por los embeddings del resto de tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enmascaramiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos las mismas operaciones, pero ahora aplicando la máscara. Veamos ahora qué ocurre cuando se aplica el enmascaramiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer MatMul"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después del primer `MatMul` seguimos teniendo el mismo resultado\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_first_MatMul.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "Tenemos una matriz `X`\n",
    "\n",
    "$$X = \\begin{pmatrix}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots\\\\\n",
    "v_m \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "v_{1,1} & v_{1,2} & \\cdots & v_{1,n} \\\\\n",
    "v_{2,1} & v_{2,2} & \\cdots & v_{2,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{m,1} & v_{m,2} & \\cdots & v_{m,n} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Que al multiplicarse por ella misma traspuesta queda\n",
    "\n",
    "$$X \\cdot X^T = \\begin{pmatrix}\n",
    "v_1 \\cdot v_1 & v_1 \\cdot v_2 & \\cdots & v_1 \\cdot v_m \\\\\n",
    "v_2 \\cdot v_1 & v_2 \\cdot v_2 & \\cdots & v_2 \\cdot v_m \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_m \\cdot v_1 & v_m \\cdot v_2 & \\cdots & v_m \\cdot v_m \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Transformer - matmul.png\" alt=\"MatMul\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica la `norma L2`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_scale.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "Y queda la siguiente matriz\n",
    "\n",
    "$$\n",
    "\\text{Scale} = \\frac{1}{\\sqrt{d_k}} \\cdot \\left( X \\cdot X^T \\right) = \\frac{1}{\\sqrt{d_k}} \\cdot \\begin{pmatrix}\n",
    "v_1 \\cdot v_1 & v_1 \\cdot v_2 & \\cdots & v_1 \\cdot v_m \\\\\n",
    "v_2 \\cdot v_1 & v_2 \\cdot v_2 & \\cdots & v_2 \\cdot v_m \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_m \\cdot v_1 & v_m \\cdot v_2 & \\cdots & v_m \\cdot v_m \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí se aplica el enmascaramiento\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_mask.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este enmascaramiento consiste en que nos quedemos con la matriz resultante de la diagonal y todos los elemetos por debajo de esa diagonal, es decir, algo así\n",
    "\n",
    "$$\n",
    "\\text{Masked} = \\frac{1}{\\sqrt{d_k}} \\cdot \\begin{pmatrix}\n",
    "v_1 \\cdot v_1 & 0 & \\cdots & 0 \\\\\n",
    "v_2 \\cdot v_1 & v_2 \\cdot v_2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_m \\cdot v_1 & v_m \\cdot v_2 & \\cdots & v_m \\cdot v_m \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo importante es el resultado que se quiere obtener, y quédate con eso, que es lo importate. Nos queremos quedar con la diagonal y todos los elementos por debajo de ella"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer eso se podría multiplicar elemento a elemento la matriz que sale del `Scale` por la matriz triangular inferior\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & \\cdots & 0 \\\\\n",
    "1 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & 1 & \\cdots & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Es decir, obtendríamos\n",
    "\n",
    "$$\n",
    "\\text{Masked} = \\frac{1}{\\sqrt{d_k}} \\cdot \\begin{pmatrix}\n",
    "v_1 \\cdot v_1 & v_1 \\cdot v_2 & \\cdots & v_1 \\cdot v_m \\\\\n",
    "v_2 \\cdot v_1 & v_2 \\cdot v_2 & \\cdots & v_2 \\cdot v_m \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_m \\cdot v_1 & v_m \\cdot v_2 & \\cdots & v_m \\cdot v_m \\\\\n",
    "\\end{pmatrix} x \\begin{pmatrix}\n",
    "1 & 0 & \\cdots & 0 \\\\\n",
    "1 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & 1 & \\cdots & 1 \\\\\n",
    "\\end{pmatrix} =  \\\\\n",
    "= \\frac{1}{\\sqrt{d_k}} \\cdot \\begin{pmatrix}\n",
    "v_1 \\cdot v_1 & 0 & \\cdots & 0 \\\\\n",
    "v_2 \\cdot v_1 & v_2 \\cdot v_2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_m \\cdot v_1 & v_m \\cdot v_2 & \\cdots & v_m \\cdot v_m \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero como esta operación no es deribable (para luego aplicar el descenso del gradiente), lo que se hace es sumarle una matriz donde todos los elementos de la diagonal y por debajo de ella son 0 y los elementos de encima de la diagonal son $-\\infty$\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "0 & -\\infty & \\cdots & -\\infty \\\\\n",
    "0 & 0 & \\cdots & -\\infty \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Así al hacer la suma obtendremos\n",
    "\n",
    "$$\n",
    "\\text{Masked} = \\frac{1}{\\sqrt{d_k}} \\cdot \\begin{pmatrix}\n",
    "v_1 \\cdot v_1 & v_1 \\cdot v_2 & \\cdots & v_1 \\cdot v_m \\\\\n",
    "v_2 \\cdot v_1 & v_2 \\cdot v_2 & \\cdots & v_2 \\cdot v_m \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_m \\cdot v_1 & v_m \\cdot v_2 & \\cdots & v_m \\cdot v_m \\\\\n",
    "\\end{pmatrix} + \\begin{pmatrix}\n",
    "0 & -\\infty & \\cdots & -\\infty \\\\\n",
    "0 & 0 & \\cdots & -\\infty \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & 0 \\\\\n",
    "\\end{pmatrix} =  \\\\\n",
    "= \\frac{1}{\\sqrt{d_k}} \\cdot \\begin{pmatrix}\n",
    "v_1 \\cdot v_1 & -\\infty & \\cdots & -\\infty \\\\\n",
    "v_2 \\cdot v_1 & v_2 \\cdot v_2 & \\cdots & -\\infty \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_m \\cdot v_1 & v_m \\cdot v_2 & \\cdots & v_m \\cdot v_m \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Y al hacer la siguiente `Softmax` todos los $-\\infty$ se convertiran en $0$, ya que $e^{-\\infty} = 0$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos que hacer la `Softmax`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_softmax.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "Por lo que nos quedaría una matriz así\n",
    "\n",
    "$$\n",
    "\\text{Softmax} = \\text{softmax}\\left( \\frac{1}{\\sqrt{d_k}} \\cdot \\begin{pmatrix}\n",
    "v_1 \\cdot v_1 & -\\infty & \\cdots & -\\infty \\\\\n",
    "v_2 \\cdot v_1 & v_2 \\cdot v_2 & \\cdots & -\\infty \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_m \\cdot v_1 & v_m \\cdot v_2 & \\cdots & v_m \\cdot v_m \\\\\n",
    "\\end{pmatrix} \\right)\n",
    "$$\n",
    "\n",
    "La cual podemos simplemente suponer como porcentajes de atención, en la cual los porcentajes de la parte superior de la matriz son $0$\n",
    "\n",
    "$$\n",
    "\\text{Softmax} = \\begin{pmatrix}\n",
    "p_{1,1} & 0 & \\cdots & 0 \\\\\n",
    "p_{2,1} & p_{2,2} & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "p_{m,1} & p_{m,2} & \\cdots & p_{m,m} \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MatMul"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al aplicar el último `MatMul`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_second_MatMul.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "En realidad lo que tenemos es la siguiente matriz\n",
    "\n",
    "$$\n",
    "\\text{Matmul} = \\begin{pmatrix}\n",
    "p_{1,1} & 0 & \\cdots & 0 \\\\\n",
    "p_{2,1} & p_{2,2} & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "p_{m,1} & p_{m,2} & \\cdots & p_{m,m} \\\\\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots\\\\\n",
    "v_m \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "p_{1,1}v_1 + 0 \\cdot v_2 + \\cdots + 0 \\cdot v_m \\\\\n",
    "p_{2,1}v_1 + p_{2,2}v_2 + \\cdots + 0 \\cdot v_m \\\\\n",
    "\\vdots \\\\\n",
    "p_{m,1}v_1 + p_{m,2}v_2 + \\cdots + p_{m,m}v_m \\\\\n",
    "\\end{pmatrix} = \\\\\n",
    "= \\begin{pmatrix}\n",
    "p_{1,1}v_1 \\\\\n",
    "p_{2,1}v_1 + p_{2,2}v_2 \\\\\n",
    "\\vdots \\\\\n",
    "p_{m,1}v_1 + p_{m,2}v_2 + \\cdots + p_{m,m}v_m \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a recordar cómo era la matriz de entrada\n",
    "\n",
    "$$X = \\begin{pmatrix}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots\\\\\n",
    "v_m \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "v_{1,1} & v_{1,2} & \\cdots & v_{1,n} \\\\\n",
    "v_{2,1} & v_{2,2} & \\cdots & v_{2,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{m,1} & v_{m,2} & \\cdots & v_{m,n} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Es decir, la primera fila correspondía al embedding del primer token, la segunda fila al embedding del segundo token y así sucesivamente hasta la última fila que corresponde al embedding del último token."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que si volvemos a ver el resultado de `Scaled Dot-Product Attention` pero aplicando el enmascaramiento vemos que la matriz es\n",
    "\n",
    "$$\n",
    "\\text{Matmul} = \\begin{pmatrix}\n",
    "p_{1,1}v_1 \\\\\n",
    "p_{2,1}v_1 + p_{2,2}v_2 \\\\\n",
    "\\vdots \\\\\n",
    "p_{m,1}v_1 + p_{m,2}v_2 + \\cdots + p_{m,m}v_m \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Representa que\n",
    "\n",
    " * La primera fila (que representaría el primer token) se corresponde a la probabilidad de atención del primer token consigo mismo por el embedding del primer token\n",
    " * La segunda fila (que representaría al segundo token) se corresponde a la suma de la probabilidad de atención del segundo token con la probabilidad de atención del primer token por el embedding del primer token, más la probabilidad de de atención del segundo token consigo mismo por el embedding del segundo token\n",
    " * Así sucesivamente, hasta la última fila (que representaría al último token) que se corresponde a la suma de probabilidades del último token con el resto de tokens por los embeddings del resto de tokens\n",
    "\n",
    "Es decir, cada fila (que correspondería a su correspondiente token, la primera fila correspondería al primer token, la segunda fila correspondería al segundo token, ...) se representa con la suma de probabilidades del token de esa fila con los anteriores por su embedding y los anteriores\n",
    "\n",
    "Cada fila corresponde a la suma ponderada de todas las probabilidades con los embeddings consigo mismo y los anteriores tokens. **Para cada fila (lo que correspondería a cada token) se ha eliminado la información de las probabilidades y embeddings de los tokens futuros**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cada fila (que correspondería a cada token) solo tiene información de si mismo y los tokens anteriores**\n",
    "\n",
    "**Se ha enmascarado el futuro**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Por qué enmascarar?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de entrenar se le da al transformer la secuencia de entrada y la secuencia de salida, no se va generando token a token. Por lo que no queremos que el transformer tenga información de los tokens futuros, porque entonces no necesitaría predecir, ya tendría la información. Por lo que no aprendería"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera que se realiza este enmascaramiento para que el transformer se pueda entrenar con la secuencia de entrada y salida de golpe y no acceda a los tokens futuros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de hacer inferencia también se enmascara.\n",
    "\n",
    "Supongamos que queremos traducir la frase `Me encanta el queso` al inglés. Por lo que en la secuencia de iteracciones sería esta (vamos a suponer que cada palablra equivale a un token)\n",
    "\n",
    "01. Input to encoder: `Me encanta el queso`  \n",
    "   Input to decoder: `<start>`  \n",
    "   Output decoder: `I`\n",
    "\n",
    "02. Input to encoder: `Me encanta el queso`  \n",
    "   Input to decoder: `<start> I`  \n",
    "   Output decoder: `love`\n",
    "\n",
    "03. Input to encoder: `Me encanta el queso`  \n",
    "   Input to decoder: `<start> I love`  \n",
    "   Output decoder: `cheese`\n",
    "\n",
    "04. Input to encoder: `Me encanta el queso`  \n",
    "   Input to decoder: `<start> I love cheese`  \n",
    "   Output decoder: `<end>`\n",
    "\n",
    "\n",
    "Como puedes ver, en cada iteracción le entra al decoder un token más que en la iteracción anterior. Pero como hemos visto, al trabajar con matrices se suelen utilizar matrices del mismo tamaño, por lo que en realidad lo que ocurre todo el rato es esto\n",
    "\n",
    "01. Input to encoder: `Me encanta el queso`  \n",
    "   Input to decoder: `<start> <pad> <pad> <pad>`  \n",
    "   Output decoder: `I`\n",
    "\n",
    "02. Input to encoder: `Me encanta el queso`  \n",
    "   Input to decoder: `<start> I <pad> <pad>`  \n",
    "   Output decoder: `love`\n",
    "\n",
    "03. Input to encoder: `Me encanta el queso`  \n",
    "   Input to decoder: `<start> I love <pad>`  \n",
    "   Output decoder: `cheese`\n",
    "\n",
    "04. Input to encoder: `Me encanta el queso`  \n",
    "   Input to decoder: `<start> I love cheese`  \n",
    "   Output decoder: `<end>`\n",
    "\n",
    "Todo el rato se mete una matriz del mismo tamaño con el token de `padding` en las posiciones en las que el transformer no tiene que predecir la siguiente palabra.\n",
    "\n",
    "Así que durante la inferencia también se enmascara para que el transformer no haga cálculos de atención con los tokens de `padding` y no se desvíe de la traducción"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En su día creamos la siguiente clase para el `Scaled Dot-Product Attention`\n",
    "\n",
    "``` python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "    \n",
    "    def forward(self, key, query, value):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            key: key vector\n",
    "            query: query vector\n",
    "            value: value vector\n",
    "        \n",
    "        Returns:\n",
    "            output vector from scaled dot product attention\n",
    "        \"\"\"\n",
    "        # MatMul\n",
    "        key_trasposed = key.transpose(-1,-2)\n",
    "        product = torch.matmul(query, key_trasposed)\n",
    "        # scale\n",
    "        scale = product / torch.sqrt(torch.tensor(self.dim_embedding))\n",
    "        # softmax\n",
    "        attention_matrix = torch.softmax(scale, dim=-1)\n",
    "        # MatMul\n",
    "        output = torch.matmul(attention_matrix, value)\n",
    "        \n",
    "        return output\n",
    "```\n",
    "\n",
    "Por lo que vamos a completarla con el enmascaramiento. Para ello vamos a utilizar la función [masked_fill](https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill.html) de Pytorch, que aplicara una máscara de $0$ en la diagonal y la parte inferior y $-\\infty$ en la parte superior\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "0 & -\\infty & \\cdots & -\\infty \\\\\n",
    "0 & 0 & \\cdots & -\\infty \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "[masked_fill](https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill.html) lo que hace es aplicar una máscara a los elementos de una matriz de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "    \n",
    "    def forward(self, key, query, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            key: key vector\n",
    "            query: query vector\n",
    "            value: value vector\n",
    "            mask: mask matrix (optional)\n",
    "        \n",
    "        Returns:\n",
    "            output vector from scaled dot product attention\n",
    "        \"\"\"\n",
    "        # MatMul\n",
    "        key_trasposed = key.transpose(-1,-2)\n",
    "        product = torch.matmul(query, key_trasposed)\n",
    "        # scale\n",
    "        scale = product / torch.sqrt(torch.tensor(self.dim_embedding))\n",
    "        # Mask (optional)\n",
    "        if mask is not None:\n",
    "            scale = scale.masked_fill(mask == 0, float('-inf'))\n",
    "        # softmax\n",
    "        attention_matrix = torch.softmax(scale, dim=-1)\n",
    "        # MatMul\n",
    "        output = torch.matmul(attention_matrix, value)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viendo la línea `scale = scale.masked_fill(mask == 0, float('-inf'))` lo que hace `masked_fill` es enmascarar la matriz `scale` con un $-\\infty$ en las posiciones de la máscara `mask` que sean iguales a $0$\n",
    "\n",
    "Por lo que generaremos una máscara triangular inferior con $1$ en la diagonal y por debajo de ella y $0$ en la parte superior\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & \\cdots & 0 \\\\\n",
    "1 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & 1 & \\cdots & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Y `masked_fill` lo que hará será transformar los $0$ de la máscara en $-\\infty$\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "0 & -\\infty & \\cdots & -\\infty \\\\\n",
    "0 & 0 & \\cdots & -\\infty \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a verlo, supongamos que tenemos la matriz resultante de `Scale`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5461, 0.5667, 0.8590],\n",
       "        [0.6783, 0.1796, 0.8954],\n",
       "        [0.5401, 0.1381, 0.5374]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "scale_matrix = torch.rand(3,3)\n",
    "scale_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triangular_mask = torch.tril(torch.ones(3,3))\n",
    "triangular_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5461,   -inf,   -inf],\n",
       "        [0.6783, 0.1796,   -inf],\n",
       "        [0.5401, 0.1381, 0.5374]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_matrix = scale_matrix.masked_fill(triangular_mask == 0, float('-inf'))\n",
    "masked_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, se han colocado $-\\infty$ en la parte superior de la matriz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer como en el encoder, vamos a coger el resultado del embedding más el positional encoding de una sentencia con BERT y lo vamos a pasar por el `Scaled Dot-Product Attention` con y sin enmascaramiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "def extract_embeddings(input_sentences, model_name='bert-base-uncased'):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "    \n",
    "    # tokenización de lote\n",
    "    inputs = tokenizer(input_sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    token_embeddings = outputs[0]\n",
    "    \n",
    "    # Los embeddings posicionales están en la segunda capa de los embeddings de la arquitectura BERT\n",
    "    positional_encodings = model.embeddings.position_embeddings.weight[:token_embeddings.shape[1], :].detach().unsqueeze(0).repeat(token_embeddings.shape[0], 1, 1)\n",
    "\n",
    "    embeddings_with_positional_encoding = token_embeddings + positional_encodings\n",
    "\n",
    "    # convierte las IDs de los tokens a tokens\n",
    "    tokens = [tokenizer.convert_ids_to_tokens(input_id) for input_id in inputs['input_ids']]\n",
    "\n",
    "    return tokens, inputs['input_ids'], token_embeddings, positional_encodings, embeddings_with_positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I gave the dog a bone because it was hungry\"\n",
    "tokens1, input_ids1, token_embeddings1, positional_encodings1, embeddings_with_positional_encoding1 = extract_embeddings(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = embeddings_with_positional_encoding1\n",
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un objeto de la clase `Scaled Dot-Product Attention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_embedding = X.shape[2]\n",
    "scaled_dot_product_attention = ScaledDotProductAttention(dim_embedding=dim_embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos primero el resultado sin enmascaramiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12, 768]),\n",
       " array([[[ 0.07,  0.01, -0.11, ...,  0.2 ,  0.25,  0.26],\n",
       "         [ 0.36, -0.13, -0.21, ...,  0.11,  0.79,  0.08],\n",
       "         [ 0.2 , -0.34,  0.18, ...,  0.08,  0.33, -0.08],\n",
       "         ...,\n",
       "         [-0.54,  0.12,  0.05, ..., -0.33,  0.2 ,  0.5 ],\n",
       "         [-0.16, -0.1 , -0.27, ...,  0.94,  0.42, -0.48],\n",
       "         [ 0.73,  0.24, -0.22, ...,  0.23, -0.63, -0.48]]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_no_mask = scaled_dot_product_attention(key=X, query=X, value=X)\n",
    "attention_no_mask.shape, (attention_no_mask.detach().numpy()*100).astype(int)/100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con enmascaramiento, primero creamos la máscara `mask` que se aplicará en `masked_fill`. Creamos una matriz triangular inferior del tamaño del número de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(sequence_len):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequence_len: length of sequence\n",
    "        \n",
    "    Returns:\n",
    "        mask matrix\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones((sequence_len, sequence_len)))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12]),\n",
       " array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = X.shape[0]\n",
    "sentence1_len = X.shape[1]\n",
    "mask = create_mask(X.shape[1])\n",
    "mask.shape, mask.detach().numpy().astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esa es la máscara que entrará en nuestro `Scaled Dot-Product Attention` de manera que `masked_fill` enmascarará la matrid `scale` con $-\\infty$ en las posiciones en las que `mask` sean 0 (`scale = scale.masked_fill(mask == 0, float('-inf'))`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12, 768]),\n",
       " array([[[ 0.07,  0.01, -0.11, ...,  0.2 ,  0.24,  0.26],\n",
       "         [ 0.44, -0.15, -0.26, ...,  0.12,  0.88,  0.07],\n",
       "         [ 0.36, -0.45,  0.2 , ...,  0.14,  0.37, -0.18],\n",
       "         ...,\n",
       "         [-0.54,  0.12,  0.06, ..., -0.33,  0.2 ,  0.5 ],\n",
       "         [-0.16, -0.1 , -0.27, ...,  0.95,  0.42, -0.48],\n",
       "         [ 0.73,  0.24, -0.22, ...,  0.23, -0.63, -0.48]]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = scaled_dot_product_attention(key=X, query=X, value=X, mask=mask)\n",
    "attention_mask.shape, (attention_mask.detach().numpy()*100).astype(int)/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver las dos juntas para evr que hay diferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin máscara:",
      "  [[[ 0.07  0.01 -0.11 ...  0.2   0.25  0.26]\n",
      "  [ 0.36 -0.13 -0.21 ...  0.11  0.79  0.08]\n",
      "  [ 0.2  -0.34  0.18 ...  0.08  0.33 -0.08]\n",
      "  ...\n",
      "  [-0.54  0.12  0.05 ... -0.33  0.2   0.5 ]\n",
      "  [-0.16 -0.1  -0.27 ...  0.94  0.42 -0.48]\n",
      "  [ 0.73  0.24 -0.22 ...  0.23 -0.63 -0.48]]]\n",
      "\n",
      "Con máscara:",
      "  [[[ 0.07  0.01 -0.11 ...  0.2   0.24  0.26]\n",
      "  [ 0.44 -0.15 -0.26 ...  0.12  0.88  0.07]\n",
      "  [ 0.36 -0.45  0.2  ...  0.14  0.37 -0.18]\n",
      "  ...\n",
      "  [-0.54  0.12  0.06 ... -0.33  0.2   0.5 ]\n",
      "  [-0.16 -0.1  -0.27 ...  0.95  0.42 -0.48]\n",
      "  [ 0.73  0.24 -0.22 ...  0.23 -0.63 -0.48]]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sin máscara:\\n{(attention_no_mask.detach().numpy()*100).astype(int)/100}\")\n",
    "print(f\"\\nCon máscara:\\n{(attention_mask.detach().numpy()*100).astype(int)/100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
