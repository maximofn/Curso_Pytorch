{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-decoder scaled dot-product attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordamos la arquitectura del `Scaled Dot-Product Attention`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention.png\" alt=\"Scaled_Dot-Product_Attention\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "Ya lo explicamos, pero ahora lo volvemos a explicar teniendo en cuenta que `K` y `V` es la matriz que proviene del encoder y `Q` proviene del decoder, por lo que se realizará una atención entre el encoder y el decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MatMul"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_decoder_multi_head_attention.png\" alt=\"Multi-Head Attention\" style=\"width:425px;height:626px;\">\n",
    "</div>\n",
    "\n",
    "Como ahora tenemos que `K` y `V` provienen del encoder, las llamaré $X_E$ y como `Q` proviene del decoder la llamaré $X_D$\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_first_MatMul.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo serían sus matrices, en primer lugar `K` y `V` o $X_E$\n",
    "\n",
    "$$K=V=X_E = \\begin{pmatrix}\n",
    "v_{E,1} \\\\\n",
    "v_{E,2} \\\\\n",
    "\\vdots\\\\\n",
    "v_{E,m} \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "v_{E,1,1} & v_{E,1,2} & \\cdots & v_{E,1,n} \\\\\n",
    "v_{E,2,1} & v_{E,2,2} & \\cdots & v_{E,2,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{E,m,1} & v_{E,m,2} & \\cdots & v_{E,m,n} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Y ahora `Q` o $X_D$\n",
    "\n",
    "$$K=V=X_D = \\begin{pmatrix}\n",
    "v_{D,1} \\\\\n",
    "v_{D,2} \\\\\n",
    "\\vdots\\\\\n",
    "v_{D,m} \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "v_{D,1,1} & v_{D,1,2} & \\cdots & v_{D,1,n} \\\\\n",
    "v_{D,2,1} & v_{D,2,2} & \\cdots & v_{D,2,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{D,m,1} & v_{D,m,2} & \\cdots & v_{D,m,n} \\\\\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que la multiplicación entre `Q` y `K`, es decir, entre $X_E$ y $X_D$ es\n",
    "\n",
    "$$X_D \\cdot X_E^T = \\begin{pmatrix}\n",
    "v_{D,1} \\cdot v_{E,1} & v_{D,1} \\cdot v_{E,2} & \\cdots & v_{D,1} \\cdot v_{E,m} \\\\\n",
    "v_{D,2} \\cdot v_{E,1} & v_{D,2} \\cdot v_{E,2} & \\cdots & v_{D,2} \\cdot v_{E,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{D,m} \\cdot v_{E,1} & v_{D,m} \\cdot v_{E,2} & \\cdots & v_{D,m} \\cdot v_{E,m} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "La multiplicación sera una multiplicación de matrices de dimensiones $\\left(m_D \\times n_D\\right) \\cdot \\left(n_E \\times m_E\\right)$, por lo que para que se pueda producir $n_D = n_E$, donde $n_D$ es la dimensión del embedding del decoder y $n_E$ es la dimensión del embedding del encoder. Es decir, para poder realizar esta operación, tanto el embedding del encoder como del decoder tienen que tener la misma dimensión.\n",
    "\n",
    "Si ambas dimensiones de embedding son iguales, obtendremos como resultado una matriz de tamaño $\\left(m_D \\times m_E\\right)$ donde $m_D$ era el número de tokens de la frase del decoder y $m_E$ era el número de tokens de la frase del encoder\n",
    "\n",
    "Representamos la dimensión\n",
    "\n",
    "$$X_D \\cdot X_E^T = \\begin{pmatrix}\n",
    "v_{D,1} \\cdot v_{E,1} & v_{D,1} \\cdot v_{E,2} & \\cdots & v_{D,1} \\cdot v_{E,m} \\\\\n",
    "v_{D,2} \\cdot v_{E,1} & v_{D,2} \\cdot v_{E,2} & \\cdots & v_{D,2} \\cdot v_{E,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{D,m} \\cdot v_{E,1} & v_{D,m} \\cdot v_{E,2} & \\cdots & v_{D,m} \\cdot v_{E,m} \\\\\n",
    "\\end{pmatrix}_{\\left(m_D \\times m_E\\right)}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se divide entre la dimensión del embedding de `K`, es decir, del encoder, pero en realidad da igual, porque hemos visto que la dimnesión del embedding del encoder y del decoder tienen que ser iguales. Esto se hace por ser una normalización `norma L2`\n",
    "\n",
    "Así que nos queda\n",
    "\n",
    "$$\n",
    "\\text{Scale} = \\frac{1}{\\sqrt{d_k}} \\cdot \\left( X_D \\cdot X_E^T \\right) = \\frac{1}{\\sqrt{d_k}} \\cdot \\begin{pmatrix}\n",
    "v_{D,1} \\cdot v_{E,1} & v_{D,1} \\cdot v_{E,2} & \\cdots & v_{D,1} \\cdot v_{E,m} \\\\\n",
    "v_{D,2} \\cdot v_{E,1} & v_{D,2} \\cdot v_{E,2} & \\cdots & v_{D,2} \\cdot v_{E,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{D,m} \\cdot v_{E,1} & v_{D,m} \\cdot v_{E,2} & \\cdots & v_{D,m} \\cdot v_{E,m} \\\\\n",
    "\\end{pmatrix}_{\\left(m_D \\times m_E\\right)}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este módulo de `Multi-Head Attention` no se realiza enmascaramiento, ya que el enmascaramiento se realiza para enmascarar el futuro de la secuencia de salida. Y eso ya lo hemos hecho en el anterior módulo de atención"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos el `Softmax` de la amtriz que tenemos\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_softmax.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "Por lo que nos quedaría una matriz así\n",
    "\n",
    "$$\n",
    "\\text{Softmax} = \\text{softmax}\\left( \\frac{1}{\\sqrt{d_k}} \\cdot \\left( X_D \\cdot X_E^T \\right) \\right) = \\\\\n",
    " = \\text{softmax}\\left( \\frac{1}{\\sqrt{d_k}} \\cdot \\begin{pmatrix}\n",
    "v_{D,1} \\cdot v_{E,1} & v_{D,1} \\cdot v_{E,2} & \\cdots & v_{D,1} \\cdot v_{E,m} \\\\\n",
    "v_{D,2} \\cdot v_{E,1} & v_{D,2} \\cdot v_{E,2} & \\cdots & v_{D,2} \\cdot v_{E,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{D,m} \\cdot v_{E,1} & v_{D,m} \\cdot v_{E,2} & \\cdots & v_{D,m} \\cdot v_{E,m} \\\\\n",
    "\\end{pmatrix}_{\\left(m_D \\times m_E\\right)} \\right)\n",
    "$$\n",
    "\n",
    "La cual podemos simplemente suponer como porcentajes de atención de los tokens salientes del encoder con los tokens del decoder\n",
    "\n",
    "$$\n",
    "\\text{Softmax} = \\begin{pmatrix}\n",
    "p_{1D,1E} & p_{1D,2E} & \\cdots & p_{1D,mE} \\\\\n",
    "p_{2D,1E} & p_{2D,2E} & \\cdots & p_{2D,mE} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "p_{mD,1E} & p_{mD,2E} & \\cdots & p_{mD,mE} \\\\\n",
    "\\end{pmatrix}_{\\left(m_D \\times m_E\\right)}\n",
    "$$\n",
    "\n",
    "Por ejemplo en la traducción de `¿Cuál es ti nombre?` a `What is your name` el porcentaje de atención entre `nombre` y `name` debería ser muy alto, pues esta matriz representa la atención entre los tokens de la frase en español con los tokens de la frase en ingles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MatMul"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último volvemos a realizar un `MatMul` entre la matriz que tenemos que es de dimensiones $\\left(m_D \\times m_E\\right)$ por `V` que como hemos dicho proviene de la salida del encoder ($X_E$), por lo que tiene dimensiones $\\left(m_E \\times n_E\\right)$. Así que la multiplicación va a ser de dimensiones $\\left(m_D \\times m_E\\right)·\\left(m_E \\times n_E\\right)$, lo que nos da una matriz de $\\left(m_D \\times n_E\\right)$, pero como las dimensiones del embedding del encoder y del decoder tienen que ser iguales $\\left(n_D = n_E\\right)$ nos queda una matriz de tamaño $\\left(m_D \\times n_D\\right)$\n",
    "\n",
    "Esto nos hace ver que hemos hecho lo correcto, porque si recuerdas, en el encoder siempre trabajamos con matrices $\\left(m_E \\times n_E\\right)$, en el módulo de atención del encoder entraba una matriz de tamaño $\\left(m_E \\times n_E\\right)$ y salía una matriz de tamaño $\\left(m_E \\times n_E\\right)$. Incluso en el encoder entero entraba una matriz de tamaño $\\left(m_E \\times n_E\\right)$ y salía una matriz de tamaño $\\left(m_E \\times n_E\\right)$.\n",
    "\n",
    "Por lo que ahora en si al decoder le entra una matriz $\\left(m_D \\times n_D\\right)$ y a la salida de este segundo módulo de atención tenemos una matriz de tamaño $\\left(m_D \\times n_D\\right)$ es que hemos hecho bien las cosas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si realizamos la operación\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_second_MatMul.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "En realidad lo que tenemos es la siguiente matriz\n",
    "\n",
    "$$\n",
    "\\text{Matmul} = \\begin{pmatrix}\n",
    "p_{1D,1E} & p_{1D,2E} & \\cdots & p_{1D,mE} \\\\\n",
    "p_{2D,1E} & p_{2D,2E} & \\cdots & p_{2D,mE} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "p_{mD,1E} & p_{mD,2E} & \\cdots & p_{mD,mE} \\\\\n",
    "\\end{pmatrix}_{\\left(m_D \\times m_E\\right)} \\cdot \\begin{pmatrix}\n",
    "v_{E,1} \\\\\n",
    "v_{E,2} \\\\\n",
    "\\vdots\\\\\n",
    "v_{E,m} \\\\\n",
    "\\end{pmatrix} = \\\\\n",
    " = \\begin{pmatrix}\n",
    "p_{1D,1E}·v_{E,1} + p_{1D,2E}·v_{E,2} + \\cdots + p_{1D,mE}·v_{E,m} \\\\\n",
    "p_{2D,1E}·v_{E,1} + p_{2D,2E}·v_{E,2} + \\cdots + p_{2D,mE}·v_{E,m} \\\\\n",
    "\\vdots \\\\\n",
    "p_{mD,1E}·v_{E,1} + p_{mD,2E}·v_{E,2} + \\cdots + p_{mD,mE}·v_{E,m} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Que representa\n",
    "\n",
    " * La primera fila (que representaría el primer token) se corresponde a la suma de probabilidades de atención del primer token del decoder con el resto de tokens del encoder por los embeddings del resto de tokens del encoder\n",
    " * La segunda fila (que representaría al segundo token) se corresponde a la suma de probabilidades de atención del segundo token del decoder con el resto de tokens del encoder por los embeddings del resto de tokens del encoder\n",
    " * Así sucesivamente, hasta la última fila (que representaría al último token) que se corresponde a la suma de probabilidades del último token del decoder con el resto de tokens del encoder por los embeddings del resto de tokens del encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase que hemos hecho hasta ahora para el `Scaled Dot-Product Attention` nos vale, ya que las operaciones son válidas, solo que tendremos que tener en cuenta que cuando la usemos, en este caso `K` y `V` tendrán que ser la matriz que sale del encoder y `Q` la matriz del decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "    \n",
    "    def forward(self, key, query, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            key: key vector\n",
    "            query: query vector\n",
    "            value: value vector\n",
    "            mask: mask matrix (optional)\n",
    "        \n",
    "        Returns:\n",
    "            output vector from scaled dot product attention\n",
    "        \"\"\"\n",
    "        # MatMul\n",
    "        key_trasposed = key.transpose(-1,-2)\n",
    "        product = torch.matmul(query, key_trasposed)\n",
    "        # scale\n",
    "        scale = product / torch.sqrt(torch.tensor(self.dim_embedding))\n",
    "        # Mask (optional)\n",
    "        if mask is not None:\n",
    "            scale = scale.masked_fill(mask == 0, float('-inf'))\n",
    "        # softmax\n",
    "        attention_matrix = torch.softmax(scale, dim=-1)\n",
    "        # MatMul\n",
    "        output = torch.matmul(attention_matrix, value)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vamos a necesitar la salida del encoder volvemos a escribir todas las clases del encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of vocabulary\n",
    "            embed_dim: dimension of embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            out: embedding vector\n",
    "        \"\"\"\n",
    "        return self.embedding(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_sequence_len, embedding_model_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq_len: length of input sequence\n",
    "            embed_model_dim: demension of embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_model_dim\n",
    "\n",
    "        # create constant 'positional_encoding' matrix with values dependant on pos and i\n",
    "        positional_encoding = torch.zeros(max_sequence_len, self.embedding_dim)\n",
    "        for pos in range(max_sequence_len):\n",
    "            for i in range(0, self.embedding_dim, 2):\n",
    "                positional_encoding[pos, i]     = torch.sin(torch.tensor(pos / (10000 ** ((2 * i) / self.embedding_dim))))\n",
    "                positional_encoding[pos, i + 1] = torch.cos(torch.tensor(pos / (10000 ** ((2 * (i+1)) / self.embedding_dim))))\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            x: output\n",
    "        \"\"\"\n",
    "        # make embeddings relatively larger\n",
    "        x = x * torch.sqrt(torch.tensor(self.embedding_dim))\n",
    "        \n",
    "        # add encoding matrix to embedding (x)\n",
    "        sequence_len = x.size(1)\n",
    "        # x = x + torch.autograd.Variable(self.positional_encoding[:,:sequence_len], requires_grad=False)\n",
    "        x = x + self.positional_encoding[:,:sequence_len]\n",
    "        return x\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            key: key vector\n",
    "            query: query vector\n",
    "            value: value vector\n",
    "            mask: mask matrix (optional)\n",
    "        \n",
    "        Returns:\n",
    "            output vector from scaled dot product attention\n",
    "        \"\"\"\n",
    "        # MatMul\n",
    "        key_trasposed = key.transpose(-1,-2)\n",
    "        product = torch.matmul(query, key_trasposed)\n",
    "        # scale\n",
    "        scale = product / torch.sqrt(torch.tensor(self.dim_embedding))\n",
    "        # Mask (optional)\n",
    "        if mask is not None:\n",
    "            scale = scale.masked_fill(mask == 0, float('-inf'))\n",
    "        # softmax\n",
    "        attention_matrix = torch.softmax(scale, dim=-1)\n",
    "        # MatMul\n",
    "        output = torch.matmul(attention_matrix, value)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            heads: number of heads\n",
    "            dim_embedding: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.dim_proyection = dim_embedding // heads\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.proyection_Q = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.proyection_K = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.proyection_V = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.attention = nn.Linear(dim_embedding, dim_embedding)\n",
    "\n",
    "        self.scaled_dot_product_attention = ScaledDotProductAttention(self.dim_proyection)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q: query vector\n",
    "            K: key vector\n",
    "            V: value vector\n",
    "            mask: mask matrix (optional)\n",
    "\n",
    "        Returns:\n",
    "            output vector from multi-head attention\n",
    "        \"\"\"\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        proyection_Q = self.proyection_Q(Q).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_K = self.proyection_K(K).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_V = self.proyection_V(V).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "        proyection_Q = proyection_Q.transpose(1,2)\n",
    "        proyection_K = proyection_K.transpose(1,2)\n",
    "        proyection_V = proyection_V.transpose(1,2)\n",
    "\n",
    "        # calculate attention\n",
    "        scaled_dot_product_attention = self.scaled_dot_product_attention(proyection_Q, proyection_K, proyection_V, mask=mask)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scaled_dot_product_attention.transpose(1,2).contiguous().view(batch_size, -1, self.dim_embedding)\n",
    "        \n",
    "        output = self.attention(concat)\n",
    "    \n",
    "        return output\n",
    "\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding (int): Embedding dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.normalization = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            sublayer (torch.Tensor): Sublayer tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return self.normalization(torch.add(x, sublayer))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_embedding, increment=4):\n",
    "        super().__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim_embedding, dim_embedding*increment),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_embedding*increment, dim_embedding)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch_size, seq_len, dim_embedding)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch_size, seq_len, dim_embedding)\n",
    "        \"\"\"\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(heads, dim_embedding)\n",
    "        self.add_and_norm_1 = AddAndNorm(dim_embedding)\n",
    "        self.feed_forward = FeedForward(dim_embedding)\n",
    "        self.add_and_norm_2 = AddAndNorm(dim_embedding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch_size, seq_len, dim_embedding)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch_size, seq_len, dim_embedding)\n",
    "        \"\"\"\n",
    "        multi_head_attention = self.multi_head_attention(x, x, x)\n",
    "        add_and_norm_1 = self.add_and_norm_1(x, multi_head_attention)\n",
    "        feed_forward = self.feed_forward(add_and_norm_1)\n",
    "        add_and_norm_2 = self.add_and_norm_2(add_and_norm_1, feed_forward)\n",
    "        return add_and_norm_2\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, Nx):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(heads, dim_embedding) for _ in range(Nx)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch_size, seq_len, dim_embedding)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch_size, seq_len, dim_embedding)\n",
    "        \"\"\"\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, dim_embedding, max_sequence_len, heads, Nx):\n",
    "        super().__init__()\n",
    "        self.input_embedding = Embedding(vocab_size, dim_embedding)\n",
    "        self.positional_encoding = PositionalEncoding(max_sequence_len, dim_embedding)\n",
    "        self.encoder = Encoder(heads, dim_embedding, Nx)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch_size, seq_len, dim_embedding)\n",
    "        \"\"\"\n",
    "        input_embedding = self.input_embedding(x)\n",
    "        positional_encoding = self.positional_encoding(input_embedding)\n",
    "        encoder = self.encoder(positional_encoding)\n",
    "        return encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a definir la función que obtiene el embbeding más el positional encoding de BERT. Ahora no usaremos su embedding, pero sí sus tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "def extract_embeddings(input_sentences, model_name='bert-base-uncased'):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "    \n",
    "    # tokenización de lote\n",
    "    inputs = tokenizer(input_sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    token_embeddings = outputs[0]\n",
    "    \n",
    "    # Los embeddings posicionales están en la segunda capa de los embeddings de la arquitectura BERT\n",
    "    positional_encodings = model.embeddings.position_embeddings.weight[:token_embeddings.shape[1], :].detach().unsqueeze(0).repeat(token_embeddings.shape[0], 1, 1)\n",
    "\n",
    "    embeddings_with_positional_encoding = token_embeddings + positional_encodings\n",
    "\n",
    "    # convierte las IDs de los tokens a tokens\n",
    "    tokens = [tokenizer.convert_ids_to_tokens(input_id) for input_id in inputs['input_ids']]\n",
    "\n",
    "    return tokens, inputs['input_ids'], token_embeddings, positional_encodings, embeddings_with_positional_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una sentencia para el encoder, ya que ahora va a entrar una sentencia al encoder y otra al decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder = \"I gave the dog a bone because it was hungry\"\n",
    "tokens_encoder, input_ids_encoder, token_embeddings_encoder, positional_encodings_encoder, embeddings_with_positional_encoding_encoder = extract_embeddings(sentence_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un objeto `encoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30522\n",
    "dim_embedding = token_embeddings_encoder.shape[-1]\n",
    "max_sequence_len = token_embeddings_encoder.shape[1]\n",
    "heads = 8\n",
    "Nx = 6\n",
    "transformer_encoder = TransformerEncoder(vocab_size, dim_embedding, max_sequence_len, heads, Nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la salida del transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output = transformer_encoder(input_ids_encoder)\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora generamos una sentencia para el decoder, lo que haremos será generar la sentencia del encoder traducida al español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder = \"I gave the dog a bone because it was hungry\"\n",
    "sentence_decoder = \"Le di un hueso al perro porque tenía hambre\"\n",
    "tokens_decoder, input_ids_decoder, token_embeddings_decoder, positional_encodings_decoder, embeddings_with_positional_encoding_decoder = extract_embeddings(sentence_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cómo es la secuencia del encoder y del decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de tokens del encoder: 12, embedding encoder shape: torch.Size([1, 12, 768]), positional encodings encoder shape: torch.Size([1, 12, 768]), embeddings with positional encodings encoder shape: torch.Size([1, 12, 768])\n",
      "Numero de tokens del decoder: 16, embedding decoder shape: torch.Size([1, 16, 768]), positional encodings decoder shape: torch.Size([1, 16, 768]), embeddings with positional encodings decoder shape: torch.Size([1, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numero de tokens del encoder: {len(tokens_encoder[0])}, embedding encoder shape: {token_embeddings_encoder.shape}, positional encodings encoder shape: {positional_encodings_encoder.shape}, embeddings with positional encodings encoder shape: {embeddings_with_positional_encoding_encoder.shape}\")\n",
    "print(f\"Numero de tokens del decoder: {len(tokens_decoder[0])}, embedding decoder shape: {token_embeddings_decoder.shape}, positional encodings decoder shape: {positional_encodings_decoder.shape}, embeddings with positional encodings decoder shape: {embeddings_with_positional_encoding_decoder.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos la frase en inglés (`encoder`) tiene 12 tokens y en español (`decoder`) tiene 16 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos un objeto de la clase `Multi-Head Attention` enmascarada y obtenemos su salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_mask(sequence_len):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequence_len: length of sequence\n",
    "        \n",
    "    Returns:\n",
    "        mask matrix\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones((sequence_len, sequence_len)))\n",
    "    return mask\n",
    "sequence_len = input_ids_decoder.shape[1]\n",
    "mask = create_mask(sequence_len)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_embedding = embeddings_with_positional_encoding_decoder.shape[-1]\n",
    "heads = 8\n",
    "masked_multi_head_attention = MultiHeadAttention(heads=heads, dim_embedding=dim_embedding)\n",
    "\n",
    "K = embeddings_with_positional_encoding_decoder\n",
    "V = embeddings_with_positional_encoding_decoder\n",
    "Q = embeddings_with_positional_encoding_decoder\n",
    "masked_attention = masked_multi_head_attention(Q, K, V, mask=mask)\n",
    "masked_attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto, la secuencia del decoder tenía 16 tokens, por lo que tiene sentido que la dimensión sea 1x16x768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos ahora un objeto de la clase `Add & Norm` y calculamos su salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_and_norm_3 = AddAndNorm(dim_embedding)\n",
    "masked_attention_add_and_norm = add_and_norm_3(embeddings_with_positional_encoding_decoder, masked_attention)\n",
    "masked_attention_add_and_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos todas las entradas para el `Encoder-Decoder Scaled Dot-Product Attention`, así que creamos un objeto y obtenemos su salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_decoder_scaled_dot_product_attention = ScaledDotProductAttention(dim_embedding)\n",
    "K = encoder_output\n",
    "V = encoder_output\n",
    "Q = masked_attention_add_and_norm\n",
    "encoder_decoder_attention = encoder_decoder_scaled_dot_product_attention(Q, K, V)\n",
    "encoder_decoder_attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No obtenemos una matriz de 1x12x768 (dimensión del embedding del encoder) sino 1x16x768 que es la dimensión del embedding del decoder. Por lo que aunque las secuencias del encoder y del decoder no tengan el mismo número de tokens no hay problema, ya que el `Encoder-Decoder Scaled Dot-Product Attention` a la salida seguirá dando el número de tokens del decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si lo pensamos, es lo que tiene que pasar, si el decoder tiene 16 tokens y el encoder 12, a la salida del `Encoder-Decoder Scaled Dot-Product Attention` tiene que generar una secuencia con el número de tokens del decoder, ya que queremos predecir el siguiente token del decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver matemáticamente por qué pasa eso, primero recordamos la arquirtectura y la fórmula del `Scaled Dot-Product Attention`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention.png\" alt=\"Scaled_Dot-Product_Attention\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "Y segundo recordamos que `K` y `V` es la matriz que proviene del encoder y `Q` proviene del decoder\n",
    "\n",
    "De modo que primero tenemos una multiplicación entre `Q` y la traspuesta de `K`, es decir tendremos una multiplicación de dimensiones $\\left(m_D \\times n_D\\right) \\cdot \\left(n_E \\times m_E\\right)$, donde $m_D$ y $m_E$ son el número de tokens del decoder y encoder respectivamente y $n_D$ y $n_E$ son la dimensión del embedding del decoder y encoder respectivamente y que para poder hacer la multiplicación de las matrices tienen que ser iguales. De modo que nos queda una matriz de dimensiones $\\left(m_D \\times m_E\\right)$\n",
    "\n",
    "Luego la operación de `Scale` no cambia el tamaño de la matriz, aquí no enmascaramos y la operación de `Softmax` tampoco cambia el tamaño\n",
    "\n",
    "Así que al final nos quedamos con la multiplicación de una matriz de tamaño $\\left(m_D \\times m_E\\right)$ por otra de tamaño $\\left(m_E \\times n_E\\right)$, es decir $\\left(m_D \\times m_E\\right) \\cdot \\left(m_E \\times n_E\\right)$, por lo que nos queda una matriz de tamaño $\\left(m_D \\times n_E\\right)$, pero como hemos dicho que la dimensión del embedding del encoder y del decoder tiene que ser la misma, entonces queda $\\left(m_D \\times n\\right)$\n",
    "\n",
    "Es decir, nos queda una matriz con el número de tokens del decoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
