{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-decoder scaled dot-product attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordamos la arquitectura del `Scaled Dot-Product Attention`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention.png\" alt=\"Scaled_Dot-Product_Attention\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "Ya lo explicamos, pero ahora lo volvemos a explicar teniendo en cuenta que `K` y `V` es la matriz que proviene del encoder y `Q` proviene del decoder, por lo que se realizará una atención entre el encoder y el decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MatMul"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_decoder_multi_head_attention.png\" alt=\"Multi-Head Attention\" style=\"width:425px;height:626px;\">\n",
    "</div>\n",
    "\n",
    "Como ahora tenemos que `K` y `V` provienen del encoder, las llamaré $X_E$ y como `Q` proviene del decoder la llamaré $X_D$\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_first_MatMul.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo serían sus matrices, en primer lugar `K` y `V` o $X_E$\n",
    "\n",
    "$$K=V=X_E = \\begin{pmatrix}\n",
    "v_{E,1} \\\\\n",
    "v_{E,2} \\\\\n",
    "\\vdots\\\\\n",
    "v_{E,m} \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "v_{E,1,1} & v_{E,1,2} & \\cdots & v_{E,1,n} \\\\\n",
    "v_{E,2,1} & v_{E,2,2} & \\cdots & v_{E,2,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{E,m,1} & v_{E,m,2} & \\cdots & v_{E,m,n} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Y ahora `Q` o $X_D$\n",
    "\n",
    "$$K=V=X_D = \\begin{pmatrix}\n",
    "v_{D,1} \\\\\n",
    "v_{D,2} \\\\\n",
    "\\vdots\\\\\n",
    "v_{D,m} \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "v_{D,1,1} & v_{D,1,2} & \\cdots & v_{D,1,n} \\\\\n",
    "v_{D,2,1} & v_{D,2,2} & \\cdots & v_{D,2,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{D,m,1} & v_{D,m,2} & \\cdots & v_{D,m,n} \\\\\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que la multiplicación entre `Q` y `K`, es decir, entre $X_E$ y $X_D$ es\n",
    "\n",
    "$$X_D \\cdot X_E^T = \\begin{pmatrix}\n",
    "v_{D,1} \\cdot v_{E,1} & v_{D,1} \\cdot v_{E,2} & \\cdots & v_{D,1} \\cdot v_{E,m} \\\\\n",
    "v_{D,2} \\cdot v_{E,1} & v_{D,2} \\cdot v_{E,2} & \\cdots & v_{D,2} \\cdot v_{E,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{D,m} \\cdot v_{E,1} & v_{D,m} \\cdot v_{E,2} & \\cdots & v_{D,m} \\cdot v_{E,m} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "La multiplicación sera una multiplicación de matrices de dimensiones $\\left(m_D \\times n_D\\right) \\cdot \\left(n_E \\times m_E\\right)$, por lo que para que se pueda producir $n_D = n_E$, donde $n_D$ es la dimensión del embedding del decoder y $n_E$ es la dimensión del embedding del encoder. Es decir, para poder realizar esta operación, tanto el embedding del encoder como del decoder tienen que tener la misma dimensión.\n",
    "\n",
    "Si ambas dimensiones de embedding son iguales, obtendremos como resultado una matriz de tamaño $\\left(m_D \\times m_E\\right)$ donde $m_D$ era el número de tokens de la frase del decoder y $m_E$ era el número de tokens de la frase del encoder\n",
    "\n",
    "Representamos la dimensión\n",
    "\n",
    "$$X_D \\cdot X_E^T = \\begin{pmatrix}\n",
    "v_{D,1} \\cdot v_{E,1} & v_{D,1} \\cdot v_{E,2} & \\cdots & v_{D,1} \\cdot v_{E,m} \\\\\n",
    "v_{D,2} \\cdot v_{E,1} & v_{D,2} \\cdot v_{E,2} & \\cdots & v_{D,2} \\cdot v_{E,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{D,m} \\cdot v_{E,1} & v_{D,m} \\cdot v_{E,2} & \\cdots & v_{D,m} \\cdot v_{E,m} \\\\\n",
    "\\end{pmatrix}_{\\left(m_D \\times m_E\\right)}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se divide entre la dimensión del embedding de `K`, es decir, del encoder, pero en realidad da igual, porque hemos visto que la dimnesión del embedding del encoder y del decoder tienen que ser iguales. Esto se hace por ser una normalización `norma L2`\n",
    "\n",
    "Así que nos queda\n",
    "\n",
    "$$\n",
    "\\text{Scale} = \\frac{1}{\\sqrt{d_k}} \\cdot \\left( X_D \\cdot X_E^T \\right) = \\frac{1}{\\sqrt{d_k}} \\cdot \\begin{pmatrix}\n",
    "v_{D,1} \\cdot v_{E,1} & v_{D,1} \\cdot v_{E,2} & \\cdots & v_{D,1} \\cdot v_{E,m} \\\\\n",
    "v_{D,2} \\cdot v_{E,1} & v_{D,2} \\cdot v_{E,2} & \\cdots & v_{D,2} \\cdot v_{E,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{D,m} \\cdot v_{E,1} & v_{D,m} \\cdot v_{E,2} & \\cdots & v_{D,m} \\cdot v_{E,m} \\\\\n",
    "\\end{pmatrix}_{\\left(m_D \\times m_E\\right)}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este módulo de `Multi-Head Attention` no se realiza enmascaramiento, ya que el enmascaramiento se realiza para enmascarar el futuro de la secuencia de salida. Y eso ya lo hemos hecho en el anterior módulo de atención"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos el `Softmax` de la amtriz que tenemos\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_softmax.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "Por lo que nos quedaría una matriz así\n",
    "\n",
    "$$\n",
    "\\text{Softmax} = \\text{softmax}\\left( \\frac{1}{\\sqrt{d_k}} \\cdot \\left( X_D \\cdot X_E^T \\right) \\right) = \\\\\n",
    " = \\text{softmax}\\left( \\frac{1}{\\sqrt{d_k}} \\cdot \\begin{pmatrix}\n",
    "v_{D,1} \\cdot v_{E,1} & v_{D,1} \\cdot v_{E,2} & \\cdots & v_{D,1} \\cdot v_{E,m} \\\\\n",
    "v_{D,2} \\cdot v_{E,1} & v_{D,2} \\cdot v_{E,2} & \\cdots & v_{D,2} \\cdot v_{E,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{D,m} \\cdot v_{E,1} & v_{D,m} \\cdot v_{E,2} & \\cdots & v_{D,m} \\cdot v_{E,m} \\\\\n",
    "\\end{pmatrix}_{\\left(m_D \\times m_E\\right)} \\right)\n",
    "$$\n",
    "\n",
    "La cual podemos simplemente suponer como porcentajes de atención de los tokens salientes del encoder con los tokens del decoder\n",
    "\n",
    "$$\n",
    "\\text{Softmax} = \\begin{pmatrix}\n",
    "p_{1D,1E} & p_{1D,2E} & \\cdots & p_{1D,mE} \\\\\n",
    "p_{2D,1E} & p_{2D,2E} & \\cdots & p_{2D,mE} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "p_{mD,1E} & p_{mD,2E} & \\cdots & p_{mD,mE} \\\\\n",
    "\\end{pmatrix}_{\\left(m_D \\times m_E\\right)}\n",
    "$$\n",
    "\n",
    "Por ejemplo en la traducción de `¿Cuál es ti nombre?` a `What is your name` el porcentaje de atención entre `nombre` y `name` debería ser muy alto, pues esta matriz representa la atención entre los tokens de la frase en español con los tokens de la frase en ingles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MatMul"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último volvemos a realizar un `MatMul` entre la matriz que tenemos que es de dimensiones $\\left(m_D \\times m_E\\right)$ por `V` que como hemos dicho proviene de la salida del encoder ($X_E$), por lo que tiene dimensiones $\\left(m_E \\times n_E\\right)$. Así que la multiplicación va a ser de dimensiones $\\left(m_D \\times m_E\\right)·\\left(m_E \\times n_E\\right)$, lo que nos da una matriz de $\\left(m_D \\times n_E\\right)$, pero como las dimensiones del embedding del encoder y del decoder tienen que ser iguales nos queda una matriz de tamaño $\\left(m_D \\times m_D\\right)$\n",
    "\n",
    "Esto nos hace ver que hemos hecho lo correcto, porque si recuerdas, en el encoder siempre trabajamos con matrices $\\left(m_E \\times n_E\\right)$, en el módulo de atención del encoder entraba una matriz de tamaño $\\left(m_E \\times n_E\\right)$ y salía una matriz de tamaño $\\left(m_E \\times n_E\\right)$. Incluso en el encoder entero entraba una matriz de tamaño $\\left(m_E \\times n_E\\right)$ y salía una matriz de tamaño $\\left(m_E \\times n_E\\right)$.\n",
    "\n",
    "Por lo que ahora en si al decoder le entra una matriz $\\left(m_D \\times n_D\\right)$ y a la salida de este segundo módulo de atención tenemos una matriz de tamaño $\\left(m_D \\times n_D\\right)$ es que hemos hecho bien las cosas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si realizamos la operación\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_second_MatMul.png\" alt=\"MatMul\">\n",
    "  <img src=\"Imagenes/Scaled_Dot-Product_Attention_formula.png\" alt=\"Scaled Dot-Product Attention formula\">\n",
    "</div>\n",
    "\n",
    "En realidad lo que tenemos es la siguiente matriz\n",
    "\n",
    "$$\n",
    "\\text{Matmul} = \\begin{pmatrix}\n",
    "p_{1D,1E} & p_{1D,2E} & \\cdots & p_{1D,mE} \\\\\n",
    "p_{2D,1E} & p_{2D,2E} & \\cdots & p_{2D,mE} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "p_{mD,1E} & p_{mD,2E} & \\cdots & p_{mD,mE} \\\\\n",
    "\\end{pmatrix}_{\\left(m_D \\times m_E\\right)} \\cdot \\begin{pmatrix}\n",
    "v_{E,1} \\\\\n",
    "v_{E,2} \\\\\n",
    "\\vdots\\\\\n",
    "v_{E,m} \\\\\n",
    "\\end{pmatrix} = \\\\\n",
    " = \\begin{pmatrix}\n",
    "p_{1D,1E}·v_{E,1} + p_{1D,2E}·v_{E,2} + \\cdots + p_{1D,mE}·v_{E,m} \\\\\n",
    "p_{2D,1E}·v_{E,1} + p_{2D,2E}·v_{E,2} + \\cdots + p_{2D,mE}·v_{E,m} \\\\\n",
    "\\vdots \\\\\n",
    "p_{mD,1E}·v_{E,1} + p_{mD,2E}·v_{E,2} + \\cdots + p_{mD,mE}·v_{E,m} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Que representa\n",
    "\n",
    " * La primera fila (que representaría el primer token) se corresponde a la suma de probabilidades de atención del primer token del decoder con el resto de tokens del encoder por los embeddings del resto de tokens del encoder\n",
    " * La segunda fila (que representaría al segundo token) se corresponde a la suma de probabilidades de atención del segundo token del decoder con el resto de tokens del encoder por los embeddings del resto de tokens del encoder\n",
    " * Así sucesivamente, hasta la última fila (que representaría al último token) que se corresponde a la suma de probabilidades del último token del decoder con el resto de tokens del encoder por los embeddings del resto de tokens del encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase que hemos hecho hasta ahora para el `Scaled Dot-Product Attention` nos vale, ya que las operaciones son válidas, solo que tendremos que tener en cuenta que cuando la usemos, en este caso `K` y `V` tendrán que ser la matriz que sale del encoder y `Q` la matriz del decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "    \n",
    "    def forward(self, key, query, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            key: key vector\n",
    "            query: query vector\n",
    "            value: value vector\n",
    "            mask: mask matrix (optional)\n",
    "        \n",
    "        Returns:\n",
    "            output vector from scaled dot product attention\n",
    "        \"\"\"\n",
    "        # MatMul\n",
    "        key_trasposed = key.transpose(-1,-2)\n",
    "        product = torch.matmul(query, key_trasposed)\n",
    "        # scale\n",
    "        scale = product / math.sqrt(self.dim_embedding)\n",
    "        # Mask (optional)\n",
    "        if mask is not None:\n",
    "            scale = scale.masked_fill(mask == 0, float('-inf'))\n",
    "        # softmax\n",
    "        attention_matrix = torch.nn.functional.softmax(scale, dim=-1)\n",
    "        # MatMul\n",
    "        output = torch.matmul(attention_matrix, value)\n",
    "        \n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
