{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulos add and norm y feed forward finales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo nos quedan los módulos `Add & Norm` y `Feed Forwar`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_add_and_norm4.png\" alt=\"Add and norm\" style=\"width:425px;height:626px;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_decoder_feed_forward.png\" alt=\"Encoder Feed Forward\" style=\"width:425px;height:626px;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_add_and_norm5.png\" alt=\"Add and norm\" style=\"width:425px;height:626px;\">\n",
    "</div>\n",
    "\n",
    "Como ya los hemos explicado, estos tres últimos los volvemos a contar en un único notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add & Norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos módulos aportan beneficios a la red por las conexiones residuales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consiste en una red fully conected como las que hemos visto al principio del curso, pero de dos capas, exactamente en el paper lo definen así\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/feed_forward_equation.png\" alt=\"Feed Forward equation\">\n",
    "</div>\n",
    "\n",
    "Esta fórmula lo que nos indica es que hay que pasar la matriz por una capa lineal con pesos $W_1$ y $b_1$, aplicarle una función de activación Relu y a continuación pasarle por otra capa lineal con pesos $W_2$ y $b_2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El motivo de esta capa es añadir una no linealidad al encoder, lo cual trae las siguientes ventajas\n",
    "\n",
    " * Incremento de la capacidad del modelo:\n",
    " * Extracción de características:\n",
    "\n",
    "En la primera capa del `Feed Forward` se podrá aumentar la dimensión de la matriz, pero a la salida del módulo `Feed Forward` la dimensión de la matriz tiene que ser $\\left(m_E \\times n_E\\right)$.\n",
    "\n",
    "En el paper proponen que si la matriz de entrada tiene una dimensión de embedding de 512, a la salida de la primera capa la matriz tenga una dimensión de embedding de 2048 y a la salida de la segunda capa vuelva a tener una dimensión de embedding de 512. Por lo que en la parte de `Feed Forward` internamente se multiplica la dimensión de la matriz por 4 y luego se vuelve a reducir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperamos las clases que ya hemos hecho antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding (int): Embedding dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.normalization = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            sublayer (torch.Tensor): Sublayer tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return self.normalization(torch.add(x, sublayer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_embedding, increment=4):\n",
    "        super().__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim_embedding, dim_embedding*increment),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_embedding*increment, dim_embedding)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch_size, seq_len, dim_embedding)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch_size, seq_len, dim_embedding)\n",
    "        \"\"\"\n",
    "        x = self.feed_forward(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
