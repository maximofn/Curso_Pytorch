{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulos add and norm y feed forward finales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo nos quedan los módulos `Add & Norm` y `Feed Forwar`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_add_and_norm4.png\" alt=\"Add and norm\" style=\"width:425px;height:626px;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_decoder_feed_forward.png\" alt=\"Encoder Feed Forward\" style=\"width:425px;height:626px;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_add_and_norm5.png\" alt=\"Add and norm\" style=\"width:425px;height:626px;\">\n",
    "</div>\n",
    "\n",
    "Como ya los hemos explicado, estos tres últimos los volvemos a contar en un único notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add & Norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos módulos aportan beneficios a la red por las conexiones residuales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consiste en una red fully conected como las que hemos visto al principio del curso, pero de dos capas, exactamente en el paper lo definen así\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/feed_forward_equation.png\" alt=\"Feed Forward equation\">\n",
    "</div>\n",
    "\n",
    "Esta fórmula lo que nos indica es que hay que pasar la matriz por una capa lineal con pesos $W_1$ y $b_1$, aplicarle una función de activación Relu y a continuación pasarle por otra capa lineal con pesos $W_2$ y $b_2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El motivo de esta capa es añadir una no linealidad al encoder, lo cual trae las siguientes ventajas\n",
    "\n",
    " * Incremento de la capacidad del modelo:\n",
    " * Extracción de características:\n",
    "\n",
    "En la primera capa del `Feed Forward` se podrá aumentar la dimensión de la matriz, pero a la salida del módulo `Feed Forward` la dimensión de la matriz tiene que ser $\\left(m_E \\times n_E\\right)$.\n",
    "\n",
    "En el paper proponen que si la matriz de entrada tiene una dimensión de embedding de 512, a la salida de la primera capa la matriz tenga una dimensión de embedding de 2048 y a la salida de la segunda capa vuelva a tener una dimensión de embedding de 512. Por lo que en la parte de `Feed Forward` internamente se multiplica la dimensión de la matriz por 4 y luego se vuelve a reducir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperamos las clases que ya hemos hecho antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding (int): Embedding dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.normalization = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            sublayer (torch.Tensor): Sublayer tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return self.normalization(torch.add(x, sublayer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_embedding, increment=4):\n",
    "        super().__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim_embedding, dim_embedding*increment),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_embedding*increment, dim_embedding)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch_size, seq_len, dim_embedding)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch_size, seq_len, dim_embedding)\n",
    "        \"\"\"\n",
    "        x = self.feed_forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vamos a necesitar la salida del encoder volvemos a escribir todas las clases del encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of vocabulary\n",
    "            embed_dim: dimension of embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            out: embedding vector\n",
    "        \"\"\"\n",
    "        return self.embedding(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_sequence_len, embedding_model_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq_len: length of input sequence\n",
    "            embed_model_dim: demension of embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_model_dim\n",
    "\n",
    "        # create constant 'positional_encoding' matrix with values dependant on pos and i\n",
    "        positional_encoding = torch.zeros(max_sequence_len, self.embedding_dim)\n",
    "        for pos in range(max_sequence_len):\n",
    "            for i in range(0, self.embedding_dim, 2):\n",
    "                positional_encoding[pos, i]     = math.sin(pos / (10000 ** ((2 *     i) / self.embedding_dim)))\n",
    "                positional_encoding[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i+1)) / self.embedding_dim)))\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            x: output\n",
    "        \"\"\"\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.embedding_dim)\n",
    "        \n",
    "        # add encoding matrix to embedding (x)\n",
    "        sequence_len = x.size(1)\n",
    "        # x = x + torch.autograd.Variable(self.positional_encoding[:,:sequence_len], requires_grad=False)\n",
    "        x = x + self.positional_encoding[:,:sequence_len]\n",
    "        return x\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            key: key vector\n",
    "            query: query vector\n",
    "            value: value vector\n",
    "            mask: mask matrix (optional)\n",
    "        \n",
    "        Returns:\n",
    "            output vector from scaled dot product attention\n",
    "        \"\"\"\n",
    "        # MatMul\n",
    "        key_trasposed = key.transpose(-1,-2)\n",
    "        product = torch.matmul(query, key_trasposed)\n",
    "        # scale\n",
    "        scale = product / math.sqrt(self.dim_embedding)\n",
    "        # Mask (optional)\n",
    "        if mask is not None:\n",
    "            scale = scale.masked_fill(mask == 0, float('-inf'))\n",
    "        # softmax\n",
    "        attention_matrix = torch.nn.functional.softmax(scale, dim=-1)\n",
    "        # MatMul\n",
    "        output = torch.matmul(attention_matrix, value)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            heads: number of heads\n",
    "            dim_embedding: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.dim_proyection = dim_embedding // heads\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.proyection_Q = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.proyection_K = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.proyection_V = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.attention = nn.Linear(dim_embedding, dim_embedding)\n",
    "\n",
    "        self.scaled_dot_product_attention = ScaledDotProductAttention(self.dim_proyection)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q: query vector\n",
    "            K: key vector\n",
    "            V: value vector\n",
    "            mask: mask matrix (optional)\n",
    "\n",
    "        Returns:\n",
    "            output vector from multi-head attention\n",
    "        \"\"\"\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        proyection_Q = self.proyection_Q(Q).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_K = self.proyection_K(K).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_V = self.proyection_V(V).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "        proyection_Q = proyection_Q.transpose(1,2)\n",
    "        proyection_K = proyection_K.transpose(1,2)\n",
    "        proyection_V = proyection_V.transpose(1,2)\n",
    "\n",
    "        # calculate attention\n",
    "        scaled_dot_product_attention = self.scaled_dot_product_attention(proyection_Q, proyection_K, proyection_V, mask=mask)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scaled_dot_product_attention.transpose(1,2).contiguous().view(batch_size, -1, self.dim_embedding)\n",
    "        \n",
    "        output = self.attention(concat)\n",
    "    \n",
    "        return output\n",
    "\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding (int): Embedding dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.normalization = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            sublayer (torch.Tensor): Sublayer tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return self.normalization(torch.add(x, sublayer))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_embedding, increment=4):\n",
    "        super().__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim_embedding, dim_embedding*increment),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_embedding*increment, dim_embedding)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch_size, seq_len, dim_embedding)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch_size, seq_len, dim_embedding)\n",
    "        \"\"\"\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(heads, dim_embedding)\n",
    "        self.add_and_norm_1 = AddAndNorm(dim_embedding)\n",
    "        self.feed_forward = FeedForward(dim_embedding)\n",
    "        self.add_and_norm_2 = AddAndNorm(dim_embedding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch_size, seq_len, dim_embedding)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch_size, seq_len, dim_embedding)\n",
    "        \"\"\"\n",
    "        multi_head_attention = self.multi_head_attention(x, x, x)\n",
    "        add_and_norm_1 = self.add_and_norm_1(x, multi_head_attention)\n",
    "        feed_forward = self.feed_forward(add_and_norm_1)\n",
    "        add_and_norm_2 = self.add_and_norm_2(add_and_norm_1, feed_forward)\n",
    "        return add_and_norm_2\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, Nx):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(heads, dim_embedding) for _ in range(Nx)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch_size, seq_len, dim_embedding)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch_size, seq_len, dim_embedding)\n",
    "        \"\"\"\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, dim_embedding, max_sequence_len, heads, Nx):\n",
    "        super().__init__()\n",
    "        self.input_embedding = Embedding(vocab_size, dim_embedding)\n",
    "        self.positional_encoding = PositionalEncoding(max_sequence_len, dim_embedding)\n",
    "        self.encoder = Encoder(heads, dim_embedding, Nx)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch_size, seq_len, dim_embedding)\n",
    "        \"\"\"\n",
    "        input_embedding = self.input_embedding(x)\n",
    "        positional_encoding = self.positional_encoding(input_embedding)\n",
    "        encoder = self.encoder(positional_encoding)\n",
    "        return encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a definir la función que obtiene el embbeding más el positional encoding de BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "def extract_embeddings(input_sentences, model_name='bert-base-uncased'):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "    \n",
    "    # tokenización de lote\n",
    "    inputs = tokenizer(input_sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    token_embeddings = outputs[0]\n",
    "    \n",
    "    # Los embeddings posicionales están en la segunda capa de los embeddings de la arquitectura BERT\n",
    "    positional_encodings = model.embeddings.position_embeddings.weight[:token_embeddings.shape[1], :].detach().unsqueeze(0).repeat(token_embeddings.shape[0], 1, 1)\n",
    "\n",
    "    embeddings_with_positional_encoding = token_embeddings + positional_encodings\n",
    "\n",
    "    # convierte las IDs de los tokens a tokens\n",
    "    tokens = [tokenizer.convert_ids_to_tokens(input_id) for input_id in inputs['input_ids']]\n",
    "\n",
    "    return tokens, inputs['input_ids'], token_embeddings, positional_encodings, embeddings_with_positional_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una sentencia para el encoder, ya que ahora va a entrar una sentencia al encoder y otra al decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "sentence_encoder = \"I gave the dog a bone because it was hungry\"\n",
    "tokens_encoder, input_ids_encoder, token_embeddings_encoder, positional_encodings_encoder, embeddings_with_positional_encoding_encoder = extract_embeddings(sentence_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un objeto `encoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30522\n",
    "dim_embedding = token_embeddings_encoder.shape[-1]\n",
    "max_sequence_len = token_embeddings_encoder.shape[1]\n",
    "heads = 8\n",
    "Nx = 6\n",
    "transformer_encoder = TransformerEncoder(vocab_size, dim_embedding, max_sequence_len, heads, Nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la salida del transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output = transformer_encoder(input_ids_encoder)\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora generamos una sentencia para el decoder, lo que haremos será generar la sentencia del encoder traducida al español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "sentence_encoder = \"I gave the dog a bone because it was hungry\"\n",
    "sentence_decoder = \"Le di un hueso al perro porque tenía hambre\"\n",
    "tokens_decoder, input_ids_decoder, token_embeddings_decoder, positional_encodings_decoder, embeddings_with_positional_encoding_decoder = extract_embeddings(sentence_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos un objeto de la clase `Multi-Head Attention` enmascarada y obtenemos su salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_mask(sequence_len):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequence_len: length of sequence\n",
    "        \n",
    "    Returns:\n",
    "        mask matrix\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones((sequence_len, sequence_len)))\n",
    "    return mask\n",
    "sequence_len = input_ids_decoder.shape[1]\n",
    "mask = create_mask(sequence_len)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_embedding = embeddings_with_positional_encoding_decoder.shape[-1]\n",
    "heads = 8\n",
    "masked_multi_head_attention = MultiHeadAttention(heads=heads, dim_embedding=dim_embedding)\n",
    "\n",
    "K = embeddings_with_positional_encoding_decoder\n",
    "V = embeddings_with_positional_encoding_decoder\n",
    "Q = embeddings_with_positional_encoding_decoder\n",
    "masked_attention = masked_multi_head_attention(Q, K, V, mask=mask)\n",
    "masked_attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos ahora un objeto de la clase `Add & Norm` y calculamos su salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_and_norm_3 = AddAndNorm(dim_embedding)\n",
    "masked_attention_add_and_norm = add_and_norm_3(embeddings_with_positional_encoding_decoder, masked_attention)\n",
    "masked_attention_add_and_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos un objeto de la clase `ENcoder-Decoder Multi-Head Attention` y obtenemos su salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_decoder_multi_head_attention = MultiHeadAttention(heads=heads, dim_embedding=dim_embedding)\n",
    "K = encoder_output\n",
    "V = encoder_output\n",
    "Q = masked_attention_add_and_norm\n",
    "\n",
    "encoder_decoder_attention = encoder_decoder_multi_head_attention(Q, K, V)\n",
    "encoder_decoder_attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos ahora un objeto de la clase `Add & Norm` y calculamos su salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_and_norm_4 = AddAndNorm(dim_embedding)\n",
    "encoder_decoder_attention_add_and_norm = add_and_norm_4(masked_attention_add_and_norm, encoder_decoder_attention)\n",
    "encoder_decoder_attention_add_and_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos ahora un objeto de la clase `Feed Forward` y calculamos su salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward = FeedForward(dim_embedding)\n",
    "feed_forward_output = feed_forward(encoder_decoder_attention_add_and_norm)\n",
    "feed_forward_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos ahora un objeto de la clase `Add & Norm` y calculamos su salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_and_norm_5 = AddAndNorm(dim_embedding)\n",
    "add_and_norm_5_output = add_and_norm_5(encoder_decoder_attention_add_and_norm, feed_forward_output)\n",
    "add_and_norm_5_output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
