{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear y softmax finales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último tenemos dos capas más, una `Linear` y una `Softmax`\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_linear.png\" alt=\"Add and norm\" style=\"width:425px;height:626px;\">\n",
    "  <img src=\"Imagenes/transformer_architecture_model_softmax.png\" alt=\"Encoder Feed Forward\" style=\"width:425px;height:626px;\">\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos dicho el transformer se pensó como un traductor de un idioma a otro, así que ponte que lo queremos usar para hacer un traductor del español al inglés. Como hemos visto hasta ahora, la matriz que sale del decoder es una matriz de tamaño $\\left(m_D \\times n_D\\right)$, pero esto es una matriz de secuencias, nosotros lo que necesitamos es predecir la siguiente palabra. Pues con el módulo `Linear` lo que se hace es contruir un última red fully connected de manera que a la salida tengamos un vector con un montón de posibles tokens del inglés\n",
    "\n",
    "Y ¿cuántos tokens tenemos que tener en ese vector? Todos los que tengamos, es decir, si hemos definido dos millones de tokens para definir todas las posibles palabras del ingĺes, a la salida deberemos tener un vector con esos dos millones de tokens, a esto normalmente se le llama `vocab size` o tamaño de nuestro vocabulario.\n",
    "\n",
    "Por lo que la capa `Linear` es una red fully connected que transformará nuestra matriz de tamaño $\\left(m_E \\times n_E\\right)$ a un vector de tamaño `vocab size`. Y lo hará con unos pesos que en función de la matriz que salga del trnasformer, tras todos los mecanismos de atención, haga que tengan más valor unos tokens que otros"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y de todos los posibles tokens de nuestro vocabulario ¿con cuál nos qudamos? Pues para eso tenemos el último módulo `Softmax` con el que convertimos todos los elementos de nuestro vector en probabilidades, de manera que nos quedamos con el token que tenga más probabilidad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos dos clases para estos últimos módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, dim_embedding, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class Softmax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
