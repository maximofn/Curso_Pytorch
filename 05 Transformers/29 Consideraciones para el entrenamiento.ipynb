{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consideraciones para el entrenamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el punto 5 del paper hablan de cómo han entrenado al transformer, así que vamos a verlo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar al transformer dicen que han usado el dataset [WMT 2014 English-German](https://aclanthology.org/W14-3302.pdf), pero mejor hacer un traductor de inglés a español, así que vamos a usar el dataset [opus100](https://huggingface.co/datasets/opus100) del inglés al español (`en-es`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Han usado 8 Nvidia P100. Supongo que ninguno de nosotros tenemos acceso a 8 GPUs de ese tipo, por lo que haremos un código que sea ejecutable en [Colab](https://colab.research.google.com/), que al día de escribir este curso ofrece GPUs como la Nvidia T4 que tiene la misma VRAM que la P100, pero con una arquitectura más nueva. Por lo que en vez de usar 8 GPUs como las del paper, usaremos solo una pero un poco mejor que una de las que usaron ellos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dicen que entrenaron el modelo 100.000 pasos o 12 horas. Hay que diferenciar los steps de las epochs. Un step es cuando un solo batch size pasa por el modelo para entrenar y se realiza el backpropagation, mientras que una epoch es cuando todo el dataset ha pasado por el modelo.\n",
    "\n",
    "Colab ofrece 12 horas de GPU, así que hacermos algo similar, entrenaremos unos 100.000 steps o cuando estemos un poco por debajo de las 12 horas pararemos el entrenamiento. Pero como nosotros solo vamos a poder usar una GPU seguramente no llegaremos a 100.000 steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizador"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los investigadores dicen que usaron un optimizador Adam con $\\beta_1 = 0.9$, $\\beta_2 = 0.98$ y $\\epsilon = 10^{-9}$ y que variaron el learning rate según la fórmula\n",
    "\n",
    "$$lr = d_{model}^{-0.5} \\cdot min \\left(stepNumber^{-0.5}, stepNumber \\cdot stepWarmUp^{-1.5} \\right)$$\n",
    "\n",
    "Esto corresponde a un aumento lineal de la tasa de aprendizaje para los primeros pasos de entrenamiento (pasos de calentamiento (warmup)), y a una disminución posterior proporcional a la raíz cuadrada inversa del número de pasos. Utilizaron 4000 pasos de calentamiento (warmup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación del optimizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar el optimizador es sencillo, ya que como hemos visto ya que la función `torch.optim.Adam` hace todo, así que lo que tenemos que hacer es\n",
    "\n",
    "``` python\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación del scheduler del lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer esto vamos a usar la función de Pytorch `torch.optim.lr_scheduler.LambdaLR`, que añade un scheduler al learning rate. Lo vamos a implementar de la siguiente manera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero creamos una función que calcula el valor que debe tener el learning rate en función de la época en la que está, es decir, creamos una función que resuelva la fórmula que indican en el paper\n",
    "\n",
    "```python\n",
    "def calculate_lr(step_num, dim_embeding_model=512, warmup_steps=4000):\n",
    "    step_num += 1e-7 # Avoid division by zero\n",
    "    return (dim_embeding_model**-0.5) * min(step_num**-0.5, step_num*(warmup_steps**-1.5))\n",
    "```\n",
    "\n",
    "Se puede ver que hemos añadido `step_num += 1e-7` porque si no, en la época 0 intenta hacer `step_num**-0.5` lo cual daría error porque no se puede elevar el número 0 a un número negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación añadimos el scheduler del learning rate al optimizador que hemos creado antes\n",
    "\n",
    "```python\n",
    "lr_lambda = lambda step: calculate_lr(step, dim_embeding_model=dim_embedding)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, en el bucle de entrenamiento, después de hacer `optimizer.step()`, hay que además hacer `scheduler.step()` para modificar el valor del learning rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante el entrenamiento utilizaron dropout con una probabilidad de 0.1 después de cada capa de atención y cada capa feed-forward. Además utilizaron label smoothing\n",
    "\n",
    "El dropout ya lo hemos explicado, pero hacemos un recordatorio rápido. El dropout consiste en durante el entrenamiento desactivar la conexión entre neuronas aleatoriamente y con una probabilidad.\n",
    "\n",
    "El label smoothing es un suavizado de las probabilidades de la secuencia de salida. Supongamos que le metemos al transformer la frase en español `¿Cuál es tu nombre?` y el trasnformer ha generado `What is your` de modo que la siguiente palabra tiene que ser `name`. Por lo que de todos los posibles tokens del inglés, el correspondiente a `name` debería tener la etiqueta de 1 y el resto de 0. Pero el label smoothing lo que hace es que name tenga una etiqueta de por ejemplo 0.95 y el resto 0.000001\n",
    "\n",
    "Se define un valor de $1-\\epsilon$, y se asigna la etiqueta del token que toca a $1-\\epsilon$ y el resto de tokens tienen la etiqueta de $\\epsilon / \\left(numTokens-1 \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación del dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una clase para el dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Dropout(torch.nn.Module):\n",
    "    def __init__(self, p=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            p (float): probability of an element to be zeroed. Default: 0.1\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: a tensor with the same shape of `x`\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            return torch.nn.functional.dropout(x, p=self.p)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación del label smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder implementar el label smoothing, simplemente se indica el valor de $\\epsilon$ en la función de pérdida\n",
    "\n",
    "```python\n",
    "loss = nn.CrossEntropyLoss(ignore_index=pad_token, reduction='mean', label_smoothing=0.1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vamos a usar la función de pérdida `nn.CrossEntropyLoss` ya no necesitamos usar la última capa de softmax del transformer, ya que esta función ya lo hace por nosotros, por lo que la clase `Linear_and_softmax` quedaría así\n",
    "\n",
    "```python\n",
    "class Linear_and_softmax(nn.Module):\n",
    "    def __init__(self, dim_embedding, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = CustomLinear(dim_embedding, vocab_size)\n",
    "        # self.softmax = Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialización de los pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las clases `nn.Linear` vamos a inicializar los pesos según técnica de [Kaiming He](https://arxiv.org/pdf/1502.01852v1.pdf), que es una técnica de inicialización de los pesos para redes neuronales que tiene en cuenta la no linealidad de las funciones de activación, como las activaciones ReLU.\n",
    "Por eso la vamos a usar en las capas `nn.Linear`, ya que a continuación de estas hay una activación `ReLU`\n",
    "\n",
    "De modo que vamos a crear una nueva clase `CustomLinear` de la siguiente manera\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(CustomLinear, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        init.kaiming_uniform_(self.linear.weight, nonlinearity='relu')\n",
    "        if self.linear.bias is not None:\n",
    "            init.zeros_(self.linear.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la clase `nn.Embedding` vamos a inicializar los pesos según la técnica de [Xavier Glorot](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), que es un esquema de inicialización para redes neuronales. Los sesgos se inicializan en 0 y los pesos $W_{ij}$ en cada capa se inicializan como\n",
    "\n",
    "$$ W_{ij} \\sim U\\left[-\\frac{\\sqrt{6}}{\\sqrt{fan_{in} + fan_{out}}}, \\frac{\\sqrt{6}}{\\sqrt{fan_{in} + fan_{out}}}\\right] $$\n",
    "\n",
    "Donde $U$ es una distribución uniforme y $fan_{in}$ es el tamaño de la capa anterior (número de columnas en $W$) y &fan_{out}$ es el tamaño de la capa actual\n",
    "\n",
    "La principal ventaja de la inicialización de `Xavier` es que ayuda a combatir el problema del desvanecimiento o explosión de gradientes\n",
    "\n",
    "De modo que vamos a crear una nueva clase `CustomEmbedding` de la siguiente manera\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class CustomEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(CustomEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        init.xavier_uniform_(self.embedding.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación del transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a escribir cómo quedaría el transformer con el dropout, que es lo único que se añade al modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero escribimos todas las clases de bajo nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(CustomLinear, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        init.kaiming_uniform_(self.linear.weight, nonlinearity='relu')\n",
    "        if self.linear.bias is not None:\n",
    "            init.zeros_(self.linear.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class CustomEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(CustomEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        init.xavier_uniform_(self.embedding.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = CustomEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_sequence_len, embedding_model_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_model_dim\n",
    "        positional_encoding = torch.zeros(max_sequence_len, self.embedding_dim)\n",
    "        for pos in range(max_sequence_len):\n",
    "            for i in range(0, self.embedding_dim, 2):\n",
    "                positional_encoding[pos, i]     = torch.sin(torch.tensor(pos / (10000 ** ((2 * i) / self.embedding_dim))))\n",
    "                positional_encoding[pos, i + 1] = torch.cos(torch.tensor(pos / (10000 ** ((2 * (i+1)) / self.embedding_dim))))\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * torch.sqrt(torch.tensor(self.embedding_dim))\n",
    "        sequence_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:,:sequence_len]\n",
    "        return x\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        key_trasposed = key.transpose(-1,-2)\n",
    "        product = torch.matmul(query, key_trasposed)\n",
    "        scale = product / torch.sqrt(torch.tensor(self.dim_embedding))\n",
    "        if mask is not None:\n",
    "            scale = scale.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_matrix = torch.softmax(scale, dim=-1)\n",
    "        output = torch.matmul(attention_matrix, value)\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.dim_proyection = dim_embedding // heads\n",
    "        self.heads = heads\n",
    "        self.proyection_Q = CustomLinear(dim_embedding, dim_embedding)\n",
    "        self.proyection_K = CustomLinear(dim_embedding, dim_embedding)\n",
    "        self.proyection_V = CustomLinear(dim_embedding, dim_embedding)\n",
    "        self.scaled_dot_product_attention = ScaledDotProductAttention(self.dim_proyection)\n",
    "        self.attention = CustomLinear(dim_embedding, dim_embedding)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        proyection_Q = self.proyection_Q(Q).view(batch_size, self.heads, -1, self.dim_proyection)\n",
    "        proyection_K = self.proyection_K(K).view(batch_size, self.heads, -1, self.dim_proyection)\n",
    "        proyection_V = self.proyection_V(V).view(batch_size, self.heads, -1, self.dim_proyection)\n",
    "        # proyection_Q = proyection_Q.transpose(1,2)\n",
    "        # proyection_K = proyection_K.transpose(1,2)\n",
    "        # proyection_V = proyection_V.transpose(1,2)\n",
    "        scaled_dot_product_attention = self.scaled_dot_product_attention(proyection_Q, proyection_K, proyection_V, mask=mask)\n",
    "        concat = scaled_dot_product_attention.transpose(1,2).contiguous().view(batch_size, -1, self.dim_embedding)\n",
    "        output = self.attention(concat)\n",
    "        return output\n",
    "\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        super().__init__()\n",
    "        self.normalization = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return self.normalization(torch.add(x, sublayer))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_embedding, increment=4):\n",
    "        super().__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            CustomLinear(dim_embedding, dim_embedding*increment),\n",
    "            nn.ReLU(),\n",
    "            CustomLinear(dim_embedding*increment, dim_embedding)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, dim_embedding, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = CustomLinear(dim_embedding, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class Softmax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class Dropout(torch.nn.Module):\n",
    "    def __init__(self, p=0.1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            return torch.nn.functional.dropout(x, p=self.p)\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escribimos las clases de medio nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(heads, dim_embedding)\n",
    "        self.dropout_1 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_1 = AddAndNorm(dim_embedding)\n",
    "        self.feed_forward = FeedForward(dim_embedding)\n",
    "        self.dropout_2 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_2 = AddAndNorm(dim_embedding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        multi_head_attention = self.multi_head_attention(x, x, x)\n",
    "        dropout1 = self.dropout_1(multi_head_attention)\n",
    "        add_and_norm_1 = self.add_and_norm_1(x, dropout1)\n",
    "        feed_forward = self.feed_forward(add_and_norm_1)\n",
    "        dropout2 = self.dropout_2(feed_forward)\n",
    "        add_and_norm_2 = self.add_and_norm_2(add_and_norm_1, dropout2)\n",
    "        return add_and_norm_2\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, Nx, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(heads, dim_embedding, prob_dropout) for _ in range(Nx)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_multi_head_attention = MultiHeadAttention(heads, dim_embedding)\n",
    "        self.dropout_1 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_1 = AddAndNorm(dim_embedding)\n",
    "        self.encoder_decoder_multi_head_attention = MultiHeadAttention(heads, dim_embedding)\n",
    "        self.dropout_2 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_2 = AddAndNorm(dim_embedding)\n",
    "        self.feed_forward = FeedForward(dim_embedding)\n",
    "        self.dropout_3 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_3 = AddAndNorm(dim_embedding)\n",
    "    \n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        Q = x\n",
    "        K = x\n",
    "        V = x\n",
    "        masked_multi_head_attention = self.masked_multi_head_attention(Q, K, V, mask=mask)\n",
    "        dropout1 = self.dropout_1(masked_multi_head_attention)\n",
    "        add_and_norm_1 = self.add_and_norm_1(dropout1, x)\n",
    "\n",
    "        Q = add_and_norm_1\n",
    "        K = encoder_output\n",
    "        V = encoder_output\n",
    "        encoder_decoder_multi_head_attention = self.encoder_decoder_multi_head_attention(Q, K, V)\n",
    "        dropout2 = self.dropout_2(encoder_decoder_multi_head_attention)\n",
    "        add_and_norm_2 = self.add_and_norm_2(dropout2, add_and_norm_1)\n",
    "\n",
    "        feed_forward = self.feed_forward(add_and_norm_2)\n",
    "        dropout3 = self.dropout_3(feed_forward)\n",
    "        add_and_norm_3 = self.add_and_norm_3(dropout3, add_and_norm_2)\n",
    "\n",
    "        return add_and_norm_3\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, Nx, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(heads, dim_embedding, prob_dropout) for _ in range(Nx)])\n",
    "    \n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        for decoder_layer in self.layers:\n",
    "            x = decoder_layer(x, encoder_output, mask)\n",
    "        return x\n",
    "\n",
    "class Linear_and_softmax(nn.Module):\n",
    "    def __init__(self, dim_embedding, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = CustomLinear(dim_embedding, vocab_size)\n",
    "        # self.softmax = Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por último escribimos la clase transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, src_max_seq_len, tgt_max_seq_len, dim_embedding, Nx, heads, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(heads, dim_embedding, Nx, prob_dropout)\n",
    "        self.decoder = Decoder(heads, dim_embedding, Nx, prob_dropout)\n",
    "        self.sourceEmbedding = Embedding(src_vocab_size, dim_embedding)\n",
    "        self.targetEmbedding = Embedding(tgt_vocab_size, dim_embedding)\n",
    "        self.sourcePositional_encoding = PositionalEncoding(src_max_seq_len, dim_embedding)\n",
    "        self.targetPositional_encoding = PositionalEncoding(tgt_max_seq_len, dim_embedding)\n",
    "        self.linear = Linear_and_softmax(dim_embedding, tgt_vocab_size)\n",
    "    \n",
    "    def encode(self, source):\n",
    "        embedding = self.sourceEmbedding(source)\n",
    "        positional_encoding = self.sourcePositional_encoding(embedding)\n",
    "        encoder_output = self.encoder(positional_encoding)\n",
    "        return encoder_output\n",
    "    \n",
    "    def decode(self, encoder_output, target, target_mask):\n",
    "        embedding = self.targetEmbedding(target)\n",
    "        positional_encoding = self.targetPositional_encoding(embedding)\n",
    "        decoder_output = self.decoder(positional_encoding, encoder_output, target_mask)\n",
    "        return decoder_output\n",
    "    \n",
    "    def projection(self, decoder_output):\n",
    "        linear_output = self.linear(decoder_output)\n",
    "        # softmax_output = self.softmax(linear_output)\n",
    "        return linear_output\n",
    "    \n",
    "    def forward(self, source, target, target_mask):\n",
    "        # Encode\n",
    "        embedding_encoder = self.sourceEmbedding(source)\n",
    "        positional_encoding_encoder = self.sourcePositional_encoding(embedding_encoder)\n",
    "        encoder_output = self.encoder(positional_encoding_encoder)\n",
    "\n",
    "        # Decode\n",
    "        embedding_decoder = self.targetEmbedding(target)\n",
    "        positional_encoding_decoder = self.targetPositional_encoding(embedding_decoder)\n",
    "        decoder_output = self.decoder(positional_encoding_decoder, encoder_output, target_mask)\n",
    "\n",
    "        # Projection\n",
    "        proj_output = self.linear(decoder_output)\n",
    "        return proj_output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescatamos las función que crea la máscara para el transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(sequence_len):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequence_len: length of sequence\n",
    "        \n",
    "    Returns:\n",
    "        mask matrix\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones((sequence_len, sequence_len)))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a definir la función que obtiene el embbeding más el positional encoding de BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "def extract_embeddings(input_sentences, model_name='bert-base-uncased'):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "    \n",
    "    # tokenización de lote\n",
    "    inputs = tokenizer(input_sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    token_embeddings = outputs[0]\n",
    "    \n",
    "    # Los embeddings posicionales están en la segunda capa de los embeddings de la arquitectura BERT\n",
    "    positional_encodings = model.embeddings.position_embeddings.weight[:token_embeddings.shape[1], :].detach().unsqueeze(0).repeat(token_embeddings.shape[0], 1, 1)\n",
    "\n",
    "    embeddings_with_positional_encoding = token_embeddings + positional_encodings\n",
    "\n",
    "    # convierte las IDs de los tokens a tokens\n",
    "    tokens = [tokenizer.convert_ids_to_tokens(input_id) for input_id in inputs['input_ids']]\n",
    "\n",
    "    return tokens, inputs['input_ids'], token_embeddings, positional_encodings, embeddings_with_positional_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una sentencia para el encoder y otra para el decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "sentence_encoder = \"I gave the dog a bone because it was hungry\"\n",
    "sentence_decoder = \"Le di un hueso al perro porque tenía hambre\"\n",
    "\n",
    "tokens_encoder, input_ids_encoder, token_embeddings_encoder, positional_encodings_encoder, embeddings_with_positional_encoding_encoder = extract_embeddings(sentence_encoder)\n",
    "tokens_decoder, input_ids_decoder, token_embeddings_decoder, positional_encodings_decoder, embeddings_with_positional_encoding_decoder = extract_embeddings(sentence_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un objeto `transformer` y obtenemos su salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src vocab_size: 30522, dim_embedding: 768, src max_sequence_len: 12, heads: 8, Nx: 6\n",
      "tgt vocab_size: 30522, dim_embedding: 768, tgt max_sequence_len: 16, heads: 8, Nx: 6\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = 30522\n",
    "tgt_vocab_size = 30522\n",
    "dim_embedding = max(token_embeddings_encoder.shape[-1], token_embeddings_decoder.shape[-1])\n",
    "src_max_sequence_len = token_embeddings_encoder.shape[1]\n",
    "tgt_max_sequence_len = token_embeddings_decoder.shape[1]\n",
    "heads = 8\n",
    "Nx = 6\n",
    "print(f\"src vocab_size: {src_vocab_size}, dim_embedding: {dim_embedding}, src max_sequence_len: {src_max_sequence_len}, heads: {heads}, Nx: {Nx}\")\n",
    "print(f\"tgt vocab_size: {tgt_vocab_size}, dim_embedding: {dim_embedding}, tgt max_sequence_len: {tgt_max_sequence_len}, heads: {heads}, Nx: {Nx}\")\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, src_max_sequence_len, tgt_max_sequence_len, dim_embedding, Nx, heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = create_mask(tgt_max_sequence_len)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 30522])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_output = transformer(input_ids_encoder, input_ids_decoder, target_mask=mask)\n",
    "transformer_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en el notebook anterior obtenemos 1x16x30522, es decir 1 batch, 16 tokens y 30522 posibles tokens del vocabulario"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
