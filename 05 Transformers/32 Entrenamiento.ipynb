{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero de todo cargamos los datos a los que le hicimos la limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "path = \"data/opus100_croped\"\n",
    "opus100_croped = load_from_disk(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a hacer una inspección rápida de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset keys: dict_keys(['test', 'train', 'validation'])\n"
     ]
    }
   ],
   "source": [
    "print(f\"dataset keys: {opus100_croped.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length: 983138,\n",
      "validation length: 1963,\n",
      "test length: 1955\n"
     ]
    }
   ],
   "source": [
    "print(f\"train length: {len(opus100_croped['train'])},\\nvalidation length: {len(opus100_croped['validation'])},\\ntest length: {len(opus100_croped['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train keys: dict_keys(['translation']),\n",
      "validation keys: dict_keys(['translation']),\n",
      "test keys: dict_keys(['translation'])\n"
     ]
    }
   ],
   "source": [
    "print(f\"train keys: {opus100_croped['train'][0].keys()},\\nvalidation keys: {opus100_croped['validation'][0].keys()},\\ntest keys: {opus100_croped['test'][0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train translation keys: dict_keys(['en', 'es']),\n",
      "validation translation keys: dict_keys(['en', 'es']),\n",
      "test translation keys: dict_keys(['en', 'es'])\n"
     ]
    }
   ],
   "source": [
    "print(f\"train translation keys: {opus100_croped['train'][0]['translation'].keys()},\\nvalidation translation keys: {opus100_croped['validation'][0]['translation'].keys()},\\ntest translation keys: {opus100_croped['test'][0]['translation'].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Come by around 7:30.', 'es': 'Ven a eso de las 7:30.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy.random as random\n",
    "idx = random.randint(0, len(opus100_croped[\"train\"]))\n",
    "opus100_croped[\"train\"][idx][\"translation\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset y dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos visto cómo es el dataset que nos hemos descargado, vamos a crear un dataset y un dataloader de python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero creamos la clase `Opus100Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Opus100Dataset(Dataset):\n",
    "    def __init__(self, dataset, source_language, target_language, tokenizer, start_token, end_token, padding_token, max_length):\n",
    "        self.dataset = dataset\n",
    "        self.source_language = source_language\n",
    "        self.target_language = target_language\n",
    "        self.tokenizer = tokenizer\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.padding_token = padding_token\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def encode(self, text):\n",
    "        encoded = self.tokenizer(text)\n",
    "        encoded = self.start_token + encoded + self.end_token\n",
    "        if len(encoded) > self.max_length:  # Truncate if too long\n",
    "            encoded = encoded[:self.max_length]\n",
    "        else:  # Pad if too short\n",
    "            encoded = encoded + self.padding_token * (self.max_length - len(encoded))\n",
    "        return torch.tensor(encoded)\n",
    "    \n",
    "    def decode(self, tensor, decoder):\n",
    "        end_token_position = (tensor == self.end_token[0]).nonzero(as_tuple=True)[0]\n",
    "        encoded_sentence = tensor[1:end_token_position].tolist()\n",
    "        return decoder(encoded_sentence)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = self.dataset[idx][\"translation\"][self.source_language]\n",
    "        source = self.encode(source)\n",
    "\n",
    "        target = self.dataset[idx][\"translation\"][self.target_language]\n",
    "        target = self.encode(target)\n",
    "        return source, target\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos un objeto para `train`, `validation` y `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "start_token = chr(1)\n",
    "start_token = encoder.encode(start_token)\n",
    "\n",
    "end_token = chr(2)\n",
    "end_token = encoder.encode(end_token)\n",
    "\n",
    "padding_token = chr(3)\n",
    "padding_token = encoder.encode(padding_token)\n",
    "\n",
    "max_secuence_length = 104 #128\n",
    "\n",
    "train_dataset = Opus100Dataset(opus100_croped[\"train\"], \"en\", \"es\", encoder.encode, start_token, end_token, padding_token, max_secuence_length)\n",
    "validation_dataset = Opus100Dataset(opus100_croped[\"validation\"], \"en\", \"es\", encoder.encode, start_token, end_token, padding_token, max_secuence_length)\n",
    "test_dataset = Opus100Dataset(opus100_croped[\"test\"], \"en\", \"es\", encoder.encode, start_token, end_token, padding_token, max_secuence_length)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver si las longitudes coinciden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len opus100 train 983138, len train_dataset 983138\n",
      "len opus100 validation 1963, len validation_dataset 1963\n",
      "len opus100 test 1955, len test_dataset 1955\n"
     ]
    }
   ],
   "source": [
    "print(f\"len opus100 train {len(opus100_croped['train'])}, len train_dataset {len(train_dataset)}\")\n",
    "print(f\"len opus100 validation {len(opus100_croped['validation'])}, len validation_dataset {len(validation_dataset)}\")\n",
    "print(f\"len opus100 test {len(opus100_croped['test'])}, len test_dataset {len(test_dataset)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora si una muestra de entrenamiento coincide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opus100_idx_en: So the more we see, the less we know., opus100_idx_es: Así que cuanto más vemos, menos sabemos.\n",
      "sample train_dataset en: So the more we see, the less we know., sample train_dataset es: Así que cuanto más vemos, menos sabemos.\n"
     ]
    }
   ],
   "source": [
    "idx = 259178 #random.randint(0, len(train_dataset))\n",
    "\n",
    "opus100_idx_en = opus100_croped[\"train\"][idx][\"translation\"][\"en\"]\n",
    "opus100_idx_es = opus100_croped[\"train\"][idx][\"translation\"][\"es\"]\n",
    "print(f\"opus100_idx_en: {opus100_idx_en}, opus100_idx_es: {opus100_idx_es}\")\n",
    "\n",
    "sample_train_dataset_idx_en, sample_train_dataset_idx_es = train_dataset[idx]\n",
    "sample_train_dataset_idx_en_sentence = train_dataset.decode(sample_train_dataset_idx_en, encoder.decode)\n",
    "sample_train_dataset_idx_es_sentence = train_dataset.decode(sample_train_dataset_idx_es, encoder.decode)\n",
    "print(f\"sample train_dataset en: {sample_train_dataset_idx_en_sentence}, sample train_dataset es: {sample_train_dataset_idx_es_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver si tiene la longitud que debería tener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train_dataset_idx_en.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ahora a ver muestras de validación y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opus100_idx_en: I want you alive!, opus100_idx_es: ¡Te quiero vivo!\n",
      "sample validation_dataset en: I want you alive!, sample validation_dataset es: ¡Te quiero vivo!\n"
     ]
    }
   ],
   "source": [
    "idx = 1724 #random.randint(0, len(validation_dataset))\n",
    "\n",
    "opus100_idx_en = opus100_croped[\"validation\"][idx][\"translation\"][\"en\"]\n",
    "opus100_idx_es = opus100_croped[\"validation\"][idx][\"translation\"][\"es\"]\n",
    "print(f\"opus100_idx_en: {opus100_idx_en}, opus100_idx_es: {opus100_idx_es}\")\n",
    "\n",
    "sample_validation_dataset_idx_en, sample_validation_dataset_idx_es = validation_dataset[idx]\n",
    "sample_validation_dataset_idx_en_sentence = validation_dataset.decode(sample_validation_dataset_idx_en, encoder.decode)\n",
    "sample_validation_dataset_idx_es_sentence = validation_dataset.decode(sample_validation_dataset_idx_es, encoder.decode)\n",
    "print(f\"sample validation_dataset en: {sample_validation_dataset_idx_en_sentence}, sample validation_dataset es: {sample_validation_dataset_idx_es_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opus100_idx_en: Most likely, this is Michael Farmer., opus100_idx_es: Lo más probable es que sea Michael Farmer.\n",
      "sample test_dataset en: Most likely, this is Michael Farmer., sample test_dataset es: Lo más probable es que sea Michael Farmer.\n"
     ]
    }
   ],
   "source": [
    "idx = 1044 #random.randint(0, len(test_dataset))\n",
    "\n",
    "opus100_idx_en = opus100_croped[\"test\"][idx][\"translation\"][\"en\"]\n",
    "opus100_idx_es = opus100_croped[\"test\"][idx][\"translation\"][\"es\"]\n",
    "print(f\"opus100_idx_en: {opus100_idx_en}, opus100_idx_es: {opus100_idx_es}\")\n",
    "\n",
    "sample_test_dataset_idx_en, sample_test_dataset_idx_es = test_dataset[idx]\n",
    "sample_test_dataset_idx_en_sentence = test_dataset.decode(sample_test_dataset_idx_en, encoder.decode)\n",
    "sample_test_dataset_idx_es_sentence = test_dataset.decode(sample_test_dataset_idx_es, encoder.decode)\n",
    "print(f\"sample test_dataset en: {sample_test_dataset_idx_en_sentence}, sample test_dataset es: {sample_test_dataset_idx_es_sentence}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el dataset está bien creado, ahora creamos el `Dataloader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BS = 4\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BS, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BS, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver una muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 1, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 2, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 3, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 4, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 5, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 6, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 7, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 8, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 9, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 10, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 11, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 12, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 13, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 14, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 15, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 16, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 17, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 18, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 19, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 20, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 21, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 22, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 23, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 24, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 25, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 26, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 27, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 28, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 29, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 30, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 31, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 32, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 33, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 34, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 35, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 36, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 37, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 38, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 39, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 40, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 41, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 42, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 43, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 44, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 45, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 46, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 47, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 48, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 49, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 50, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 51, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 52, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 53, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 54, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 55, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 56, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 57, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 58, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 59, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 60, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 61, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 62, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 63, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 64, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 65, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 66, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 67, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 68, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 69, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 70, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 71, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 72, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 73, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 74, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 75, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 76, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 77, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n",
      "batch: 78, source shape: torch.Size([4, 104]), target shape: torch.Size([4, 104])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [104] at entry 0 and [105] at entry 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m batch, (source, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch: \u001b[39m\u001b[39m{\u001b[39;00mbatch\u001b[39m}\u001b[39;00m\u001b[39m, source shape: \u001b[39m\u001b[39m{\u001b[39;00msource\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, target shape: \u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    139\u001b[0m         storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    140\u001b[0m         out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n\u001b[1;32m    142\u001b[0m \u001b[39melif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstr_\u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[1;32m    143\u001b[0m         \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstring_\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mndarray\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmemmap\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    145\u001b[0m         \u001b[39m# array of string classes and object\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [104] at entry 0 and [105] at entry 3"
     ]
    }
   ],
   "source": [
    "for batch, (source, target) in enumerate(train_dataloader):\n",
    "    print(f\"batch: {batch}, source shape: {source.shape}, target shape: {target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_en shape: torch.Size([4, 104]), batch_es shape: torch.Size([4, 104])\n"
     ]
    }
   ],
   "source": [
    "batch_en, batch_es = next(iter(train_dataloader))\n",
    "print(f\"batch_en shape: {batch_en.shape}, batch_es shape: {batch_es.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a escribir todo el código del transformer y creamos un objeto de este"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases de bajo nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of vocabulary\n",
    "            embed_dim: dimension of embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            out: embedding vector\n",
    "        \"\"\"\n",
    "        return self.embedding(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_sequence_len, embedding_model_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq_len: length of input sequence\n",
    "            embed_model_dim: demension of embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_model_dim\n",
    "\n",
    "        # create constant 'positional_encoding' matrix with values dependant on pos and i\n",
    "        positional_encoding = torch.zeros(max_sequence_len, self.embedding_dim)\n",
    "        for pos in range(max_sequence_len):\n",
    "            for i in range(0, self.embedding_dim, 2):\n",
    "                positional_encoding[pos, i]     = math.sin(pos / (10000 ** ((2 *     i) / self.embedding_dim)))\n",
    "                positional_encoding[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i+1)) / self.embedding_dim)))\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            x: output\n",
    "        \"\"\"\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.embedding_dim)\n",
    "        \n",
    "        # add encoding matrix to embedding (x)\n",
    "        sequence_len = x.size(1)\n",
    "        # x = x + torch.autograd.Variable(self.positional_encoding[:,:sequence_len], requires_grad=False)\n",
    "        x = x + self.positional_encoding[:,:sequence_len]\n",
    "        return x\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            key: key vector\n",
    "            query: query vector\n",
    "            value: value vector\n",
    "            mask: mask matrix (optional)\n",
    "        \n",
    "        Returns:\n",
    "            output vector from scaled dot product attention\n",
    "        \"\"\"\n",
    "        # MatMul\n",
    "        key_trasposed = key.transpose(-1,-2)\n",
    "        product = torch.matmul(query, key_trasposed)\n",
    "        # scale\n",
    "        scale = product / math.sqrt(self.dim_embedding)\n",
    "        # Mask (optional)\n",
    "        if mask is not None:\n",
    "            scale = scale.masked_fill(mask == 0, float('-inf'))\n",
    "        # softmax\n",
    "        attention_matrix = torch.nn.functional.softmax(scale, dim=-1)\n",
    "        # MatMul\n",
    "        output = torch.matmul(attention_matrix, value)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            heads: number of heads\n",
    "            dim_embedding: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.dim_proyection = dim_embedding // heads\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.proyection_Q = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.proyection_K = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.proyection_V = nn.Linear(dim_embedding, dim_embedding)\n",
    "        self.attention = nn.Linear(dim_embedding, dim_embedding)\n",
    "\n",
    "        self.scaled_dot_product_attention = ScaledDotProductAttention(self.dim_proyection)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q: query vector\n",
    "            K: key vector\n",
    "            V: value vector\n",
    "            mask: mask matrix (optional)\n",
    "\n",
    "        Returns:\n",
    "            output vector from multi-head attention\n",
    "        \"\"\"\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        proyection_Q = self.proyection_Q(Q).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_K = self.proyection_K(K).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_V = self.proyection_V(V).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "        proyection_Q = proyection_Q.transpose(1,2)\n",
    "        proyection_K = proyection_K.transpose(1,2)\n",
    "        proyection_V = proyection_V.transpose(1,2)\n",
    "\n",
    "        # calculate attention\n",
    "        scaled_dot_product_attention = self.scaled_dot_product_attention(proyection_Q, proyection_K, proyection_V, mask=mask)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scaled_dot_product_attention.transpose(1,2).contiguous().view(batch_size, -1, self.dim_embedding)\n",
    "        \n",
    "        output = self.attention(concat)\n",
    "    \n",
    "        return output\n",
    "\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_embedding (int): Embedding dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.normalization = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            sublayer (torch.Tensor): Sublayer tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return self.normalization(torch.add(x, sublayer))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_embedding, increment=4):\n",
    "        super().__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim_embedding, dim_embedding*increment),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_embedding*increment, dim_embedding)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch_size, seq_len, dim_embedding)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch_size, seq_len, dim_embedding)\n",
    "        \"\"\"\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, dim_embedding, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class Softmax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class Dropout(torch.nn.Module):\n",
    "    def __init__(self, p=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            p (float): probability of an element to be zeroed. Default: 0.1\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: a tensor with the same shape of `x`\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            return torch.nn.functional.dropout(x, p=self.p)\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases de medio nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(heads, dim_embedding)\n",
    "        self.dropout_1 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_1 = AddAndNorm(dim_embedding)\n",
    "        self.feed_forward = FeedForward(dim_embedding)\n",
    "        self.dropout_2 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_2 = AddAndNorm(dim_embedding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch_size, seq_len, dim_embedding)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch_size, seq_len, dim_embedding)\n",
    "        \"\"\"\n",
    "        multi_head_attention = self.multi_head_attention(x, x, x)\n",
    "        dropout1 = self.dropout_1(multi_head_attention)\n",
    "        add_and_norm_1 = self.add_and_norm_1(x, dropout1)\n",
    "        feed_forward = self.feed_forward(add_and_norm_1)\n",
    "        dropout2 = self.dropout_2(feed_forward)\n",
    "        add_and_norm_2 = self.add_and_norm_2(add_and_norm_1, dropout2)\n",
    "        return add_and_norm_2\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, Nx, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(heads, dim_embedding, prob_dropout) for _ in range(Nx)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch_size, seq_len, dim_embedding)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch_size, seq_len, dim_embedding)\n",
    "        \"\"\"\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, dim_embedding, max_sequence_len, heads, Nx, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_embedding = Embedding(vocab_size, dim_embedding)\n",
    "        self.positional_encoding = PositionalEncoding(max_sequence_len, dim_embedding)\n",
    "        self.encoder = Encoder(heads, dim_embedding, Nx, prob_dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch_size, seq_len, dim_embedding)\n",
    "        \"\"\"\n",
    "        input_embedding = self.input_embedding(x)\n",
    "        positional_encoding = self.positional_encoding(input_embedding)\n",
    "        encoder = self.encoder(positional_encoding)\n",
    "        return encoder\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, prob_dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            heads: number of heads\n",
    "            dim_embedding: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.masked_multi_head_attention = MultiHeadAttention(heads, dim_embedding)\n",
    "        self.dropout_1 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_1 = AddAndNorm(dim_embedding)\n",
    "        self.encoder_decoder_multi_head_attention = MultiHeadAttention(heads, dim_embedding)\n",
    "        self.dropout_2 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_2 = AddAndNorm(dim_embedding)\n",
    "        self.feed_forward = FeedForward(dim_embedding)\n",
    "        self.dropout_3 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_3 = AddAndNorm(dim_embedding)\n",
    "    \n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "            encoder_output: output vector from encoder\n",
    "            mask: mask matrix (optional)\n",
    "        \n",
    "        Returns:\n",
    "            output vector from decoder layer\n",
    "        \"\"\"\n",
    "        Q = x\n",
    "        K = x\n",
    "        V = x\n",
    "        masked_multi_head_attention = self.masked_multi_head_attention(Q, K, V, mask=mask)\n",
    "        dropout1 = self.dropout_1(masked_multi_head_attention)\n",
    "        add_and_norm_1 = self.add_and_norm_1(dropout1, x)\n",
    "\n",
    "        Q = add_and_norm_1\n",
    "        K = encoder_output\n",
    "        V = encoder_output\n",
    "        encoder_decoder_multi_head_attention = self.encoder_decoder_multi_head_attention(Q, K, V)\n",
    "        dropout2 = self.dropout_2(encoder_decoder_multi_head_attention)\n",
    "        add_and_norm_2 = self.add_and_norm_2(dropout2, add_and_norm_1)\n",
    "\n",
    "        feed_forward = self.feed_forward(add_and_norm_2)\n",
    "        dropout3 = self.dropout_3(feed_forward)\n",
    "        add_and_norm_3 = self.add_and_norm_3(dropout3, add_and_norm_2)\n",
    "\n",
    "        return add_and_norm_3\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, Nx, prob_dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            heads: number of heads\n",
    "            dim_embedding: dimension of embedding vector\n",
    "            Nx: number of decoder layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(heads, dim_embedding, prob_dropout) for _ in range(Nx)])\n",
    "    \n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "            encoder_output: output vector from encoder\n",
    "            mask: mask matrix (optional)\n",
    "        \n",
    "        Returns:\n",
    "            output vector from decoder\n",
    "        \"\"\"\n",
    "        for decoder_layer in self.layers:\n",
    "            x = decoder_layer(x, encoder_output, mask)\n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, Nx, vocab_size, max_sequence_len, prob_dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            heads: number of heads\n",
    "            dim_embedding: dimension of embedding vector\n",
    "            Nx: number of decoder layers\n",
    "            vocab_size: size of vocabulary\n",
    "            max_sequence_len: maximum length of sequence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, dim_embedding)\n",
    "        self.positional_encoding = PositionalEncoding(max_sequence_len, dim_embedding)\n",
    "        self.decoder = Decoder(heads, dim_embedding, Nx, prob_dropout)\n",
    "        self.linear = Linear(dim_embedding, vocab_size)\n",
    "        self.softmax = Softmax()\n",
    "    \n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "            encoder_output: output vector from encoder\n",
    "            mask: mask matrix (optional)\n",
    "        \n",
    "        Returns:\n",
    "            output vector from decoder\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.decoder(x, encoder_output, mask)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase de alto nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, dim_embedding, max_sequence_len, heads, Nx, prob_dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of vocabulary\n",
    "            dim_embedding: dimension of embedding vector\n",
    "            max_sequence_len: maximum length of sequence\n",
    "            heads: number of heads\n",
    "            Nx: number of decoder layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(vocab_size, dim_embedding, max_sequence_len, heads, Nx, prob_dropout)\n",
    "        self.decoder = TransformerDecoder(heads, dim_embedding, Nx, vocab_size, max_sequence_len, prob_dropout)\n",
    "    \n",
    "    def forward(self, source, target, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source: source vector\n",
    "            target: target vector\n",
    "\n",
    "        Returns:\n",
    "            output vector from decoder\n",
    "        \"\"\"\n",
    "        encoder_output = self.encoder(source)\n",
    "        decoder_output = self.decoder(target, encoder_output, mask)\n",
    "        return decoder_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(sequence_len):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequence_len: length of sequence\n",
    "        \n",
    "    Returns:\n",
    "        mask matrix\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones((sequence_len, sequence_len)))\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 100277, dim_embedding: 512, max_secuence_length: 104, heads: 8, Nx: 6, prob_dropout: 0.1\n"
     ]
    }
   ],
   "source": [
    "vocab_size = encoder.n_vocab\n",
    "dim_embedding = 512\n",
    "max_secuence_length = 104 #128\n",
    "heads = 8\n",
    "Nx = 6\n",
    "prob_dropout = 0.1\n",
    "print(f\"vocab_size: {vocab_size}, dim_embedding: {dim_embedding}, max_secuence_length: {max_secuence_length}, heads: {heads}, Nx: {Nx}, prob_dropout: {prob_dropout}\")\n",
    "\n",
    "transformer = Transformer(vocab_size=vocab_size,\n",
    "                          dim_embedding=dim_embedding,\n",
    "                          max_sequence_len=max_secuence_length,\n",
    "                          heads=heads,\n",
    "                          Nx=Nx,\n",
    "                          prob_dropout=prob_dropout)\n",
    "\n",
    "mask = create_mask(max_secuence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar el modelo con una muestra del dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample sentence train_dataset en: So the more we see, the less we know.\n",
      "sample sentence train_dataset es: Así que cuanto más vemos, menos sabemos.\n",
      "\n",
      "sample train_dataset en: tensor([ 189, 4516,  279,  810,  584, 1518,   11,  279, 2753,  584, 1440,   13,\n",
      "         190,  191,  191,  191,  191,  191,  191,  191,  191,  191,  191,  191,\n",
      "         191,  191,  191,  191,  191,  191,  191,  191,  191,  191,  191,  191,\n",
      "         191,  191,  191,  191,  191,  191,  191,  191,  191,  191,  191,  191,\n",
      "         191,  191,  191,  191,  191,  191,  191,  191,  191,  191,  191,  191,\n",
      "         191,  191,  191,  191,  191,  191,  191,  191,  191,  191,  191,  191,\n",
      "         191,  191,  191,  191,  191,  191,  191,  191,  191,  191,  191,  191,\n",
      "         191,  191,  191,  191,  191,  191,  191,  191,  191,  191,  191,  191,\n",
      "         191,  191,  191,  191,  191,  191,  191,  191]), shape: torch.Size([104])\n",
      "sample train_dataset es: tensor([  189,  2170,  2483,  1744, 87587, 11158,   348, 15295,    11, 32895,\n",
      "        19972, 15295,    13,   190,   191,   191,   191,   191,   191,   191,\n",
      "          191,   191,   191,   191,   191,   191,   191,   191,   191,   191,\n",
      "          191,   191,   191,   191,   191,   191,   191,   191,   191,   191,\n",
      "          191,   191,   191,   191,   191,   191,   191,   191,   191,   191,\n",
      "          191,   191,   191,   191,   191,   191,   191,   191,   191,   191,\n",
      "          191,   191,   191,   191,   191,   191,   191,   191,   191,   191,\n",
      "          191,   191,   191,   191,   191,   191,   191,   191,   191,   191,\n",
      "          191,   191,   191,   191,   191,   191,   191,   191,   191,   191,\n",
      "          191,   191,   191,   191,   191,   191,   191,   191,   191,   191,\n",
      "          191,   191,   191,   191]), shape: torch.Size([104])\n"
     ]
    }
   ],
   "source": [
    "idx = 259178 #random.randint(0, len(train_dataset))\n",
    "\n",
    "sample_train_dataset_idx_en, sample_train_dataset_idx_es = train_dataset[idx]\n",
    "sample_train_dataset_idx_en_sentence = train_dataset.decode(sample_train_dataset_idx_en, encoder.decode)\n",
    "sample_train_dataset_idx_es_sentence = train_dataset.decode(sample_train_dataset_idx_es, encoder.decode)\n",
    "print(f\"sample sentence train_dataset en: {sample_train_dataset_idx_en_sentence}\")\n",
    "print(f\"sample sentence train_dataset es: {sample_train_dataset_idx_es_sentence}\")\n",
    "print(\"\")\n",
    "print(f\"sample train_dataset en: {sample_train_dataset_idx_en}, shape: {sample_train_dataset_idx_en.shape}\")\n",
    "print(f\"sample train_dataset es: {sample_train_dataset_idx_es}, shape: {sample_train_dataset_idx_es.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 104, 100277])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = sample_train_dataset_idx_en.unsqueeze(0)\n",
    "target = sample_train_dataset_idx_es.unsqueeze(0)\n",
    "output = transformer(source, target, mask)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_en shape: torch.Size([4, 104]), batch_es shape: torch.Size([4, 104])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 104, 100277])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_en, batch_es = next(iter(train_dataloader))\n",
    "print(f\"batch_en shape: {batch_en.shape}, batch_es shape: {batch_es.shape}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_en = batch_en.to(device)\n",
    "batch_es = batch_es.to(device)\n",
    "mask = mask.to(device)\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "output = transformer(batch_en, batch_es, mask)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss(ignore_index=padding_token[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ciclo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, device, mask):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X, y, mask)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, device, mask):\n",
    "    # size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            pred = model(X, y, mask)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "transformer = transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [4, 100277], got [4, 104]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     train_loop(train_dataloader, transformer, loss_function, optimizer, device, mask)\n\u001b[1;32m      5\u001b[0m     test_loop(validation_dataloader, transformer, loss_function, device, mask)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 10\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, device, mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# Compute prediction error\u001b[39;00m\n\u001b[1;32m      9\u001b[0m pred \u001b[39m=\u001b[39m model(X, y, mask)\n\u001b[0;32m---> 10\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniconda3/envs/cursopytorch/lib/python3.10/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [4, 100277], got [4, 104]"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, transformer, loss_function, optimizer, device, mask)\n",
    "    test_loop(validation_dataloader, transformer, loss_function, device, mask)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
