{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos entrenado nuestro transformer podemos probar a ver qué tal lo hace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases de bajo nivel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos primero a implementar un transformer con todo el código que hemos usado antes, primero escribimos las funciones de bajo nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(CustomLinear, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        init.kaiming_uniform_(self.linear.weight, nonlinearity='relu')\n",
    "        if self.linear.bias is not None:\n",
    "            init.zeros_(self.linear.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class CustomEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(CustomEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        init.xavier_uniform_(self.embedding.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = CustomEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_sequence_len, embedding_model_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_model_dim\n",
    "        positional_encoding = torch.zeros(max_sequence_len, self.embedding_dim)\n",
    "        for pos in range(max_sequence_len):\n",
    "            for i in range(0, self.embedding_dim, 2):\n",
    "                positional_encoding[pos, i]     = torch.sin(torch.tensor(pos / (10000 ** ((2 * i) / self.embedding_dim))))\n",
    "                positional_encoding[pos, i + 1] = torch.cos(torch.tensor(pos / (10000 ** ((2 * (i+1)) / self.embedding_dim))))\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * torch.sqrt(torch.tensor(self.embedding_dim))\n",
    "        sequence_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:,:sequence_len]\n",
    "        return x\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        key_trasposed = key.transpose(-1,-2)\n",
    "        product = torch.matmul(query, key_trasposed)\n",
    "        scale = product / torch.sqrt(torch.tensor(self.dim_embedding))\n",
    "        if mask is not None:\n",
    "            scale = scale.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_matrix = torch.softmax(scale, dim=-1)\n",
    "        output = torch.matmul(attention_matrix, value)\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.dim_proyection = dim_embedding // heads\n",
    "        self.heads = heads\n",
    "        self.proyection_Q = CustomLinear(dim_embedding, dim_embedding)\n",
    "        self.proyection_K = CustomLinear(dim_embedding, dim_embedding)\n",
    "        self.proyection_V = CustomLinear(dim_embedding, dim_embedding)\n",
    "        self.scaled_dot_product_attention = ScaledDotProductAttention(self.dim_proyection)\n",
    "        self.attention = CustomLinear(dim_embedding, dim_embedding)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        proyection_Q = self.proyection_Q(Q).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_K = self.proyection_K(K).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_V = self.proyection_V(V).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_Q = proyection_Q.transpose(1,2)\n",
    "        proyection_K = proyection_K.transpose(1,2)\n",
    "        proyection_V = proyection_V.transpose(1,2)\n",
    "        scaled_dot_product_attention = self.scaled_dot_product_attention(proyection_Q, proyection_K, proyection_V, mask=mask)\n",
    "        concat = scaled_dot_product_attention.transpose(1,2).contiguous().view(batch_size, -1, self.dim_embedding)\n",
    "        output = self.attention(concat)\n",
    "        return output\n",
    "\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        super().__init__()\n",
    "        self.normalization = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return self.normalization(torch.add(x, sublayer))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_embedding, increment=4):\n",
    "        super().__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            CustomLinear(dim_embedding, dim_embedding*increment),\n",
    "            nn.ReLU(),\n",
    "            CustomLinear(dim_embedding*increment, dim_embedding)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, dim_embedding, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = CustomLinear(dim_embedding, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class Softmax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class Dropout(torch.nn.Module):\n",
    "    def __init__(self, p=0.1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            return torch.nn.functional.dropout(x, p=self.p)\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases de medio nivel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora las clases de medio nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(heads, dim_embedding)\n",
    "        self.dropout_1 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_1 = AddAndNorm(dim_embedding)\n",
    "        self.feed_forward = FeedForward(dim_embedding)\n",
    "        self.dropout_2 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_2 = AddAndNorm(dim_embedding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        multi_head_attention = self.multi_head_attention(x, x, x)\n",
    "        dropout1 = self.dropout_1(multi_head_attention)\n",
    "        add_and_norm_1 = self.add_and_norm_1(x, dropout1)\n",
    "        feed_forward = self.feed_forward(add_and_norm_1)\n",
    "        dropout2 = self.dropout_2(feed_forward)\n",
    "        add_and_norm_2 = self.add_and_norm_2(add_and_norm_1, dropout2)\n",
    "        return add_and_norm_2\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, Nx, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(heads, dim_embedding, prob_dropout) for _ in range(Nx)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_multi_head_attention = MultiHeadAttention(heads, dim_embedding)\n",
    "        self.dropout_1 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_1 = AddAndNorm(dim_embedding)\n",
    "        self.encoder_decoder_multi_head_attention = MultiHeadAttention(heads, dim_embedding)\n",
    "        self.dropout_2 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_2 = AddAndNorm(dim_embedding)\n",
    "        self.feed_forward = FeedForward(dim_embedding)\n",
    "        self.dropout_3 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_3 = AddAndNorm(dim_embedding)\n",
    "    \n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        Q = x\n",
    "        K = x\n",
    "        V = x\n",
    "        masked_multi_head_attention = self.masked_multi_head_attention(Q, K, V, mask=mask)\n",
    "        dropout1 = self.dropout_1(masked_multi_head_attention)\n",
    "        add_and_norm_1 = self.add_and_norm_1(dropout1, x)\n",
    "\n",
    "        Q = add_and_norm_1\n",
    "        K = encoder_output\n",
    "        V = encoder_output\n",
    "        encoder_decoder_multi_head_attention = self.encoder_decoder_multi_head_attention(Q, K, V)\n",
    "        dropout2 = self.dropout_2(encoder_decoder_multi_head_attention)\n",
    "        add_and_norm_2 = self.add_and_norm_2(dropout2, add_and_norm_1)\n",
    "\n",
    "        feed_forward = self.feed_forward(add_and_norm_2)\n",
    "        dropout3 = self.dropout_3(feed_forward)\n",
    "        add_and_norm_3 = self.add_and_norm_3(dropout3, add_and_norm_2)\n",
    "\n",
    "        return add_and_norm_3\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, Nx, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(heads, dim_embedding, prob_dropout) for _ in range(Nx)])\n",
    "    \n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        for decoder_layer in self.layers:\n",
    "            x = decoder_layer(x, encoder_output, mask)\n",
    "        return x\n",
    "\n",
    "class Linear_and_softmax(nn.Module):\n",
    "    def __init__(self, dim_embedding, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = CustomLinear(dim_embedding, vocab_size)\n",
    "        # self.softmax = Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases de alto nivel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por último la clase transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, src_max_seq_len, tgt_max_seq_len, dim_embedding, Nx, heads, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(heads, dim_embedding, Nx, prob_dropout)\n",
    "        self.decoder = Decoder(heads, dim_embedding, Nx, prob_dropout)\n",
    "        self.sourceEmbedding = Embedding(src_vocab_size, dim_embedding)\n",
    "        self.targetEmbedding = Embedding(tgt_vocab_size, dim_embedding)\n",
    "        self.sourcePositional_encoding = PositionalEncoding(src_max_seq_len, dim_embedding)\n",
    "        self.targetPositional_encoding = PositionalEncoding(tgt_max_seq_len, dim_embedding)\n",
    "        self.linear = Linear_and_softmax(dim_embedding, tgt_vocab_size)\n",
    "    \n",
    "    def encode(self, source):\n",
    "        embedding = self.sourceEmbedding(source)\n",
    "        positional_encoding = self.sourcePositional_encoding(embedding)\n",
    "        encoder_output = self.encoder(positional_encoding)\n",
    "        return encoder_output\n",
    "    \n",
    "    def decode(self, encoder_output, target, target_mask):\n",
    "        embedding = self.targetEmbedding(target)\n",
    "        positional_encoding = self.targetPositional_encoding(embedding)\n",
    "        decoder_output = self.decoder(positional_encoding, encoder_output, target_mask)\n",
    "        return decoder_output\n",
    "    \n",
    "    def projection(self, decoder_output):\n",
    "        linear_output = self.linear(decoder_output)\n",
    "        # softmax_output = self.softmax(linear_output)\n",
    "        return linear_output\n",
    "    \n",
    "    def forward(self, source, target, target_mask):\n",
    "        # Encode\n",
    "        embedding_encoder = self.sourceEmbedding(source)\n",
    "        positional_encoding_encoder = self.sourcePositional_encoding(embedding_encoder)\n",
    "        encoder_output = self.encoder(positional_encoding_encoder)\n",
    "\n",
    "        # Decode\n",
    "        embedding_decoder = self.targetEmbedding(target)\n",
    "        positional_encoding_decoder = self.targetPositional_encoding(embedding_decoder)\n",
    "        decoder_output = self.decoder(positional_encoding_decoder, encoder_output, target_mask)\n",
    "\n",
    "        # Projection\n",
    "        proj_output = self.linear(decoder_output)\n",
    "        return proj_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Máscara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la máscara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(sequence_len):\n",
    "    mask = torch.tril(torch.ones((1, sequence_len, sequence_len)))\n",
    "    return mask\n",
    "\n",
    "max_secuence_length = 10 + 2\n",
    "mask = create_mask(max_secuence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(f\"cuda\")\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los tokenizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer_source_path = \"tokenizers/tokenizer_en.json\"\n",
    "tokenizer_target_path = \"tokenizers/tokenizer_es.json\"\n",
    "\n",
    "tokenizer_source = Tokenizer.from_file(tokenizer_source_path)\n",
    "tokenizer_target = Tokenizer.from_file(tokenizer_target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora creamos un objeto del transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source vocab size: 30000, target vocab size: 30000, source max sequence len: 22, target max sequence len: 22, dim_embedding: 512, heads: 8, Nx: 6, prob_dropout: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de 90.25 millones de parámetros\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "source_vocab_size = tokenizer_source.get_vocab_size()\n",
    "target_vocab_size = tokenizer_target.get_vocab_size()\n",
    "max_sequence_len = 22\n",
    "src_max_seq_len = max_sequence_len\n",
    "tgt_max_seq_len = max_sequence_len\n",
    "dim_embedding = 512\n",
    "Nx = 6\n",
    "heads = 8\n",
    "prob_dropout = 0.1\n",
    "print(f\"source vocab size: {source_vocab_size}, target vocab size: {target_vocab_size}, source max sequence len: {src_max_seq_len}, target max sequence len: {tgt_max_seq_len}, dim_embedding: {dim_embedding}, heads: {heads}, Nx: {Nx}, prob_dropout: {prob_dropout}\")\n",
    "\n",
    "transformer = Transformer(\n",
    "    src_vocab_size = source_vocab_size,\n",
    "    tgt_vocab_size = target_vocab_size,\n",
    "    src_max_seq_len = src_max_seq_len,\n",
    "    tgt_max_seq_len = tgt_max_seq_len,\n",
    "    dim_embedding = dim_embedding,\n",
    "    Nx = Nx,\n",
    "    heads = heads,\n",
    "    prob_dropout = prob_dropout,\n",
    ")\n",
    "\n",
    "transformer.to(device)\n",
    "print(f\"Modelo de {(sum(p.numel() for p in transformer.parameters())/1e6):.2f} millones de parámetros\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de encoder y decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero definimos los tokens especiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknow_token: 0, padding_token: 1, start_token: 2, end_token: 3\n"
     ]
    }
   ],
   "source": [
    "unknow_token = '[UNK]'\n",
    "unknow_token = tokenizer_source.token_to_id(unknow_token)\n",
    "\n",
    "padding_token = '[PAD]'\n",
    "padding_token = tokenizer_source.token_to_id(padding_token)\n",
    "\n",
    "start_token = '[SOS]'\n",
    "start_token = tokenizer_source.token_to_id(start_token)\n",
    "\n",
    "end_token = '[EOS]'\n",
    "end_token = tokenizer_source.token_to_id(end_token)\n",
    "unknow_token, start_token, end_token\n",
    "\n",
    "print(f\"unknow_token: {unknow_token}, padding_token: {padding_token}, start_token: {start_token}, end_token: {end_token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definimos las funciones para codificar y decodificar sentencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence, tokenizer, start_token, end_token, pad_token, max_length, device):\n",
    "    # start with SOS\n",
    "    encode_sentence_list = [start_token]\n",
    "\n",
    "    # encode sentence\n",
    "    encode_sentence = tokenizer.encode(sentence)\n",
    "\n",
    "    # Add to list\n",
    "    encode_sentence_list.extend(encode_sentence.ids)\n",
    "\n",
    "    # end with EOS\n",
    "    encode_sentence_list = encode_sentence_list + [end_token]\n",
    "\n",
    "    # if the sentence is less than max_length, we add padding tokens\n",
    "    if len(encode_sentence_list) < max_length:\n",
    "        encode_sentence_list = encode_sentence_list + [pad_token] * (max_length - len(encode_sentence_list))\n",
    "    \n",
    "    # if the sentence is greater than max_length, we truncate\n",
    "    else:\n",
    "        encode_sentence_list = encode_sentence_list[:max_length]\n",
    "    \n",
    "    # convert to tensor\n",
    "    encode_sentence_tensor = torch.tensor([encode_sentence_list]).to(device)\n",
    "\n",
    "    return encode_sentence, encode_sentence_list, encode_sentence_tensor\n",
    "\n",
    "def decode_sentence(tokens, tokenizer, start_token, end_token):\n",
    "    # Decode raw sentence\n",
    "    decode_raw_sentence = tokenizer.decode(tokens)\n",
    "\n",
    "    # Remove padding tokens\n",
    "    tokens = [token for token in tokens if token != padding_token]\n",
    "\n",
    "    # Remove SOS and EOS tokens\n",
    "    tokens = [token for token in tokens if token != start_token and token != end_token]\n",
    "\n",
    "    # Decode\n",
    "    decode_sentence = tokenizer.decode(tokens)\n",
    "\n",
    "    return decode_raw_sentence, decode_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos las funciones con una sentencia en inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode english sentence: [11, 32, 1999, 12, 372, 45, 29, 320]\n",
      "Encode english sentence with SOS, EOS and padding: [2, 11, 32, 1999, 12, 372, 45, 29, 320, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "English sencence shape: torch.Size([1, 22])\n",
      "\n",
      "Decode raw english encoded sentence: I have learned a lot from this course\n",
      "Decode english encoded sentence: I have learned a lot from this course\n"
     ]
    }
   ],
   "source": [
    "sentence_en = \"I have learned a lot from this course\"\n",
    "encode_sentence_en, encode_sentence_list_en, encode_sentence_tensor_en = encode_sentence(\n",
    "    sentence_en,\n",
    "    tokenizer_source,\n",
    "    start_token,\n",
    "    end_token,\n",
    "    padding_token,\n",
    "    src_max_seq_len,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(f\"Encode english sentence: {encode_sentence_en.ids}\")\n",
    "print(f\"Encode english sentence with SOS, EOS and padding: {encode_sentence_list_en}\")\n",
    "print(f\"English sencence shape: {encode_sentence_tensor_en.shape}\")\n",
    "\n",
    "decode_raw_sentence_en, decode_sentence_en = decode_sentence(\n",
    "    encode_sentence_list_en,\n",
    "    tokenizer_source,\n",
    "    start_token,\n",
    "    end_token\n",
    ")\n",
    "print(f\"\\nDecode raw english encoded sentence: {decode_raw_sentence_en}\")\n",
    "print(f\"Decode english encoded sentence: {decode_sentence_en}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora con una sentencia en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode spanish sentence: [417, 4915, 19, 1847, 26, 60, 1199]\n",
      "Encode spanish sentence with SOS, EOS and padding: [2, 417, 4915, 19, 1847, 26, 60, 1199, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Spanish sencence shape: torch.Size([1, 22])\n",
      "\n",
      "Decode raw spanish encoded sentence: He aprendido un montón con este curso\n",
      "Decode spanish encoded sentence: He aprendido un montón con este curso\n"
     ]
    }
   ],
   "source": [
    "sentence_es = \"He aprendido un montón con este curso\"\n",
    "encode_sentence_es, encode_sentence_list_es, encode_sentence_tensor_es = encode_sentence(\n",
    "    sentence_es,\n",
    "    tokenizer_target,\n",
    "    start_token,\n",
    "    end_token,\n",
    "    padding_token,\n",
    "    tgt_max_seq_len,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(f\"Encode spanish sentence: {encode_sentence_es.ids}\")\n",
    "print(f\"Encode spanish sentence with SOS, EOS and padding: {encode_sentence_list_es}\")\n",
    "print(f\"Spanish sencence shape: {encode_sentence_tensor_es.shape}\")\n",
    "\n",
    "decode_raw_sentence_es, decode_sentence_es = decode_sentence(\n",
    "    encode_sentence_list_es,\n",
    "    tokenizer_target,\n",
    "    start_token,\n",
    "    end_token\n",
    ")\n",
    "print(f\"\\nDecode raw spanish encoded sentence: {decode_raw_sentence_es}\")\n",
    "print(f\"Decode spanish encoded sentence: {decode_sentence_es}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de generación de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_generate(source, start_token, end_token, model, device, max_len):\n",
    "    # Output of transformer encoder\n",
    "    encoder_output = model.encode(source)\n",
    "\n",
    "    # Input to transformer decoder\n",
    "    decoder_input = torch.empty(1,1).fill_(start_token).type_as(source).to(device)\n",
    "    \n",
    "    # Looping until the 'max_len' is reached or the End of Sentence token is generated\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "            \n",
    "        # Building a mask for the decoder\n",
    "        decoder_mask = create_mask(decoder_input.size(1)).to(device)\n",
    "        \n",
    "        # Calculating the output of the decoder\n",
    "        decoder_output = model.decode(encoder_output, decoder_input, decoder_mask)\n",
    "        \n",
    "        # Applying the projection layer to get the probabilities for the next token\n",
    "        prob = model.projection(decoder_output[:, -1])\n",
    "        \n",
    "        # Selecting token with the highest probability\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "        \n",
    "        # If the next token is an End of Sentence token, we finish the loop\n",
    "        if next_word == end_token:\n",
    "            break\n",
    "            \n",
    "    return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencia con el modelo sin entrenar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la sentencia que queremos traducir y la tokenizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence: I have learned a lot from this course\n",
      "Encode english sentence: [2, 417, 4915, 19, 1847, 26, 60, 1199, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "sentence_en = \"I have learned a lot from this course\"\n",
    "encode_sentence_en, encode_sentence_list_en, encode_sentence_tensor_en = encode_sentence(\n",
    "    sentence_en,\n",
    "    tokenizer_source,\n",
    "    start_token,\n",
    "    end_token,\n",
    "    padding_token,\n",
    "    src_max_seq_len,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(f\"English sentence: {sentence_en}\")\n",
    "print(f\"Encode english sentence: {encode_sentence_list_es}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos la secuencia por el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode translated sentence: [2, 12510, 22301, 12510, 26848, 12510, 24650, 22301, 12510, 24650, 22301, 26848]\n"
     ]
    }
   ],
   "source": [
    "encode_translated = greedy_generate(\n",
    "    encode_sentence_tensor_en,\n",
    "    start_token,\n",
    "    end_token,\n",
    "    transformer,\n",
    "    device,\n",
    "    max_secuence_length\n",
    ").detach().cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "print(f\"Encode translated sentence: {encode_translated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decodificamos la salida del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: banca Criterios banca desplazada banca paces Criterios banca paces Criterios desplazada\n"
     ]
    }
   ],
   "source": [
    "decode_raw_sentence_es, decode_sentence_es = decode_sentence(\n",
    "    encode_translated,\n",
    "    tokenizer_target,\n",
    "    start_token,\n",
    "    end_token\n",
    ")\n",
    "\n",
    "print(f\"Translated sentence: {decode_raw_sentence_es}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, con el modelo sin entrenar obtenemos algo que no tiene sentido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencia con modelo entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weights = \"model/transformer_137_0.pth\"\n",
    "transformer = torch.load(weights, map_location='cpu')\n",
    "\n",
    "if isinstance(transformer, nn.DataParallel):\n",
    "    print(\"DataParallel\")\n",
    "    transformer = transformer.module\n",
    "\n",
    "transformer.to(device)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la sentencia que queremos traducir y la tokenizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence: I have learned a lot from this course\n",
      "Encode english sentence: [2, 417, 4915, 19, 1847, 26, 60, 1199, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "sentence_en = \"I have learned a lot from this course\"\n",
    "encode_sentence_en, encode_sentence_list_en, encode_sentence_tensor_en = encode_sentence(\n",
    "    sentence_en,\n",
    "    tokenizer_source,\n",
    "    start_token,\n",
    "    end_token,\n",
    "    padding_token,\n",
    "    src_max_seq_len,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(f\"English sentence: {sentence_en}\")\n",
    "print(f\"Encode english sentence: {encode_sentence_list_es}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos la secuencia por el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode translated sentence: [2, 15801, 142, 126, 316, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "encode_translated = greedy_generate(\n",
    "    encode_sentence_tensor_en,\n",
    "    start_token,\n",
    "    end_token,\n",
    "    transformer,\n",
    "    device,\n",
    "    tgt_max_seq_len\n",
    ").detach().cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "print(f\"Encode translated sentence: {encode_translated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decodificamos la salida del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: Aprendí mucho desde entonces .\n"
     ]
    }
   ],
   "source": [
    "decode_raw_sentence_es, decode_sentence_es = decode_sentence(\n",
    "    encode_translated,\n",
    "    tokenizer_target,\n",
    "    start_token,\n",
    "    end_token\n",
    ")\n",
    "\n",
    "print(f\"Translated sentence: {decode_raw_sentence_es}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tiene mucho más sentido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de traducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función a la que le metemos una sentencia y nos la devuelve traducida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator(\n",
    "        sentence_source,\n",
    "        tokenizer_source,\n",
    "        tokenizer_target,\n",
    "        start_token,\n",
    "        end_token,\n",
    "        padding_token,\n",
    "        src_max_seq_len,\n",
    "        tgt_max_seq_len,\n",
    "        model,\n",
    "        device,\n",
    "):\n",
    "    _, _, encode_sentence_tensor_source = encode_sentence(\n",
    "        sentence_source,\n",
    "        tokenizer_source,\n",
    "        start_token,\n",
    "        end_token,\n",
    "        padding_token,\n",
    "        src_max_seq_len,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    encode_translated = greedy_generate(\n",
    "        encode_sentence_tensor_source,\n",
    "        start_token,\n",
    "        end_token,\n",
    "        model,\n",
    "        device,\n",
    "        tgt_max_seq_len\n",
    "    ).detach().cpu().numpy().tolist()\n",
    "\n",
    "    decode_raw_sentence_es, _ = decode_sentence(\n",
    "        encode_translated,\n",
    "        tokenizer_target,\n",
    "        start_token,\n",
    "        end_token\n",
    "    )\n",
    "\n",
    "    return decode_raw_sentence_es\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cojo una secuencia cualquiera, cojo un comentario de la función `greedy_decode` del notebook de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de máscara para construir una máscara'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Building a mask for the decoder input\"\n",
    "translator(sentence,\n",
    "           tokenizer_source,\n",
    "           tokenizer_target,\n",
    "           start_token,\n",
    "           end_token,\n",
    "           padding_token,\n",
    "           src_max_seq_len,\n",
    "           tgt_max_seq_len,\n",
    "           transformer,\n",
    "           device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque no lo ha hecho perfecto, traduce algo con sentido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí hay dos cosas que decir\n",
    "\n",
    "La primera es que no hemos entrenado el mejor traductor del mundo, hemos cogido un dataset pequeño, del cual nos hemos quedado con una pequeña parte de ese dataset para poder entrenarlo en una GPU, incluso habrá gente que no lo pueda entrenar en su propia GPU. Por lo que sin buenos datos no se obtienen buenos modelos. Pero como he dicho en todo el momento en este curso, el objetivo no ha sido entrenar los mejores modelos, sino que aprendáis los fundamentos del deep learning\n",
    "\n",
    "Por otro lado, estamos generando la traducción con una generación de tokens llamada `greedy search` la cual no es la mejor, ya que produce salidas muy repetitivas. Veamos un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de la salida de la salida de la salida'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Calculating the output of the decoder\"\n",
    "translator(sentence,\n",
    "           tokenizer_source,\n",
    "           tokenizer_target,\n",
    "           start_token,\n",
    "           end_token,\n",
    "           padding_token,\n",
    "           src_max_seq_len,\n",
    "           tgt_max_seq_len,\n",
    "           transformer,\n",
    "           device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver aquí la traducción es muy repetitiva. Cuando usamos LLMs como chatGPT, Llama, etc. lo que suelen hacer para generar tokens son técnicas como temperature, top-k sampling, top-p sampling, etc. que son técnicas que permiten generar tokens de manera más diversa. Pero eso ya se escapa del contenido de este curso, así que te dejo unos enlaces a esas técnicas por si quieres profundizar en ellas\n",
    "\n",
    " * [Temperature](https://maximofn.com/hugging-face-transformers/#Sampling-temperature)\n",
    " * [Top-k sampling](https://maximofn.com/hugging-face-transformers/#Sampling-top-k)\n",
    " * [Top-p sampling](https://maximofn.com/hugging-face-transformers/#Sampling-top-p-(nucleus-sampling))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
