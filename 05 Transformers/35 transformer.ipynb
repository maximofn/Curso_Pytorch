{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Transformer From Scratch With PyTorch](https://www.kaggle.com/code/lusfernandotorres/transformer-from-scratch-with-pytorch?scriptVersionId=157489239)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Math\n",
    "import math\n",
    "\n",
    "# HuggingFace libraries \n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Pathlib \n",
    "from pathlib import Path\n",
    "\n",
    "# typing\n",
    "from typing import Any\n",
    "\n",
    "# Library for progress bars in loops\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing library of warnings\n",
    "import warnings\n",
    "\n",
    "MI_TRANSFORMER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_TRANSFORMER = False\n",
    "\n",
    "# Creating Input Embeddings\n",
    "class InputEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Dimension of vectors (512)\n",
    "        self.vocab_size = vocab_size # Size of the vocabulary\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # PyTorch layer that converts integer indices to dense embeddings\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if MI_TRANSFORMER:\n",
    "            return self.embedding(x)\n",
    "        else:\n",
    "            return self.embedding(x) * math.sqrt(self.d_model) # Normalizing the variance of the embeddings\n",
    "\n",
    "MI_TRANSFORMER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_TRANSFORMER = False\n",
    "\n",
    "if MI_TRANSFORMER:\n",
    "    class PositionalEncoding(nn.Module):\n",
    "        def __init__(self, max_sequence_len, embedding_model_dim, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.embedding_dim = embedding_model_dim\n",
    "            positional_encoding = torch.zeros(max_sequence_len, self.embedding_dim)\n",
    "            for pos in range(max_sequence_len):\n",
    "                for i in range(0, self.embedding_dim, 2):\n",
    "                    positional_encoding[pos, i]     = torch.sin(torch.tensor(pos / (10000 ** ((2 * i) / self.embedding_dim))))\n",
    "                    positional_encoding[pos, i + 1] = torch.cos(torch.tensor(pos / (10000 ** ((2 * (i+1)) / self.embedding_dim))))\n",
    "            positional_encoding = positional_encoding.unsqueeze(0)\n",
    "            self.register_buffer('positional_encoding', positional_encoding)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x * torch.sqrt(torch.tensor(self.embedding_dim))\n",
    "            sequence_len = x.size(1)\n",
    "            x = x + self.positional_encoding[:,:sequence_len]\n",
    "            return x\n",
    "else:\n",
    "    # Creating the Positional Encoding\n",
    "    class PositionalEncoding(nn.Module):\n",
    "        \n",
    "        def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.d_model = d_model # Dimensionality of the model\n",
    "            self.seq_len = seq_len # Maximum sequence length\n",
    "            self.dropout = nn.Dropout(dropout) # Dropout layer to prevent overfitting\n",
    "            \n",
    "            # Creating a positional encoding matrix of shape (seq_len, d_model) filled with zeros\n",
    "            pe = torch.zeros(seq_len, d_model) \n",
    "            \n",
    "            # Creating a tensor representing positions (0 to seq_len - 1)\n",
    "            position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) # Transforming 'position' into a 2D tensor['seq_len, 1']\n",
    "            \n",
    "            # Creating the division term for the positional encoding formula\n",
    "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "            \n",
    "            # Apply sine to even indices in pe\n",
    "            pe[:, 0::2] = torch.sin(position * div_term)\n",
    "            # Apply cosine to odd indices in pe\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "            \n",
    "            # Adding an extra dimension at the beginning of pe matrix for batch handling\n",
    "            pe = pe.unsqueeze(0)\n",
    "            \n",
    "            # Registering 'pe' as buffer. Buffer is a tensor not considered as a model parameter\n",
    "            self.register_buffer('pe', pe) \n",
    "            \n",
    "        def forward(self,x):\n",
    "            # Addind positional encoding to the input tensor X\n",
    "            x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "            return self.dropout(x) # Dropout for regularization\n",
    "\n",
    "MI_TRANSFORMER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Layer Normalization\n",
    "class LayerNormalization(nn.Module):\n",
    "    \n",
    "    def __init__(self, eps: float = 10**-6) -> None: # We define epsilon as 0.000001 to avoid division by zero\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "        # We define alpha as a trainable parameter and initialize it with ones\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) # One-dimensional tensor that will be used to scale the input data\n",
    "        \n",
    "        # We define bias as a trainable parameter and initialize it with zeros\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) # One-dimensional tenso that will be added to the input data\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True) # Computing the mean of the input data. Keeping the number of dimensions unchanged\n",
    "        std = x.std(dim = -1, keepdim = True) # Computing the standard deviation of the input data. Keeping the number of dimensions unchanged\n",
    "        \n",
    "        # Returning the normalized input\n",
    "        return self.alpha * (x-mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Feed Forward Layers\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # First linear transformation\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # W1 & b1\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout to prevent overfitting\n",
    "        # Second linear transformation\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # W2 & b2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (Batch, seq_len, d_model) --> (batch, seq_len, d_ff) -->(batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Multi-Head Attention block\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None: # h = number of heads\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        \n",
    "        # We ensure that the dimensions of the model is divisible by the number of heads\n",
    "        assert d_model % h == 0, 'd_model is not divisible by h'\n",
    "        \n",
    "        # d_k is the dimension of each attention head's key, query, and value vectors\n",
    "        self.d_k = d_model // h # d_k formula, like in the original \"Attention Is All You Need\" paper\n",
    "        \n",
    "        # Defining the weight matrices\n",
    "        self.w_q = nn.Linear(d_model, d_model) # W_q\n",
    "        self.w_k = nn.Linear(d_model, d_model) # W_k\n",
    "        self.w_v = nn.Linear(d_model, d_model) # W_v\n",
    "        self.w_o = nn.Linear(d_model, d_model) # W_o\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer to avoid overfitting\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):# mask => When we want certain words to NOT interact with others, we \"hide\" them\n",
    "        \n",
    "        d_k = query.shape[-1] # The last dimension of query, key, and value\n",
    "        \n",
    "        # We calculate the Attention(Q,K,V) as in the formula in the image above \n",
    "        attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k) # @ = Matrix multiplication sign in PyTorch\n",
    "        \n",
    "        # Before applying the softmax, we apply the mask to hide some interactions between words\n",
    "        if mask is not None: # If a mask IS defined...\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9) # Replace each value where mask is equal to 0 by -1e9\n",
    "        attention_scores = attention_scores.softmax(dim = -1) # Applying softmax\n",
    "        if dropout is not None: # If a dropout IS defined...\n",
    "            attention_scores = dropout(attention_scores) # We apply dropout to prevent overfitting\n",
    "            \n",
    "        return (attention_scores @ value), attention_scores # Multiply the output matrix by the V matrix, as in the formula\n",
    "        \n",
    "    def forward(self, q, k, v, mask): \n",
    "        \n",
    "        query = self.w_q(q) # Q' matrix\n",
    "        key = self.w_k(k) # K' matrix\n",
    "        value = self.w_v(v) # V' matrix\n",
    "        \n",
    "        \n",
    "        # Splitting results into smaller matrices for the different heads\n",
    "        # Splitting embeddings (third dimension) into h parts\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
    "        \n",
    "        # Obtaining the output and the attention scores\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Obtaining the H matrix\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        \n",
    "        return self.w_o(x) # Multiply the H matrix by the weight matrix W_o, resulting in the MH-A matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Residual Connection\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout) # We use a dropout layer to prevent overfitting\n",
    "        self.norm = LayerNormalization() # We use a normalization layer \n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        # We normalize the input and add it to the original input 'x'. This creates the residual connection process.\n",
    "        return x + self.dropout(sublayer(self.norm(x))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Encoder Block\n",
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    # This block takes in the MultiHeadAttentionBlock and FeedForwardBlock, as well as the dropout rate for the residual connections\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # Storing the self-attention block and feed-forward block\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)]) # 2 Residual Connections with dropout\n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        # Applying the first residual connection with the self-attention block\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask)) # Three 'x's corresponding to query, key, and value inputs plus source mask\n",
    "        \n",
    "        # Applying the second residual connection with the feed-forward block \n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x # Output tensor after applying self-attention and feed-forward layers with residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Encoder \n",
    "# An Encoder can have several Encoder Blocks\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    # The Encoder takes in instances of 'EncoderBlock'\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers # Storing the EncoderBlocks\n",
    "        self.norm = LayerNormalization() # Layer for the normalization of the output of the encoder layers\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # Iterating over each EncoderBlock stored in self.layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask) # Applying each EncoderBlock to the input tensor 'x'\n",
    "        return self.norm(x) # Normalizing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Decoder Block\n",
    "class DecoderBlock(nn.Module):\n",
    "    \n",
    "    # The DecoderBlock takes in two MultiHeadAttentionBlock. One is self-attention, while the other is cross-attention.\n",
    "    # It also takes in the feed-forward block and the dropout rate\n",
    "    def __init__(self,  self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)]) # List of three Residual Connections with dropout rate\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \n",
    "        # Self-Attention block with query, key, and value plus the target language mask\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        \n",
    "        # The Cross-Attention block using two 'encoder_ouput's for key and value plus the source language mask. It also takes in 'x' for Decoder queries\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        \n",
    "        # Feed-forward block with residual connections\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Decoder\n",
    "# A Decoder can have several Decoder Blocks\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    # The Decoder takes in instances of 'DecoderBlock'\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # Storing the 'DecoderBlock's\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization() # Layer to normalize the output\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \n",
    "        # Iterating over each DecoderBlock stored in self.layers\n",
    "        for layer in self.layers:\n",
    "            # Applies each DecoderBlock to the input 'x' plus the encoder output and source and target masks\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x) # Returns normalized output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buiding Linear Layer\n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None: # Model dimension and the size of the output vocabulary\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size) # Linear layer for projecting the feature space of 'd_model' to the output space of 'vocab_size'\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.proj(x), dim = -1) # Applying the log Softmax function to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Transformer Architecture\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    # This takes in the encoder and decoder, as well the embeddings for the source and target language.\n",
    "    # It also takes in the Positional Encoding for the source and target language, as well as the projection layer\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "        \n",
    "    # Encoder     \n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src) # Applying source embeddings to the input source language\n",
    "        src = self.src_pos(src) # Applying source positional encoding to the source embeddings\n",
    "        return self.encoder(src, src_mask) # Returning the source embeddings plus a source mask to prevent attention to certain elements\n",
    "    \n",
    "    # Decoder\n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt) # Applying target embeddings to the input target language (tgt)\n",
    "        tgt = self.tgt_pos(tgt) # Applying target positional encoding to the target embeddings\n",
    "        \n",
    "        # Returning the target embeddings, the output of the encoder, and both source and target masks\n",
    "        # The target mask ensures that the model won't 'see' future elements of the sequence\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    # Applying Projection Layer with the Softmax function to the Decoder output\n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building & Initializing Transformer\n",
    "\n",
    "# Definin function and its parameter, including model dimension, number of encoder and decoder stacks, heads, etc.\n",
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "    \n",
    "    # Creating Embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size) # Source language (Source Vocabulary to 512-dimensional vectors)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size) # Target language (Target Vocabulary to 512-dimensional vectors)\n",
    "    \n",
    "    # Creating Positional Encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout) # Positional encoding for the source language embeddings\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout) # Positional encoding for the target language embeddings\n",
    "    \n",
    "    # Creating EncoderBlocks\n",
    "    encoder_blocks = [] # Initial list of empty EncoderBlocks\n",
    "    for _ in range(N): # Iterating 'N' times to create 'N' EncoderBlocks (N = 6)\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "        \n",
    "        # Combine layers into an EncoderBlock\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block) # Appending EncoderBlock to the list of EncoderBlocks\n",
    "        \n",
    "    # Creating DecoderBlocks\n",
    "    decoder_blocks = [] # Initial list of empty DecoderBlocks\n",
    "    for _ in range(N): # Iterating 'N' times to create 'N' DecoderBlocks (N = 6)\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Cross-Attention\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "        \n",
    "        # Combining layers into a DecoderBlock\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block) # Appending DecoderBlock to the list of DecoderBlocks\n",
    "        \n",
    "    # Creating the Encoder and Decoder by using the EncoderBlocks and DecoderBlocks lists\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Creating projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size) # Map the output of Decoder to the Target Vocabulary Space\n",
    "    \n",
    "    # Creating the transformer by combining everything above\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "            \n",
    "    return transformer # Assembled and initialized Transformer. Ready to be trained and validated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Tokenizer\n",
    "def build_tokenizer(config, ds, lang):\n",
    "    \n",
    "    # Crating a file path for the tokenizer \n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    \n",
    "    # Checking if Tokenizer already exists\n",
    "    if not Path.exists(tokenizer_path): \n",
    "        \n",
    "        # If it doesn't exist, we create a new one\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]')) # Initializing a new world-level tokenizer\n",
    "        tokenizer.pre_tokenizer = Whitespace() # We will split the text into tokens based on whitespace\n",
    "        \n",
    "        # Creating a trainer for the new tokenizer\n",
    "        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\", \n",
    "                                                     \"[SOS]\", \"[EOS]\"], min_frequency = 2) # Defining Word Level strategy and special tokens\n",
    "        \n",
    "        # Training new tokenizer on sentences from the dataset and language specified \n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n",
    "        tokenizer.save(str(tokenizer_path)) # Saving trained tokenizer to the file path specified at the beginning of the function\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path)) # If the tokenizer already exist, we load it\n",
    "    return tokenizer # Returns the loaded tokenizer or the trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through dataset to extract the original sentence and its translation \n",
    "def get_all_sentences(ds, lang):\n",
    "    for pair in ds:\n",
    "        yield pair['translation'][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(config):\n",
    "    \n",
    "    # Loading the train portion of the OpusBooks dataset.\n",
    "    # The Language pairs will be defined in the 'config' dictionary we will build later\n",
    "    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split = 'train') \n",
    "    \n",
    "    # Building or loading tokenizer for both the source and target languages \n",
    "    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "    \n",
    "    # Splitting the dataset for training and validation \n",
    "    train_ds_size = int(0.9 * len(ds_raw)) # 90% for training\n",
    "    val_ds_size = len(ds_raw) - train_ds_size # 10% for validation\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) # Randomly splitting the dataset\n",
    "                                    \n",
    "    # Processing data with the BilingualDataset class, which we will define below\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "                                    \n",
    "    # Iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "    for pair in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_src.encode(pair['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "        \n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "    \n",
    "    # Creating dataloaders for the training and validadion sets\n",
    "    # Dataloaders are used to iterate over the dataset in batches during training and validation\n",
    "    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True) # Batch size will be defined in the config dictionary\n",
    "    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt # Returning the DataLoader objects and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casual_mask(size):\n",
    "        # Creating a square matrix of dimensions 'size x size' filled with ones\n",
    "        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
    "        return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "    \n",
    "    # This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages, and the strings of source and target languages\n",
    "    # 'seq_len' defines the sequence length for both languages\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        \n",
    "        # Defining special tokens by using the target language tokenizer\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "        \n",
    "    # Total number of instances in the dataset (some pairs are larger than others)\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    # Using the index to retrive source and target texts\n",
    "    def __getitem__(self, index: Any) -> Any:\n",
    "        src_target_pair = self.ds[index]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "        \n",
    "        # Tokenizing source and target texts \n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "        \n",
    "        # Computing how many padding tokens need to be added to the tokenized texts \n",
    "        # Source tokens\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n",
    "        # Target tokens\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # Subtracting the '[SOS]' special token\n",
    "        \n",
    "        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n",
    "        # given the current sequence length limit (this will be defined in the config dictionary below)\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError('Sentence is too long')\n",
    "         \n",
    "        # Building the encoder input tensor by combining several elements\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "            self.sos_token, # inserting the '[SOS]' token\n",
    "            torch.tensor(enc_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n",
    "            self.eos_token, # Inserting the '[EOS]' token\n",
    "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Building the decoder input tensor by combining several elements\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token, # inserting the '[SOS]' token \n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "        \n",
    "        )\n",
    "        \n",
    "        # Creating a label tensor, the expected output for training the model\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                self.eos_token, # Inserting the '[EOS]' token \n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n",
    "                \n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "        \n",
    "        return {\n",
    "            'encoder_input': encoder_input,\n",
    "            'decoder_input': decoder_input, \n",
    "            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)), \n",
    "            'label': label,\n",
    "            'src_text': src_text,\n",
    "            'tgt_text': tgt_text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to obtain the most probable next token\n",
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    # Retrieving the indices from the start and end of sequences of the target tokens\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "    \n",
    "    # Computing the output of the encoder for the source sequence\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initializing the decoder input with the Start of Sentence token\n",
    "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
    "    \n",
    "    # Looping until the 'max_len', maximum length, is reached\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "            \n",
    "        # Building a mask for the decoder input\n",
    "        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "        \n",
    "        # Calculating the output of the decoder\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "        \n",
    "        # Applying the projection layer to get the probabilities for the next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        \n",
    "        # Selecting token with the highest probability\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "        \n",
    "        # If the next token is an End of Sentence token, we finish the loop\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "            \n",
    "    return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to evaluate the model on the validation dataset\n",
    "# num_examples = 2, two examples per run\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
    "    model.eval() # Setting model to evaluation mode\n",
    "    count = 0 # Initializing counter to keep track of how many examples have been processed\n",
    "    \n",
    "    console_width = 80 # Fixed witdh for printed messages\n",
    "    \n",
    "    # Creating evaluation loop\n",
    "    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "            \n",
    "            # Ensuring that the batch_size of the validation set is 1\n",
    "            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n",
    "            \n",
    "            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "            \n",
    "            # Retrieving source and target texts from the batch\n",
    "            source_text = batch['src_text'][0]\n",
    "            target_text = batch['tgt_text'][0] # True translation \n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n",
    "            \n",
    "            # Printing results\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f'SOURCE: {source_text}')\n",
    "            print_msg(f'TARGET: {target_text}')\n",
    "            print_msg(f'PREDICTED: {model_out_text}')\n",
    "            \n",
    "            # After two examples, we break the loop\n",
    "            if count == num_examples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pass as parameters the config dictionary, the length of the vocabylary of the source language and the target language\n",
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    \n",
    "    # Loading model using the 'build_transformer' function.\n",
    "    # We will use the lengths of the source language and target language vocabularies, the 'seq_len', and the dimensionality of the embeddings\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define settings for building and training the transformer model\n",
    "def get_config():\n",
    "    return{\n",
    "        'batch_size': 16,\n",
    "        'num_epochs': 20,\n",
    "        'lr': 10**-4,\n",
    "        'seq_len': 350,\n",
    "        'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n",
    "        'lang_src': 'en',\n",
    "        'lang_tgt': 'it',\n",
    "        'model_folder': 'weights',\n",
    "        'model_basename': 'tmodel_',\n",
    "        'preload': None,\n",
    "        'tokenizer_file': 'tokenizer_{0}.json',\n",
    "        'experiment_name': 'runs/tmodel'\n",
    "    }\n",
    "    \n",
    "\n",
    "# Function to construct the path for saving and retrieving model weights\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = config['model_folder'] # Extracting model folder from the config\n",
    "    model_basename = config['model_basename'] # Extracting the base name for model files\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n",
    "    return str(Path('.')/ model_folder/ model_filename) # Combining current directory, the model folder, and the model filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Setting up device to run on GPU to train faster\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device {device}\")\n",
    "    \n",
    "    # Creating model directory to store weights\n",
    "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Retrieving dataloaders and tokenizers for source and target languages using the 'get_ds' function\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    \n",
    "    # Initializing model on the GPU using the 'get_model' function\n",
    "    model = get_model(config,tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    \n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "    \n",
    "    # Setting up the Adam optimizer with the specified learning rate from the '\n",
    "    # config' dictionary plus an epsilon value\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n",
    "    \n",
    "    # Initializing epoch and global step variables\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    \n",
    "    # Checking if there is a pre-trained model to load\n",
    "    # If true, loads it\n",
    "    if config['preload']:\n",
    "        model_filename = get_weights_file_path(config, config['preload'])\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename) # Loading model\n",
    "        \n",
    "        # Sets epoch to the saved in the state plus one, to resume from where it stopped\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        # Loading the optimizer state from the saved model\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        # Loading the global step state from the saved model\n",
    "        global_step = state['global_step']\n",
    "        \n",
    "    # Initializing CrossEntropyLoss function for training\n",
    "    # We ignore padding tokens when computing loss, as they are not relevant for the learning process\n",
    "    # We also apply label_smoothing to prevent overfitting\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n",
    "    \n",
    "    # Initializing training loop \n",
    "    \n",
    "    # Iterating over each epoch from the 'initial_epoch' variable up to\n",
    "    # the number of epochs informed in the config\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        \n",
    "        # Initializing an iterator over the training dataloader\n",
    "        # We also use tqdm to display a progress bar\n",
    "        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n",
    "        \n",
    "        # For each batch...\n",
    "        for batch in batch_iterator:\n",
    "            model.train() # Train the model\n",
    "            \n",
    "            # Loading input data and masks onto the GPU\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            decoder_input = batch['decoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)\n",
    "            \n",
    "            # Running tensors through the Transformer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = model.project(decoder_output)\n",
    "            \n",
    "            # Loading the target labels onto the GPU\n",
    "            label = batch['label'].to(device)\n",
    "            \n",
    "            # Computing loss between model's output and true labels\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            \n",
    "            # Updating progress bar\n",
    "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
    "            \n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "            \n",
    "            # Performing backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Updating parameters based on the gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Clearing the gradients to prepare for the next batch\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            global_step += 1 # Updating global step count\n",
    "            \n",
    "        # We run the 'run_validation' function at the end of each epoch\n",
    "        # to evaluate model performance\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "         \n",
    "        # Saving model\n",
    "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
    "        # Writting current model state to the 'model_filename'\n",
    "        torch.save({\n",
    "            'epoch': epoch, # Current epoch\n",
    "            'model_state_dict': model.state_dict(),# Current model state\n",
    "            'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n",
    "            'global_step': global_step # Current global step \n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config() # Retrieving config settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset opus_books (/home/wallabot/.cache/huggingface/datasets/opus_books/en-it/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of source sentence: 309\n",
      "Max length of target sentence: 274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 00: 100%|██████████| 1819/1819 [18:43<00:00,  1.62it/s, loss=6.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: But for that...'\n",
      "TARGET: Ma per questo...\n",
      "PREDICTED: — Sì .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: She was standing among that group, very erect as usual, and was talking to the master of the house with her head slightly turned toward him, when Kitty approached.\n",
      "TARGET: Stava in piedi, tenendosi, come sempre, straordinariamente diritta e quando Kitty si avvicinò al gruppo, parlava col padrone di casa volgendo lieve il capo verso di lui.\n",
      "PREDICTED: — Ma , , , — disse , — disse , — disse , — disse , — disse , e e e e non .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 01: 100%|██████████| 1819/1819 [18:09<00:00,  1.67it/s, loss=5.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: But, as I was saying: sitting in that window-seat, do you think of nothing but your future school?\n",
      "TARGET: Ma quando state alla finestra, pensate forse alla vostra futura scuola?\n",
      "PREDICTED: E , non era la sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: '- come to the following: adultery of husband or wife and the detection of the guilty party by mutual consent, or involuntary detection without such consent.\n",
      "TARGET: — Si riducono ai seguenti: adulterio di uno dei coniugi e prova della colpevolezza di una delle parti risultante da reciproco accordo, oppure, fuori di un tale accordo, prova legale.\n",
      "PREDICTED: Il sua momento , e la sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 02: 100%|██████████| 1819/1819 [18:52<00:00,  1.61it/s, loss=6.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Whatever I do with its cage, I cannot get at it--the savage, beautiful creature!\n",
      "TARGET: Anche se m'impadronissi della gabbia, non potrei trattenere il bell'uccello selvaggio.\n",
      "PREDICTED: Non non ho detto , ma non mi , e non mi .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Thus it happens that whenever those who are hostile have the opportunity to attack they do it like partisans, whilst the others defend lukewarmly, in such wise that the prince is endangered along with them.\n",
      "TARGET: Donde nasce che qualunque volta quelli che sono nimici hanno occasione di assaltare, lo fanno partigianamente, e quelli altri defendano tepidamente; in modo che insieme con loro si periclita.\n",
      "PREDICTED: Non sono , non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 03: 100%|██████████| 1819/1819 [17:50<00:00,  1.70it/s, loss=5.916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Yes, I am under the painful necessity of applying for a divorce,' he continued.\n",
      "TARGET: — Sì, sono stato messo nella penosa necessità di esigere il divorzio — egli disse.\n",
      "PREDICTED: — Non è , ma non è mai nulla , ma non è mai nulla .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'She particularly wished me to go and see her,' continued Anna. 'I shall be glad to see the old lady again, and will go to-morrow.\n",
      "TARGET: — Mi ha pregato tanto di andare da lei — continuò Anna — e io sono contenta di vedere quella vecchietta, e domani ci andrò.\n",
      "PREDICTED: — Non è , ma non è mai mai mai , ma non è mai nulla , ma non è mai un ’ altra , e non è mai nulla .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 04: 100%|██████████| 1819/1819 [17:04<00:00,  1.77it/s, loss=5.916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Do you accept my solution of the mystery?\"\n",
      "TARGET: Accettate la mia spiegazione?\n",
      "PREDICTED: E non vi , ma non vi ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: And interlacing his fingers, palms downwards, he stretched them and the joints cracked.\n",
      "TARGET: E incrociate le dita le une nelle altre, con le palme all’ingiù, Aleksej Aleksandrovic le stiracchiò e le dita scricchiolarono nelle giunture.\n",
      "PREDICTED: E , dopo la sua cosa , e , e , si , e si .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 05: 100%|██████████| 1819/1819 [17:04<00:00,  1.78it/s, loss=6.033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'But one can't talk to Kitty about it!\n",
      "TARGET: — Con Kitty non se ne può parlare!\n",
      "PREDICTED: — Ma non è nulla ? — chiese .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'No.'\n",
      "TARGET: — No.\n",
      "PREDICTED: — Sì , signore .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 06: 100%|██████████| 1819/1819 [17:04<00:00,  1.78it/s, loss=5.352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: But when I had done this, I was unable to stir it up again, or to get under it, much less to move it forward towards the water; so I was forced to give it over; and yet, though I gave over the hopes of the boat, my desire to venture over for the main increased, rather than decreased, as the means for it seemed impossible.\n",
      "TARGET: Ma raggiunta questa meta, mi trovai nuovamente inabile a moverla, a mettermici sotto, tanto più poi a spingerla in acqua, onde finalmente fui costretto ad abbandonar la mia impresa. Pure, anche perdute tutte le speranze ch’io avea riposte nella scialuppa, cresceva in me, anzichè diminuire, il desiderio d’avventurarmi verso la terra comparsami innanzi, e crescea con tanta maggior forza quanto più impossibile ne apparivano i mezzi.\n",
      "PREDICTED: Ma io mi a me , e a me , e a a , e a , e a a , e a , e , a , che non poteva essere più più più , e che non poteva , e non mi , e non mi , e non mi , e non mi di , e che non poteva , e non mi di .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: To his regret he felt that he was himself sound and unhurt.\n",
      "TARGET: Per sua disgrazia sentiva d’essere incolume e sano.\n",
      "PREDICTED: La principessa era un ’ espressione , e la sua voce .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 07: 100%|██████████| 1819/1819 [17:04<00:00,  1.77it/s, loss=5.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Venus had risen above the branch and the car of the Great Bear as well as its shafts showed clearly against the dark blue sky, but he still waited.\n",
      "TARGET: — Non è ora? — chiese Stepan Arkad’ic.\n",
      "PREDICTED: Il signor Rochester era un po ’ di riposo , e , dopo aver fatto un ’ altra volta , che si , e si .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Koznyshev when he got home and went again over all his reasons, came to the conclusion that at first he had judged wrongly.\n",
      "TARGET: Ritornando a casa ed esaminando tutti gli argomenti, Sergej Ivanovic scoprì che non aveva ragionato in modo giusto.\n",
      "PREDICTED: Levin , che aveva detto , era stato più più , e non poteva dire che non poteva fare .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 08: 100%|██████████| 1819/1819 [17:04<00:00,  1.77it/s, loss=5.310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: At the station of a big town the Volunteers were again greeted with songs and cheers; again women and men turned up with collecting-boxes, the provincial ladies presented nosegays and accompanied the Volunteers to the refreshment-bar; but all this was far feebler and weaker than in Moscow.\n",
      "TARGET: A una grande stazione di una città, di nuovo canti e grida accolsero i volontari, apparvero di nuovo raccoglitrici e raccoglitori di offerte con le cassette, e le signore del capoluogo del governatorato offrirono fasci di fiori ai volontari e li seguirono al ristorante; ma tutto questo era in tono molto più debole e in proporzioni minori che non a Mosca.\n",
      "PREDICTED: Dopo un ’ altra volta , che era stato stato stato stato , e , in un momento , in cui , in quel momento , si era stato un ’ altra volta , e , come , per un attimo , si era un ’ altra volta , e che non si era ancora più più più più più più più più più .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Before going to Moscow, she – being an adept at dressing on comparatively little money – left three dresses to be altered.\n",
      "TARGET: Prima della sua partenza per Mosca ella, che in genere era abilissima nel vestirsi senza spendere eccessivamente, aveva dato a rimodernare tre abiti alla sarta.\n",
      "PREDICTED: La principessa , che la sua Aleksandrovic era stata stata stata stata stata stata stata stata stata stata più , e la sua vita era stata stata stata stata stata stata stata stata stata stata più .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 09: 100%|██████████| 1819/1819 [17:04<00:00,  1.78it/s, loss=5.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The boulevard and children.\n",
      "TARGET: Il viale e i bambini.\n",
      "PREDICTED: Il signor Rochester era molto .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: So Alice began telling them her adventures from the time when she first saw the White Rabbit. She was a little nervous about it just at first, the two creatures got so close to her, one on each side, and opened their eyes and mouths so very wide, but she gained courage as she went on.\n",
      "TARGET: Così Alice cominciò a raccontare i suoi casi, dal momento dell'incontro col Coniglio bianco; ma tosto si cominciò a sentire un po' a disagio, chè le due bestie le si stringevano da un lato e l'altro, spalancando gli occhi e le bocche; ma la bambina poco dopo riprese coraggio.\n",
      "PREDICTED: Il signor Rochester si alzò e si mise a guardare il suo volto , e poi , dopo aver guardato la porta , si alzò a guardare il cappello , e poi , , si a guardare il cappello , e poi , , si a guardare .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 10: 100%|██████████| 1819/1819 [17:04<00:00,  1.78it/s, loss=4.592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Anna, who had forgotten her agitation while she was working, stood at a table in the sitting-room packing her hand-bag when Annushka drew her attention to the noise of approaching carriage wheels.\n",
      "TARGET: Anna, che per il lavoro dei preparativi aveva abbandonato l’interna agitazione, approntava la sua sacca da viaggio stando in piedi accanto alla tavola nello studio, quando Annuška ne attirò l’attenzione verso un rumore di vettura che s’avvicinava.\n",
      "PREDICTED: Anna , che aveva visto il viso , si alzò , e , il cappello , si alzò , si alzò , e , , si mise a guardare il cappello , si alzò a guardare il cappello .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: No: stillness returned: each murmur and movement ceased gradually, and in about an hour Thornfield Hall was again as hushed as a desert. It seemed that sleep and night had resumed their empire.\n",
      "TARGET: La calma rinacque, tutte le voci e i rumori tacquero a poco a poco, Thornfield pareva di nuovo un deserto, sul quale imperasse la notte e il silenzio.\n",
      "PREDICTED: , e , con la sua , e poi , dopo aver la sua , e la sua , si , e si mise a guardare .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 11: 100%|██████████| 1819/1819 [17:04<00:00,  1.78it/s, loss=5.435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Turning now to the opposite characters of Commodus, Severus, Antoninus Caracalla, and Maximinus, you will find them all cruel and rapacious-men who, to satisfy their soldiers, did not hesitate to commit every kind of iniquity against the people; and all, except Severus, came to a bad end; but in Severus there was so much valour that, keeping the soldiers friendly, although the people were oppressed by him, he reigned successfully; for his valour made him so much admired in the sight of the soldiers and people that the latter were kept in a way astonished and awed and the former respectful and satisfied.\n",
      "TARGET: Discorrendo ora, per opposito, le qualità di Commodo, di Severo, Antonino Caracalla e Massimino, li troverrete crudelissimi e rapacissimi; li quali, per satisfare a' soldati, non perdonorono ad alcuna qualità di iniuria che ne' populi si potessi commettere; e tutti, eccetto Severo, ebbono triste fine. Perché in Severo fu tanta virtù, che, mantenendosi soldati amici, ancora che populi fussino da lui gravati, possé sempre regnare felicemente; perché quelle sua virtù lo facevano nel conspetto de' soldati e de' populi sí mirabile, che questi rimanevano quodammodo attoniti e stupidi, e quelli altri reverenti e satisfatti.\n",
      "PREDICTED: La prima cosa , che si , è , e ' tempi , non è più difficile , e ' tempi , che ti , e ' tempi , e non ti , e ' tempi , e ' tempi , che si , e ' tempi , e ' tempi , e ' tempi , che ti , e ' tempi , e ' tempi , e ' tempi , e ' tempi , e ' tempi , che ti , e ' tempi , e ' tempi , e ' grandi , e ' principi , e ' principi , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Masha has worn me out.\n",
      "TARGET: — Maša mi ha tormentato.\n",
      "PREDICTED: — È la mia mano .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 12: 100%|██████████| 1819/1819 [17:04<00:00,  1.78it/s, loss=5.126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Ryabinin looked at it and smiled, shaking his head.\n",
      "TARGET: Rjabinin, guardando verso questa, scosse la testa con un sorriso.\n",
      "PREDICTED: La porta e la baciò con la mano .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: At this point, Mrs. Poppets knocked at the door to know if we were ready for supper.\n",
      "TARGET: A questo punto picchiò all’uscio la signora Poppets per sapere se non volessimo andare a cena.\n",
      "PREDICTED: E , per quanto ci , si , si a guardare .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 13: 100%|██████████| 1819/1819 [17:04<00:00,  1.78it/s, loss=4.268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Quickly!'\n",
      "TARGET: Presto.\n",
      "PREDICTED: !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Probably at Oblonsky's indication, he looked round to where Koznyshev and the Princess were standing and silently raised his hat.\n",
      "TARGET: Probabilmente, per indicazione di Oblonskij, egli si voltò a guardare dalla parte dove stavano la principessa e Sergej Ivanovic, e sollevò il cappello in silenzio.\n",
      "PREDICTED: E Stepan Arkad ’ ic , che aveva visto il tè , si alzò e si alzò e si mise a guardare il capo .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 14: 100%|██████████| 1819/1819 [17:04<00:00,  1.78it/s, loss=4.808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: THOUGH VRONSKY'S WHOLE INNER LIFE WAS ABSORBED by his passion, his external life ran unalterably and inevitably along its former customary rails of social and regimental connections and interests.\n",
      "TARGET: Sebbene la vita intima di Vronskij fosse tutta piena della sua passione, la sua vita esteriore si svolgeva immutata e immutabile sull’abituale carreggiata di prima, tra i rapporti e gli interessi del gran mondo e del reggimento.\n",
      "PREDICTED: La sua relazione era stata stata stata stata stata stata stata stata eccitata , e la sua attività di famiglia , la , la e la di tutti i suoi affari .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Well, what's to be done!\n",
      "TARGET: Ma che fare!\n",
      "PREDICTED: E che cosa è accaduto !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 15: 100%|██████████| 1819/1819 [17:04<00:00,  1.78it/s, loss=4.664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"They have no mother?\"\n",
      "TARGET: — Non hanno madre?\n",
      "PREDICTED: — È vero ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: An austere patriot's passion for his fatherland!\n",
      "TARGET: Passione austera di un patriotta per la patria!\n",
      "PREDICTED: Il signor Rochester , che ha dato la sua opinione .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 16: 100%|██████████| 1819/1819 [17:02<00:00,  1.78it/s, loss=4.662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I am singled out and separated, But I am singled out, too, from as it were, from all the world, all the ship’s crew, to be spared to be miserable. from death; and He that miraculously saved me from death can deliver me from this condition.\n",
      "TARGET: Io solo forse tra tutti gli uomini sono stato scelto a menare una vita di una miseria senza pari. Ma così solo io sono stato scelto tra tutti quelli del naviglio a scampare dalla morte; e colui che dalla morte mi ha salvato in modo di miracolo mi può liberare dallo stato in cui sono.\n",
      "PREDICTED: Io sono felice e , e non mi più di quanto fosse stato , come se fossi stato , se non mi fossi reso un ’ altra volta , e che non avrei avuto altro , e che non avrei potuto .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'What? From Nilsson?' asked Betsy, quite horrified, though she could not have distinguished Nilsson's voice from that of a chorus girl.\n",
      "TARGET: — E non ascoltate la Nilsson? — chiese con orrore Betsy che non avrebbe saputo in nessun modo distinguere la Nilsson da una qualsiasi corista.\n",
      "PREDICTED: — Che c ’ è ? — chiese Kitty , cercando di afferrare che cosa avesse pensato con la sua stizza . — Come è un uomo ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 17: 100%|██████████| 1819/1819 [17:02<00:00,  1.78it/s, loss=4.121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"These were vile discoveries; but except for the treachery of concealment, I should have made them no subject of reproach to my wife, even when I found her nature wholly alien to mine, her tastes obnoxious to me, her cast of mind common, low, narrow, and singularly incapable of being led to anything higher, expanded to anything larger--\n",
      "TARGET: — Furono scoperte odiose quelle che feci; ero dolente che mi avessero tradito celandomi la verità; ma senza la parte che vi aveva presa mia moglie, non avrei mai pensato a rimproverarle la sventura della sua famiglia, neppure quando mi accorsi che la sua indole era completamente diversa dalla mia, e che i suoi gusti non mi piacevano. Aveva uno spirito volgare, basso, limitato e incapace di capire le cose nobili ed elevate.\n",
      "PREDICTED: — Allora , — disse , — ma non ho mai veduto il mio carattere , ma non potevo appartenere a me , ma non potevo sopportare il mio cuore , ma la mia coscienza di , e la mia coscienza di , la mia cara situazione , che è la mia cara creatura , che è la mia bellezza e la mia .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'I wish I hadn't mentioned Dinah!' she said to herself in a melancholy tone. 'Nobody seems to like her, down here, and I'm sure she's the best cat in the world!\n",
      "TARGET: “Non dovevo nominare Dina! — disse malinconicamente tra sè. — Pare che quaggiù nessuno le voglia bene; ed è la migliore gatta del mondo!\n",
      "PREDICTED: — Non capisco , — disse , — che non ho mai veduto il suo amore , — e io ho lasciato la sua opinione , e ho visto che non ho il diritto di dire che io sia stato di lui .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 18: 100%|██████████| 1819/1819 [17:01<00:00,  1.78it/s, loss=4.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: We passed the bridge, and soon after that I asked if she saw the lock.\n",
      "TARGET: Passammo il ponte, e subito dopo le chiesi se vedesse la chiusa.\n",
      "PREDICTED: la strada e feci un salto davanti a me .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Adele flew to the window. I followed, taking care to stand on one side, so that, screened by the curtain, I could see without being seen.\n",
      "TARGET: Andai io pure alla finestra nascondendomi dietro le tende per vedere senza esser veduta.\n",
      "PREDICTED: Bessie si sedeva davanti a me , e io fui alzata , perché udii colazione , mi alzai e mi figuro che ero entrato .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 19: 100%|██████████| 1819/1819 [17:01<00:00,  1.78it/s, loss=4.439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Formerly,\" I answered, \"because you did not love me; now, I reply, because you almost hate me.\n",
      "TARGET: — Altra volta — dissi, — era perché non mi amavate; ora è perché mi aborrite quasi.\n",
      "PREDICTED: — No , — risposi , — non vi ho mai pensato di voi , ma non voglio sopportare la mia volontà .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The water revived his father more than all the rum or spirits I had given him, for he was fainting with thirst.\n",
      "TARGET: Quando mi fu vicino mi consegnò frettolosamente queste, e recò l’acqua a suo padre; il quale poichè ne ebbe bevuto un poco, ne fu ristorato più che dal rum, che io gli avea donato; che il povero selvaggio moriva della sete.\n",
      "PREDICTED: La tempesta era grande , ma io , siccome la mia navicella , mi rispose che fosse un grosso mistero .\n"
     ]
    }
   ],
   "source": [
    "train_model(config) # Training model with the config arguments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
