{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introducción a Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carga de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datasets y Dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un Dataset es un conjunto de datos etiquetados, por ejemplo un montón de fotos etiquetadas. Pytorch ofrece varios Datasets (como FashionMNIST)\n",
        "\n",
        "Un Dataloader es un iterable que envuelve los Datasets en batches (grupos) para poder cargar uno de estos batches, metérselo a la red neuronal y así poder trabajar con varios datos del Dataset en paralelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cuando usamos un Dataset de Pytorch tenemos varias opciones:\n",
        " * root: elegimos la carpeta donde se va a guardar\n",
        " * train: indicamos si queremos usar la parte del dataset de entrenamiento o de test\n",
        " * download: descargar el dataset si no está descargado\n",
        " * transform: Aplicar transformaciones a los datos\n",
        " * target_transform: Aplicar transformaciones a las etiquetas\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26422272it [01:08, 383360.28it/s]                              \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "29696it [00:00, 316927.40it/s]           \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4422656it [00:09, 471305.48it/s]                             \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6144it [00:00, 1047169.89it/s]          \n",
            "/home/mfnunez/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554788289/work/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez tenemos el Dataset lo convertimos en un Dataloader, indicando el batch size de cada batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
            "Shape of y:  torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creando el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En primer lugar vemos si tenemos disponible una GPU y si es así definimos esta como el dispositivo donde vamos a hacer el procesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos la red neuronal\n",
        "\n",
        "Para esto vamos a utilizar el módulo _nn.Module_\n",
        "Cada vez que creamos una red neuronal en Pytorch es necesario crear dos métodos como mínimo\n",
        " * El método __init()__ en el que se inicializa _nn.Module_ y se declara cómo va a ser la arquitectura de la red neuronal\n",
        " * El método __forward()__ que describe cómo van a pasar los datos por la red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()   # Se inicializa el módulo nn.Module\n",
        "        self.flatten = nn.Flatten()             # Se crea una primera capa que aplana la imagen de entrada\n",
        "        self.linear_relu_stack = nn.Sequential( # Se crea una arquitectura secuencial:\n",
        "            nn.Linear(28*28, 512),                  # Se añade una primera capa lineal que está preparada para que le entre un vector de 28*28 (784)\n",
        "                                                    # y sacará un vector de 512\n",
        "            nn.ReLU(),                              # Se añade una no linealidad\n",
        "            nn.Linear(512, 512),                    # Se añade una segunda capa lineal que le entran 512 datos y saca 512 datos\n",
        "            nn.ReLU(),                              # Se añade una no linealidad\n",
        "            nn.Linear(512, 10)                      # Se añade una tercera capa lineal que le entran 512 datos y saca un array de tamaño 10 (el número\n",
        "                                                    # de etiquetas)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)                         # Se pasa la imagen por la capa de aplanado para aplanar la imagen\n",
        "        logits = self.linear_relu_stack(x)          # Se pasa el vector resultante por la red\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)                  # Se instancia el modelo\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimización de los parámetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Todo entrenamiento de una red neuronal consiste en la optimización de los parametros de esta para minimizar (o maximizar) una función de coste.\n",
        "Se define la función de coste y la manera de optimizar (el optimizador) los parametros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definición de los bucles de entrenamiento y test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En cada época de entrenamiento se evaluan todos los batches del Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)                  # Número de datos del dataset\n",
        "    model.train()                                   # Se pone el modelo en modo entrenamiento\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):     # For para obtener los datos y etiquetas de cada batch\n",
        "        \n",
        "        X, y = X.to(device), y.to(device)           # Se mandan los datos y las etiquetas al dispositivo (en este caso la GPU)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)                             # Se obtienen las salidas de la red para los datos\n",
        "        loss = loss_fn(pred, y)                     # Se calcula la pérdida de estas salidas con respecto a las etiquetas en función\n",
        "                                                    # de la función de pérdida elegida\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()                       # Se ponen a cero los gradientes\n",
        "        loss.backward()                             # Se obtienen los gradientes de los parámetros\n",
        "        optimizer.step()                            # Se actualizan los parámetros\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)                 # Se calcula la pérdida y el número de datos procesados\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")      # Se imprime la pérdida y el número de datos procesados de todos los que hay en un batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Durante el test se evaluan los datos de test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)              # Número de datos del dataset\n",
        "    num_batches = len(dataloader)               # Número de batches\n",
        "    model.eval()                                # Se pone el modelo en modo evaluación\n",
        "    test_loss, correct = 0, 0                   # Se inicializan las variables de pérdida y accuracy\n",
        "\n",
        "    with torch.no_grad():                                                       # Se indica que no se calculen los gradientes\n",
        "        for X, y in dataloader:                                                 # Se obtienen las imágenes y etiquetas del dataloader\n",
        "            X, y = X.to(device), y.to(device)                                   # Se mandan al dispositivo (en este caso la GPU)\n",
        "\n",
        "            pred = model(X)                                                     # Se obtienen las predicciones del modelo\n",
        "\n",
        "            test_loss += loss_fn(pred, y).item()                                # Se acumula el error\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()     # Se acumula cada vez que se acierta uno\n",
        "\n",
        "    test_loss /= num_batches                                                                    # Se calcula el error dividiendo el total entre el número de batches\n",
        "    correct /= size                                                                             # Se obtiene el accuracy dividiendo los resultados buenos entre el \n",
        "                                                                                                # total de datos\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")     # Se imprime el accuracy y el error medio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se entrena durante varias épocas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.308877  [    0/60000]\n",
            "loss: 2.291079  [ 6400/60000]\n",
            "loss: 2.265797  [12800/60000]\n",
            "loss: 2.255189  [19200/60000]\n",
            "loss: 2.246202  [25600/60000]\n",
            "loss: 2.215523  [32000/60000]\n",
            "loss: 2.219025  [38400/60000]\n",
            "loss: 2.189023  [44800/60000]\n",
            "loss: 2.182710  [51200/60000]\n",
            "loss: 2.148128  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.2%, Avg loss: 2.142635 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.156960  [    0/60000]\n",
            "loss: 2.142013  [ 6400/60000]\n",
            "loss: 2.076811  [12800/60000]\n",
            "loss: 2.095929  [19200/60000]\n",
            "loss: 2.047673  [25600/60000]\n",
            "loss: 1.987178  [32000/60000]\n",
            "loss: 2.014789  [38400/60000]\n",
            "loss: 1.934459  [44800/60000]\n",
            "loss: 1.931124  [51200/60000]\n",
            "loss: 1.861800  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.1%, Avg loss: 1.856072 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.892417  [    0/60000]\n",
            "loss: 1.855688  [ 6400/60000]\n",
            "loss: 1.730256  [12800/60000]\n",
            "loss: 1.779484  [19200/60000]\n",
            "loss: 1.668417  [25600/60000]\n",
            "loss: 1.625547  [32000/60000]\n",
            "loss: 1.647462  [38400/60000]\n",
            "loss: 1.548367  [44800/60000]\n",
            "loss: 1.565529  [51200/60000]\n",
            "loss: 1.469859  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.7%, Avg loss: 1.481015 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.549556  [    0/60000]\n",
            "loss: 1.514754  [ 6400/60000]\n",
            "loss: 1.356506  [12800/60000]\n",
            "loss: 1.440034  [19200/60000]\n",
            "loss: 1.319539  [25600/60000]\n",
            "loss: 1.322142  [32000/60000]\n",
            "loss: 1.340117  [38400/60000]\n",
            "loss: 1.263090  [44800/60000]\n",
            "loss: 1.291205  [51200/60000]\n",
            "loss: 1.206337  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.8%, Avg loss: 1.222338 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.296545  [    0/60000]\n",
            "loss: 1.285441  [ 6400/60000]\n",
            "loss: 1.108645  [12800/60000]\n",
            "loss: 1.226129  [19200/60000]\n",
            "loss: 1.103080  [25600/60000]\n",
            "loss: 1.129201  [32000/60000]\n",
            "loss: 1.156904  [38400/60000]\n",
            "loss: 1.089844  [44800/60000]\n",
            "loss: 1.124373  [51200/60000]\n",
            "loss: 1.053130  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.8%, Avg loss: 1.065503 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Guardar el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se pueden guardar los parámetros del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carga del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se pueden cargar los parámetros de un modelo previamente guardados, para ello primero hay que hacer una instancia del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = NeuralNetwork()                             # Se instancia el modelo\n",
        "model.load_state_dict(torch.load(\"model.pth\"))      # Se cargan sus parámetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predicciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se puede usar el modelo para hacer predicciones\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
          ]
        }
      ],
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d1c24abb23a313e1f9ae042292cd8e6e3c60c5818227ced3d46e3df2c65171ef"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
