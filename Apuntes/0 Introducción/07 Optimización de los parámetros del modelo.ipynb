{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimización de los parámetros del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora que tenemos un modelo y datos, es hora de entrenar, validar y probar nuestro modelo optimizando sus parámetros en nuestros datos. Entrenar un modelo es un proceso iterativo; en cada iteración (llamada época) el modelo hace una suposición sobre la salida, calcula el error en su suposición (pérdida), recopila las derivadas del error con respecto a sus parámetros (como vimos en la sección anterior) y **optimiza** estos parámetros usando el descenso de gradiente. Para obtener un recorrido más detallado de este proceso, consulte este video sobre [backpropagation de 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Código previo necesario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hiperparámetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los hiperparámetros son parámetros ajustables que le permiten controlar el proceso de optimización del modelo. Diferentes valores de hiperparámetros pueden afectar el entrenamiento del modelo y las tasas de convergencia (lea más sobre el ajuste de hiperparámetros)\n",
        "\n",
        "Definimos los siguientes hiperparámetros para el entrenamiento:\n",
        " * **Número de épocas**: el número de veces que se itera sobre el conjunto de datos\n",
        " * **Tamaño de lote**: el número de muestras de datos propagadas a través de la red antes de que se actualicen los parámetros.\n",
        " * **Tasa de aprendizaje**: cuánto actualizar los parámetros de los modelos en cada lote/época. Los valores más pequeños producen una velocidad de aprendizaje lenta, mientras que los valores altos pueden provocar un comportamiento impredecible durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bucle de optimización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez que establezcamos nuestros hiperparámetros, podemos entrenar y optimizar nuestro modelo con un ciclo de optimización. Cada iteración del ciclo de optimización se denomina época.\n",
        "\n",
        "Cada época consta de dos partes principales:\n",
        " * **El ciclo de entrenamiento**: itera sobre el conjunto de datos de entrenamiento e intenta converger a los parámetros óptimos.\n",
        " * **El ciclo de validación/prueba**: itera sobre el conjunto de datos de prueba para comprobar si el rendimiento del modelo está mejorando.\n",
        "\n",
        "Familiaricémonos brevemente con algunos de los conceptos utilizados en el ciclo de entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function\n",
        "\n",
        "Cuando se le presentan algunos datos de entrenamiento, es probable que nuestra red no capacitada no dé la respuesta correcta. La función de pérdida mide el grado de disimilitud del resultado obtenido con el valor objetivo, y es la función de pérdida lo que queremos minimizar durante el entrenamiento. Para calcular la pérdida, hacemos una predicción utilizando las entradas de nuestra muestra de datos dada y la comparamos con el valor real de la etiqueta de datos.\n",
        "\n",
        "Las funciones de pérdida comunes incluyen [``nn.MSELoss``](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) (error cuadrático medio) para tareas de regresión y [``nn.NLLLoss``](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) (probabilidad de registro negativo) para clasificación. [``nn.CrossEntropyLoss``](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) combina ``nn.LogSoftmaxy`` ``nn.NLLLoss``.\n",
        "\n",
        "Pasamos los logits de salida de nuestro modelo a ``nn.CrossEntropyLoss``, que normalizarán los logits y calcularán el error de predicción."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La optimización es el proceso de ajustar los parámetros del modelo para reducir el error del modelo en cada paso de entrenamiento. **Los algoritmos de optimización** definen cómo se realiza este proceso (en este ejemplo usamos Descenso de gradiente estocástico). Toda la lógica de optimización está encapsulada en el objeto ``optimizer``. Aquí usamos el optimizador SGD; Además, hay muchos [optimizadores diferentes](https://pytorch.org/docs/stable/optim.html) disponibles en PyTorch, como ADAM y RMSProp, que funcionan mejor para diferentes tipos de modelos y datos.\n",
        "\n",
        "Inicializamos el optimizador registrando los parámetros del modelo que necesitan ser entrenados y pasando el hiperparámetro de tasa de aprendizaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dentro del ciclo de entrenamiento, la optimización ocurre en tres pasos:\n",
        " * Llamada a ``optimizer.zero_grad()`` para restablecer los gradientes de los parámetros del modelo. Los degradados se suman por defecto; para evitar el doble conteo, los ponemos a cero explícitamente en cada iteración.\n",
        " * Retropropagación de la pérdida de predicción con una llamada a ``loss.backwards()``. PyTorch deposita los gradientes de pérdida con cada parámetro.\n",
        " * Una vez que tenemos nuestros degradados, llamamos ``optimizer.step()`` a ajustar los parámetros por los degradados recogidos en la backward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementación completa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos ``train_loop`` que recorre nuestro código de optimización y ``test_loop`` que evalúa el rendimiento del modelo en comparación con nuestros datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inicializamos la función de pérdida y el optimizador, y lo pasamos a ``train_loop`` y ``test_loop``. No dude en aumentar el número de épocas para realizar un seguimiento de la mejora del rendimiento del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.308122  [    0/60000]\n",
            "loss: 2.294896  [ 6400/60000]\n",
            "loss: 2.277630  [12800/60000]\n",
            "loss: 2.264670  [19200/60000]\n",
            "loss: 2.258785  [25600/60000]\n",
            "loss: 2.227958  [32000/60000]\n",
            "loss: 2.232154  [38400/60000]\n",
            "loss: 2.202238  [44800/60000]\n",
            "loss: 2.206822  [51200/60000]\n",
            "loss: 2.160461  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.9%, Avg loss: 2.164159 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.179595  [    0/60000]\n",
            "loss: 2.167129  [ 6400/60000]\n",
            "loss: 2.108799  [12800/60000]\n",
            "loss: 2.117221  [19200/60000]\n",
            "loss: 2.077025  [25600/60000]\n",
            "loss: 2.017354  [32000/60000]\n",
            "loss: 2.043247  [38400/60000]\n",
            "loss: 1.965428  [44800/60000]\n",
            "loss: 1.979008  [51200/60000]\n",
            "loss: 1.892849  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 55.9%, Avg loss: 1.896715 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.932781  [    0/60000]\n",
            "loss: 1.901781  [ 6400/60000]\n",
            "loss: 1.777608  [12800/60000]\n",
            "loss: 1.815290  [19200/60000]\n",
            "loss: 1.720087  [25600/60000]\n",
            "loss: 1.661584  [32000/60000]\n",
            "loss: 1.688403  [38400/60000]\n",
            "loss: 1.585201  [44800/60000]\n",
            "loss: 1.616330  [51200/60000]\n",
            "loss: 1.502001  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.3%, Avg loss: 1.523166 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.588519  [    0/60000]\n",
            "loss: 1.556287  [ 6400/60000]\n",
            "loss: 1.399856  [12800/60000]\n",
            "loss: 1.470475  [19200/60000]\n",
            "loss: 1.362115  [25600/60000]\n",
            "loss: 1.350322  [32000/60000]\n",
            "loss: 1.369209  [38400/60000]\n",
            "loss: 1.291247  [44800/60000]\n",
            "loss: 1.328078  [51200/60000]\n",
            "loss: 1.222178  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.1%, Avg loss: 1.251994 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.326881  [    0/60000]\n",
            "loss: 1.313619  [ 6400/60000]\n",
            "loss: 1.142745  [12800/60000]\n",
            "loss: 1.245553  [19200/60000]\n",
            "loss: 1.128474  [25600/60000]\n",
            "loss: 1.149157  [32000/60000]\n",
            "loss: 1.174668  [38400/60000]\n",
            "loss: 1.108777  [44800/60000]\n",
            "loss: 1.148949  [51200/60000]\n",
            "loss: 1.059267  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.1%, Avg loss: 1.084361 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.151493  [    0/60000]\n",
            "loss: 1.160943  [ 6400/60000]\n",
            "loss: 0.972917  [12800/60000]\n",
            "loss: 1.104774  [19200/60000]\n",
            "loss: 0.984017  [25600/60000]\n",
            "loss: 1.014531  [32000/60000]\n",
            "loss: 1.053949  [38400/60000]\n",
            "loss: 0.991465  [44800/60000]\n",
            "loss: 1.031828  [51200/60000]\n",
            "loss: 0.956622  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.9%, Avg loss: 0.975815 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.028421  [    0/60000]\n",
            "loss: 1.061229  [ 6400/60000]\n",
            "loss: 0.856086  [12800/60000]\n",
            "loss: 1.010755  [19200/60000]\n",
            "loss: 0.892250  [25600/60000]\n",
            "loss: 0.919711  [32000/60000]\n",
            "loss: 0.974396  [38400/60000]\n",
            "loss: 0.914928  [44800/60000]\n",
            "loss: 0.950606  [51200/60000]\n",
            "loss: 0.887754  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.3%, Avg loss: 0.902249 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.937980  [    0/60000]\n",
            "loss: 0.992342  [ 6400/60000]\n",
            "loss: 0.773238  [12800/60000]\n",
            "loss: 0.944982  [19200/60000]\n",
            "loss: 0.831321  [25600/60000]\n",
            "loss: 0.851060  [32000/60000]\n",
            "loss: 0.919125  [38400/60000]\n",
            "loss: 0.864100  [44800/60000]\n",
            "loss: 0.892307  [51200/60000]\n",
            "loss: 0.838786  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.7%, Avg loss: 0.849697 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.868922  [    0/60000]\n",
            "loss: 0.941001  [ 6400/60000]\n",
            "loss: 0.711798  [12800/60000]\n",
            "loss: 0.896418  [19200/60000]\n",
            "loss: 0.788303  [25600/60000]\n",
            "loss: 0.799831  [32000/60000]\n",
            "loss: 0.877613  [38400/60000]\n",
            "loss: 0.828729  [44800/60000]\n",
            "loss: 0.848789  [51200/60000]\n",
            "loss: 0.801607  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.9%, Avg loss: 0.810056 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.814209  [    0/60000]\n",
            "loss: 0.899944  [ 6400/60000]\n",
            "loss: 0.664421  [12800/60000]\n",
            "loss: 0.859047  [19200/60000]\n",
            "loss: 0.755929  [25600/60000]\n",
            "loss: 0.760602  [32000/60000]\n",
            "loss: 0.844173  [38400/60000]\n",
            "loss: 0.802611  [44800/60000]\n",
            "loss: 0.815116  [51200/60000]\n",
            "loss: 0.771863  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.4%, Avg loss: 0.778638 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d1c24abb23a313e1f9ae042292cd8e6e3c60c5818227ced3d46e3df2c65171ef"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
