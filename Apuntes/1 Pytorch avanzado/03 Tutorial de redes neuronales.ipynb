{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial de redes neuronales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las redes neuronales se pueden construir usando el paquete `torch.nn`.\n",
        "\n",
        "Ahora que ha visto `autograd`, `nn` depende `autograd` para definir modelos y diferenciarlos. `nn.Module` contiene capas y un método `forward(input)` que devuelve `output`.\n",
        "\n",
        "Por ejemplo, mire esta red que clasifica imágenes de dígitos:\n",
        "\n",
        "![mnist](https://pytorch.org/tutorials/_images/mnist.png)\n",
        "\n",
        "Es una red feed-forward simple. Toma la entrada, la pasa a través de varias capas una tras otra, y finalmente da la salida.\n",
        "\n",
        "Un procedimiento de entrenamiento típico para una red neuronal es el siguiente:\n",
        "\n",
        " * Definir la red neuronal que tiene algunos parámetros (o pesos) que se pueden aprender\n",
        " * Iterar sobre un conjunto de datos de entradas\n",
        " * Procesar los datos a través de la red\n",
        " * Calcule la pérdida (qué tan lejos está la salida de ser correcta)\n",
        " * Propagar gradientes de regreso a los parámetros de la red.\n",
        " * Actualice los pesos de la red, normalmente usando una regla de actualización simple: `weight = weight - learning_rate * gradient`\n",
        "\n",
        "It is a simple feed-forward network. It takes the input, feeds it\n",
        "through several layers one after the other, and then finally gives the\n",
        "output.\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "\n",
        "- Define the neural network that has some learnable parameters (or\n",
        "  weights)\n",
        "- Iterate over a dataset of inputs\n",
        "- Process input through the network\n",
        "- Compute the loss (how far is the output from being correct)\n",
        "- Propagate gradients back into the network’s parameters\n",
        "- Update the weights of the network, typically using a simple update rule:\n",
        "  ``weight = weight - learning_rate * gradient``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definición de la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # kernel\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        # 6 input image channel, 16 output channels, 5x5 square convolution\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension \n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square, you can specify with a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Solo tiene que definir la función `forward`, y la función `backward` (donde se calculan los gradientes) se define automáticamente para usted por parte de `autograd`. Puedes usar cualquiera de las operaciones de Tensor en la función `forward`.\n",
        "\n",
        "Los parámetros que se pueden aprender de un modelo son devueltos por `net.parameters()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "torch.Size([6, 1, 5, 5])\n"
          ]
        }
      ],
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Probemos con una entrada aleatoria de 32x32.\n",
        "\n",
        " > **Nota**: el tamaño de entrada esperado de esta red (LeNet) es 32x32. Para usar esta red en el conjunto de datos MNIST, cambie el tamaño de las imágenes del conjunto de datos a 32x32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0720, -0.2085, -0.0294, -0.0226,  0.0114, -0.0632,  0.0166, -0.1427,\n",
            "          0.0070, -0.0554]], grad_fn=<AddmmBackward>)\n"
          ]
        }
      ],
      "source": [
        "input = torch.randn(1, 1, 32, 32)\n",
        "out = net(input)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ponga a cero los búferes de gradiente de todos los parámetros y backprops con gradientes aleatorios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net.zero_grad()\n",
        "out.backward(torch.randn(1, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > **Nota**:\n",
        " > `torch.nn` solo admite mini lotes. El paquete `torch.nn` completo solo admite entradas que son un mini lote de muestras, y no una sola muestra.\n",
        " >\n",
        " > Por ejemplo, `nn.Conv2d` tomará un tensor 4D de `nSamples x nChannels x Height x Width`\n",
        " >\n",
        " > Si tiene una sola muestra, use `input.unsqueeze(0)` para agregar una dimensión de lote falsa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de continuar, recapitulemos todas las clases que ha visto hasta ahora.\n",
        "\n",
        "**Resumen**:\n",
        " * `torch.Tensor`- Una *matriz multidimensional* con soporte para operaciones de autograd como `backward()`. También *contiene el tensor* con el gradiente.\n",
        " * `nn.Module`- Módulo de red neuronal. *Una forma conveniente de encapsular parámetros*, con helpers para moverlos a la GPU, exportarlos, cargarlos, etc.\n",
        " * `nn.Parameter`- Una especie de tensor, que se *registra automáticamente como parámetro cuando se asigna como atributo a* `Module`.\n",
        " * `autograd.Function`- Implementa *definiciones hacia adelante y hacia atrás de una operación de autograd*. Cada operación `Tensor` crea al menos un único nodo `Function` que se conecta a las funciones que crearon `Tensor` y *codifica su historial* .\n",
        "\n",
        "**En este punto, cubrimos**:\n",
        " * Definición de una red neuronal\n",
        " * Procesando entradas y llamando a backward\n",
        "\n",
        "\n",
        "**Aún queda**:\n",
        " * Calcular la pérdida\n",
        " * Actualización de los pesos de la red"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Función de pérdida"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una función de pérdida toma el par de entradas (salida, objetivo) y calcula un valor que estima qué tan lejos está la salida del objetivo.\n",
        "\n",
        "Hay varias [funciones de pérdida](https://pytorch.org/docs/nn.html#loss-functions) diferentes en el paquete `nn`. Una pérdida simple es: `nn.MSELoss` que calcula el error cuadrático medio entre la entrada y el objetivo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10])\n",
            "torch.Size([10])\n",
            "torch.Size([1, 10])\n",
            "tensor(1.1937, grad_fn=<MseLossBackward>)\n"
          ]
        }
      ],
      "source": [
        "output = net(input)\n",
        "print(output.shape)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "print(target.shape)\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "print(target.shape)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora, si sigue `loss` en la dirección hacia atrás, usando su atributo `.grad_fn`, verá un gráfico de cálculos que se ve así:\n",
        "\n",
        "```\n",
        "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
        "          -> flatten -> linear -> relu -> linear -> relu -> linear\n",
        "          -> MSELoss\n",
        "          -> loss\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Entonces, cuando llamamos `loss.backward()`, todo el gráfico se diferencia con los parámetros de la red neuronal, y todos los Tensores en el gráfico que tengan `requires_grad=True` tendrán su `.gradTensor` acumulado con el gradiente.\n",
        "\n",
        "A modo de ilustración, sigamos algunos pasos hacia atrás:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<MseLossBackward object at 0x7f8608357730>\n",
            "<AddmmBackward object at 0x7f8608357bb0>\n",
            "<AccumulateGrad object at 0x7f8608357730>\n"
          ]
        }
      ],
      "source": [
        "print(loss.grad_fn)  # MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backprop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para propagar el error hacia atrás, todo lo que tenemos que hacer es `loss.backward()`. Sin embargo, debe borrar los degradados existentes; de lo contrario, los degradados se acumularán en los degradados existentes.\n",
        "\n",
        "Ahora llamaremos `loss.backward()` y veremos los gradientes de sesgo de conv1 antes y después del retroceso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv1.bias.grad before backward\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "conv1.bias.grad after backward\n",
            "tensor([ 0.0174, -0.0032,  0.0136, -0.0195, -0.0014, -0.0151])\n"
          ]
        }
      ],
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora, hemos visto cómo usar las funciones de pérdida.\n",
        "\n",
        "**Leer más tarde**:\n",
        "\n",
        "El paquete de red neuronal contiene varios módulos y funciones de pérdida que forman los componentes básicos de las redes neuronales profundas. Una lista completa con documentación está [aquí](https://pytorch.org/docs/nn).\n",
        "\n",
        "\n",
        "**Lo único que queda por aprender es**:\n",
        "\n",
        "Actualización de los pesos de la red"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Actualización de los pesos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La regla de actualización más simple utilizada en la práctica es el Descenso de gradiente estocástico (SGD):\n",
        "\n",
        "`weight = weight - learning_rate * gradient`\n",
        "\n",
        "Podemos implementar esto usando un código Python simple:\n",
        "\n",
        "```\n",
        "learning_rate = 0.01\n",
        "for f in net.parameters():\n",
        "    f.data.sub_(f.grad.data * learning_rate)\n",
        "```\n",
        "\n",
        "Sin embargo, como usa redes neuronales, desea usar varias reglas de actualización diferentes como SGD, Nesterov-SGD, Adam, RMSProp, etc. Para habilitar esto, creamos un pequeño paquete: `torch.optim` que implementa todos estos métodos. Usarlo es muy simple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > **Nota**:\n",
        " > Observe cómo los búferes de gradiente tuvieron que configurarse manualmente a cero usando `optimizer.zero_grad()`. Esto se debe a que los gradientes se acumulan como se explica en la sección Backprop."
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "8e4d58f53b4b3ced286559ef92073773937aa87eedd0536c036fd264999b02c5"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
