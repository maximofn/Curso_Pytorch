{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aprender Pytorch con ejemplos - Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En los ejemplos anteriores, tuvimos que implementar manualmente los pases hacia adelante y hacia atrás de nuestra red neuronal. La implementación manual del paso hacia atrás no es un gran problema para una red pequeña de dos capas, pero puede complicarse rápidamente en redes grandes y complejas.\n",
        "\n",
        "Afortunadamente, podemos usar [la diferenciación automática](https://en.wikipedia.org/wiki/Automatic_differentiation) para automatizar el cálculo de pases hacia atrás en redes neuronales. El paquete autograd en PyTorch proporciona exactamente esta funcionalidad. Al usar autograd, el pase directo de su red definirá un gráfico computacional; los nodos en el gráfico serán tensores y los bordes serán funciones que producen tensores de salida a partir de tensores de entrada. La retropropagación a través de este gráfico le permite calcular gradientes fácilmente.\n",
        "\n",
        "Esto suena complicado, es bastante sencillo de usar en la práctica. Cada tensor representa un nodo en un gráfico computacional. Si ``x`` es un Tensor que tiene, ``x.requires_grad=True`` entonces ``x.grad`` es otro Tensor que mantiene el gradiente de ``x`` con respecto a algún valor escalar.\n",
        "\n",
        "Aquí usamos PyTorch Tensors y autograd para implementar nuestro ejemplo de onda sinusoidal de ajuste con polinomio de tercer orden; ahora ya no necesitamos implementar manualmente el paso hacia atrás a través de la red:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "99 83.7930679321289\n",
            "199 58.96221923828125\n",
            "299 42.38225555419922\n",
            "399 31.299880981445312\n",
            "499 23.88750648498535\n",
            "599 18.926542282104492\n",
            "699 15.603999137878418\n",
            "799 13.377126693725586\n",
            "899 11.883524894714355\n",
            "999 10.88094425201416\n",
            "1099 10.207419395446777\n",
            "1199 9.7545747756958\n",
            "1299 9.449835777282715\n",
            "1399 9.244585037231445\n",
            "1499 9.106199264526367\n",
            "1599 9.012828826904297\n",
            "1699 8.949758529663086\n",
            "1799 8.907115936279297\n",
            "1899 8.878255844116211\n",
            "1999 8.858701705932617\n",
            "Result: y = 0.004409832414239645 + 0.8519610166549683 x + -0.0007607686566188931 x^2 + -0.09265050292015076 x^3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "# By default, requires_grad=False, which indicates that we do not need to\n",
        "# compute gradients with respect to these Tensors during the backward pass.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Create random Tensors for weights. For a third order polynomial, we need\n",
        "# 4 weights: y = a + b x + c x^2 + d x^3\n",
        "# Setting requires_grad=True indicates that we want to compute gradients with\n",
        "# respect to these Tensors during the backward pass.\n",
        "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y using operations on Tensors.\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss using operations on Tensors.\n",
        "    # Now loss is a Tensor of shape (1,)\n",
        "    # loss.item() gets the scalar value held in the loss.\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Use autograd to compute the backward pass. This call will compute the\n",
        "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
        "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
        "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
        "    loss.backward()\n",
        "\n",
        "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
        "    # because weights have requires_grad=True, but we don't need to track this\n",
        "    # in autograd.\n",
        "    with torch.no_grad():\n",
        "        a -= learning_rate * a.grad\n",
        "        b -= learning_rate * b.grad\n",
        "        c -= learning_rate * c.grad\n",
        "        d -= learning_rate * d.grad\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        a.grad = None\n",
        "        b.grad = None\n",
        "        c.grad = None\n",
        "        d.grad = None\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d1c24abb23a313e1f9ae042292cd8e6e3c60c5818227ced3d46e3df2c65171ef"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
