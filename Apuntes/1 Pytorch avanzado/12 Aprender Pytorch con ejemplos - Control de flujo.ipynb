{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aprender Pytorch con ejemplos - Control de flujo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para mostrar el poder de los gráficos dinámicos de PyTorch, implementaremos un modelo muy extraño: un polinomio de tercer-quinto orden que en cada pasada hacia adelante elige un número aleatorio entre 3 y 5 y usa esa cantidad de órdenes, reutilizando los mismos pesos varias veces para calcular el cuarto y quinto orden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1999 1978.011474609375\n",
            "3999 950.7560424804688\n",
            "5999 459.6216735839844\n",
            "7999 222.87266540527344\n",
            "9999 109.45297241210938\n",
            "11999 58.20433807373047\n",
            "13999 32.66537857055664\n",
            "15999 26.182247161865234\n",
            "17999 14.369629859924316\n",
            "19999 11.48314094543457\n",
            "21999 10.150697708129883\n",
            "23999 9.384397506713867\n",
            "25999 9.145648956298828\n",
            "27999 8.992356300354004\n",
            "29999 8.906698226928711\n",
            "Result: y = -0.008100783452391624 + 0.8540968298912048 x + 0.0009260551887564361 x^2 + -0.09319843351840973 x^3 + 8.663882181281224e-05 x^4 ? + 8.663882181281224e-05 x^5 ?\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "class DynamicNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate five parameters and assign them as members.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.a = torch.nn.Parameter(torch.randn(()))\n",
        "        self.b = torch.nn.Parameter(torch.randn(()))\n",
        "        self.c = torch.nn.Parameter(torch.randn(()))\n",
        "        self.d = torch.nn.Parameter(torch.randn(()))\n",
        "        self.e = torch.nn.Parameter(torch.randn(()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        For the forward pass of the model, we randomly choose either 4, 5\n",
        "        and reuse the e parameter to compute the contribution of these orders.\n",
        "\n",
        "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
        "        Python control-flow operators like loops or conditional statements when\n",
        "        defining the forward pass of the model.\n",
        "\n",
        "        Here we also see that it is perfectly safe to reuse the same parameter many\n",
        "        times when defining a computational graph.\n",
        "        \"\"\"\n",
        "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
        "        for exp in range(4, random.randint(4, 6)):\n",
        "            y = y + self.e * x ** exp\n",
        "        return y\n",
        "\n",
        "    def string(self):\n",
        "        \"\"\"\n",
        "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
        "        \"\"\"\n",
        "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
        "\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Construct our model by instantiating the class defined above\n",
        "model = DynamicNet()\n",
        "\n",
        "# Construct our loss function and an Optimizer. Training this strange model with\n",
        "# vanilla stochastic gradient descent is tough, so we use momentum\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
        "for t in range(30000):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y)\n",
        "    if t % 2000 == 1999:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f'Result: {model.string()}')"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "8e4d58f53b4b3ced286559ef92073773937aa87eedd0536c036fd264999b02c5"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
