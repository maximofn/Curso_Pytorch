{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Construir la red neuronal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las redes neuronales se componen de capas/módulos que realizan operaciones con datos. El espacio de nombres [``torch.nn``](https://pytorch.org/docs/stable/nn.html) proporciona todos los componentes básicos que necesita para construir su propia red neuronal. Cada módulo en PyTorch subclasifica el [``nn.Module``](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). Una red neuronal es un módulo en sí mismo que consta de otros módulos (capas). Esta estructura anidada permite construir y administrar arquitecturas complejas fácilmente.\n",
        "\n",
        "En las siguientes secciones, crearemos una red neuronal para clasificar imágenes en el conjunto de datos FashionMNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "import torch\n",
        "from torch import nn\n",
        "# from torch.utils.data import DataLoader\n",
        "# from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Obtener el dispositivo para entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Queremos poder entrenar nuestro modelo en un acelerador de hardware como la GPU, si está disponible. Revisemos para ver si [``torch.cuda``](https://pytorch.org/docs/stable/notes/cuda.html) está disponible, de lo contrario continuaremos usando la CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Definir la clase Red Neuronal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos nuestra red neuronal mediante subclases ``nn.Modulee`` inicializamos las capas de la red neuronal en ``__init__``. Cada subclase ``nn.Module`` implementa las operaciones sobre los datos de entrada en el método ``forward``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()   # Se inicializa el módulo nn.Module\n",
        "        self.flatten = nn.Flatten()             # Se crea una primera capa que aplana la imagen de entrada\n",
        "        self.linear_relu_stack = nn.Sequential( # Se crea una arquitectura secuencial:\n",
        "            nn.Linear(28*28, 512),                  # Se añade una primera capa lineal que está preparada para que le entre un vector de 28*28 (784)\n",
        "                                                    # y sacará un vector de 512\n",
        "            nn.ReLU(),                              # Se añade una no linealidad\n",
        "            nn.Linear(512, 512),                    # Se añade una segunda capa lineal que le entran 512 datos y saca 512 datos\n",
        "            nn.ReLU(),                              # Se añade una no linealidad\n",
        "            nn.Linear(512, 10)                      # Se añade una tercera capa lineal que le entran 512 datos y saca un array de tamaño 10 (el número\n",
        "                                                    # de etiquetas)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)                         # Se pasa la imagen por la capa de aplanado para aplanar la imagen\n",
        "        logits = self.linear_relu_stack(x)          # Se pasa el vector resultante por la red\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos una instancia de ``NeuralNetwork``, la movemos al ``device`` e imprimimos su estructura."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para usar el modelo, le pasamos los datos de entrada. Esto ejecuta el modelo forward, junto con algunas [operaciones en segundo plano](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866). ¡No llames a ``model.forward()`` directamente!\n",
        "\n",
        "Llamar al modelo en la entrada devuelve un tensor de 10 dimensiones con valores pronosticados sin procesar para cada clase. Obtenemos las probabilidades de predicción pasándolas a través de una instancia del módulo ``nn.Softmax``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A la salida el modelo devuelve un vector de 10 dimensiones: tensor([[0.1076, 0.1068, 0.0979, 0.1054, 0.0946, 0.1016, 0.0950, 0.0950, 0.0987,\n",
            "         0.0974]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
            "Predicted class: tensor([0], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "print(f\"A la salida el modelo devuelve un vector de {pred_probab.shape[1]} dimensiones: {pred_probab}\")\n",
        "\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Capas del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analicemos las capas en el modelo FashionMNIST. Para ilustrarlo, tomaremos un minibatch de muestra de 3 imágenes de tamaño 28x28 y veremos qué le pasa a medida que lo pasamos por la red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nn.Flatten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inicializamos la capa [``nn.Flatten``](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) para convertir cada imagen 2D 28x28 en una matriz contigua de 784 valores de píxeles (se mantiene la dimensión del minibatch (en dim = 0))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ],
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nn.Linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La [capa lineal](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) es un módulo que aplica una transformación lineal en la entrada usando sus pesos y sesgos almacenados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ],
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nn.ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las activaciones no lineales son las que crean las complejas asignaciones entre las entradas y salidas del modelo. Se aplican después de transformaciones lineales para introducir la no linealidad, lo que ayuda a las redes neuronales a aprender una amplia variedad de fenómenos.\n",
        "\n",
        "En este modelo, usamos [``nn.ReLU``](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) entre nuestras capas lineales, pero hay otras activaciones para introducir la no linealidad en su modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before ReLU: tensor([[ 0.3812, -0.3092, -0.9389, -0.1036,  0.0418, -0.7085, -0.3242,  0.2523,\n",
            "         -0.2707, -0.2178,  0.2485,  0.2694, -0.2686,  0.5476, -0.0729, -0.4035,\n",
            "          0.3148, -0.6317,  0.0987,  0.2874],\n",
            "        [ 0.5722, -0.4652, -0.5701, -0.1610,  0.3348, -0.6676, -0.1398,  0.3676,\n",
            "         -0.0971, -0.1017,  0.0411,  0.1108, -0.6608,  0.6363, -0.1903,  0.4054,\n",
            "          0.0733, -0.1758,  0.2478,  0.1387],\n",
            "        [ 0.2917, -0.4701, -0.5797,  0.0855,  0.1653, -0.5012,  0.0351,  0.2473,\n",
            "         -0.1083,  0.0202, -0.1595,  0.1441, -0.4096,  0.5220, -0.1599, -0.1122,\n",
            "          0.1384,  0.0906,  0.0486, -0.0508]], grad_fn=<AddmmBackward>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.3812, 0.0000, 0.0000, 0.0000, 0.0418, 0.0000, 0.0000, 0.2523, 0.0000,\n",
            "         0.0000, 0.2485, 0.2694, 0.0000, 0.5476, 0.0000, 0.0000, 0.3148, 0.0000,\n",
            "         0.0987, 0.2874],\n",
            "        [0.5722, 0.0000, 0.0000, 0.0000, 0.3348, 0.0000, 0.0000, 0.3676, 0.0000,\n",
            "         0.0000, 0.0411, 0.1108, 0.0000, 0.6363, 0.0000, 0.4054, 0.0733, 0.0000,\n",
            "         0.2478, 0.1387],\n",
            "        [0.2917, 0.0000, 0.0000, 0.0855, 0.1653, 0.0000, 0.0351, 0.2473, 0.0000,\n",
            "         0.0202, 0.0000, 0.1441, 0.0000, 0.5220, 0.0000, 0.0000, 0.1384, 0.0906,\n",
            "         0.0486, 0.0000]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nn.Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[``nn.Sequential``](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) es un contenedor ordenado de módulos. Los datos se pasan a través de todos los módulos en el mismo orden definido. Puede utilizar contenedores secuenciales para armar una red rápida como ``seq_modules``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 10])\n"
          ]
        }
      ],
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)\n",
        "\n",
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nn.Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La última capa lineal de la red neuronal devuelve ``logits`` (valores brutos en [-inf, inf]) que se pasan al módulo [``nn.Softmax``](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html). Los ``logits`` se escalan a valores [0, 1] que representan las probabilidades predichas del modelo para cada clase. El parámetro ``dim`` indica la dimensión a lo largo de la cual los valores deben sumar 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before softmax: tensor([[-0.2088,  0.0115,  0.2385,  0.2981, -0.2063, -0.2559, -0.2752, -0.2882,\n",
            "         -0.0599, -0.0644],\n",
            "        [-0.1546,  0.0353,  0.2467,  0.3408, -0.3545, -0.3311, -0.2543, -0.3207,\n",
            "         -0.0808,  0.0916],\n",
            "        [-0.2386,  0.1187,  0.2307,  0.1439, -0.2649, -0.2081, -0.2315, -0.3275,\n",
            "          0.0764, -0.0397]], grad_fn=<AddmmBackward>)\n",
            "\n",
            "\n",
            "After softmax: tensor([[0.0862, 0.1074, 0.1348, 0.1431, 0.0864, 0.0822, 0.0806, 0.0796, 0.1000,\n",
            "         0.0996],\n",
            "        [0.0900, 0.1088, 0.1345, 0.1477, 0.0737, 0.0755, 0.0815, 0.0762, 0.0969,\n",
            "         0.1151],\n",
            "        [0.0833, 0.1190, 0.1331, 0.1220, 0.0811, 0.0858, 0.0838, 0.0762, 0.1141,\n",
            "         0.1016]], grad_fn=<SoftmaxBackward>), tamaño softmax: torch.Size([3, 10])\n"
          ]
        }
      ],
      "source": [
        "print(f\"Before softmax: {logits}\\n\\n\")\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "\n",
        "print(f\"After softmax: {pred_probab}, tamaño softmax: {pred_probab.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parámetros del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Muchas capas dentro de una red neuronal están parametrizadas, es decir, tienen pesos y sesgos asociados que se optimizan durante el entrenamiento. La subclasificación ``nn.Module`` rastrea automáticamente todos los campos definidos dentro de su objeto de modelo y hace que todos los parámetros sean accesibles usando los métodos de su modelo ``parameters()`` o ``named_parameters()``.\n",
        "\n",
        "En este ejemplo, iteramos sobre cada parámetro e imprimimos su tamaño y una vista previa de sus valores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model structure:  NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ") \n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0206,  0.0110,  0.0090,  ...,  0.0110, -0.0005,  0.0167],\n",
            "        [-0.0214,  0.0210, -0.0219,  ...,  0.0284, -0.0220,  0.0099]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0013, -0.0295], device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0328, -0.0439,  0.0011,  ..., -0.0175, -0.0339, -0.0433],\n",
            "        [-0.0111,  0.0403,  0.0016,  ..., -0.0217, -0.0061,  0.0429]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0232, 0.0199], device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0100,  0.0319, -0.0066,  ..., -0.0039,  0.0059,  0.0270],\n",
            "        [ 0.0206, -0.0056, -0.0151,  ..., -0.0144,  0.0196,  0.0008]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0173, 0.0252], device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Model structure: \", model, \"\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d1c24abb23a313e1f9ae042292cd8e6e3c60c5818227ced3d46e3df2c65171ef"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
